{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirulMukminin0/DSS-Thesis/blob/main/Analysis_rice_stress_detection_rgb_thermal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeNZJmbybzP1"
      },
      "source": [
        "# TAHAP 0 - Mount Google Drive & Set ROOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wY19Le2Ab1u5"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 0.1 â€” Mount Google Drive & Set ROOT\n",
        "# ============================================\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# === ROOT UTAMA (KUNCI) ===\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/rice_stress_detection_rgb_thermal\")\n",
        "FIX_ROOT     = PROJECT_ROOT / \"DatasetTerbaruFIX\"\n",
        "\n",
        "# === SUBFOLDER FIX (biar tidak nyampur dengan lama) ===\n",
        "FRAMES_DIR       = FIX_ROOT / \"frames\"\n",
        "RGB_ALIGNED_DIR  = FRAMES_DIR / \"rgb_aligned\"\n",
        "THERMAL_DIR      = FRAMES_DIR / \"thermal\"\n",
        "PAIRS_CSV        = FRAMES_DIR / \"pairs_index.csv\"\n",
        "\n",
        "PATCH_ROOT       = FIX_ROOT / \"patches\"\n",
        "PATCH_RGB_DIR    = PATCH_ROOT / \"rgb\"        # optional if you choose to save patch images\n",
        "PATCH_TH_DIR     = PATCH_ROOT / \"thermal\"    # optional if you choose to save patch npy\n",
        "PATCH_META_DIR   = PATCH_ROOT / \"meta\"\n",
        "PATCHES_ALL_CSV  = PATCH_META_DIR / \"patches_all.csv\"\n",
        "PATCH_GRID_JSON  = PATCH_META_DIR / \"patch_grid.json\"\n",
        "\n",
        "LABEL_ROOT       = FIX_ROOT / \"labels\"\n",
        "LABEL_GOLD_DIR   = LABEL_ROOT / \"gold\"\n",
        "LABEL_SILVER_DIR = LABEL_ROOT / \"silver\"\n",
        "LABEL_FINAL_DIR  = LABEL_ROOT / \"final\"\n",
        "\n",
        "THERM_PREP_DIR   = FIX_ROOT / \"thermal_prep\"\n",
        "THERM_MASK_DIR   = THERM_PREP_DIR / \"masks_v2\"\n",
        "THERM_STATS_DIR  = THERM_PREP_DIR / \"stats\"\n",
        "\n",
        "EXP_ROOT         = FIX_ROOT / \"experiments\"\n",
        "DEPLOY_ROOT      = FIX_ROOT / \"deploy\"\n",
        "DEPLOY_APP_DIR   = DEPLOY_ROOT / \"flask_app\"\n",
        "DEPLOY_ART_DIR   = DEPLOY_ROOT / \"artifacts\"\n",
        "\n",
        "OUT_DIR          = FIX_ROOT / \"outputs\"\n",
        "\n",
        "def ensure_dir(p: Path):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create key directories\n",
        "for d in [\n",
        "    FIX_ROOT,\n",
        "    FRAMES_DIR, RGB_ALIGNED_DIR, THERMAL_DIR,\n",
        "    PATCH_ROOT, PATCH_RGB_DIR, PATCH_TH_DIR, PATCH_META_DIR,\n",
        "    LABEL_ROOT, LABEL_GOLD_DIR, LABEL_SILVER_DIR, LABEL_FINAL_DIR,\n",
        "    THERM_PREP_DIR, THERM_MASK_DIR, THERM_STATS_DIR,\n",
        "    EXP_ROOT, DEPLOY_ROOT, DEPLOY_APP_DIR, DEPLOY_ART_DIR,\n",
        "    OUT_DIR\n",
        "]:\n",
        "    ensure_dir(d)\n",
        "\n",
        "print(\"âœ… Mounted Drive & Initialized Paths\")\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"FIX_ROOT    :\", FIX_ROOT)\n",
        "print(\"FRAMES_DIR  :\", FRAMES_DIR)\n",
        "print(\"PATCH_ROOT  :\", PATCH_ROOT)\n",
        "print(\"LABEL_ROOT  :\", LABEL_ROOT)\n",
        "print(\"EXP_ROOT    :\", EXP_ROOT)\n",
        "print(\"DEPLOY_ROOT :\", DEPLOY_ROOT)\n",
        "print(\"OUT_DIR     :\", OUT_DIR)\n",
        "\n",
        "# sanity checks\n",
        "print(\"\\n--- Sanity Checks ---\")\n",
        "print(\"PROJECT_ROOT exists?\", PROJECT_ROOT.exists())\n",
        "print(\"FIX_ROOT exists?    \", FIX_ROOT.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-mka5u6izvd"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CEK JUMLAH PATCH: TOTAL, BERLABEL, TIDAK DIPAKAI\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ROOT SESUAI NOTEBOOK ANDA\n",
        "ROOT = Path(\"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/DatasetTerbaruFIX\")\n",
        "\n",
        "# ---- FILE PATCH HASIL PREPROCESSING ----\n",
        "PATCH_ALL = ROOT / \"patches\" / \"meta\" / \"patches_all.csv\"\n",
        "\n",
        "# ---- CARI FILE LABEL FINAL (AUTO) ----\n",
        "LABEL_DIR = ROOT / \"labels\" / \"final\"\n",
        "label_files = sorted(LABEL_DIR.glob(\"patches_all_final_labeled*.csv\"))\n",
        "\n",
        "assert PATCH_ALL.exists(), f\"Tidak ditemukan: {PATCH_ALL}\"\n",
        "assert len(label_files) > 0, \"File label final tidak ditemukan di labels/final/\"\n",
        "\n",
        "PATCH_LABELED = label_files[-1]  # ambil yang terbaru\n",
        "\n",
        "# ---- LOAD DATA ----\n",
        "df_all = pd.read_csv(PATCH_ALL)\n",
        "df_labeled = pd.read_csv(PATCH_LABELED)\n",
        "\n",
        "# ---- HITUNG ----\n",
        "total_patch_after_preprocessing = len(df_all)\n",
        "total_patch_labeled = len(df_labeled)\n",
        "total_patch_unused = total_patch_after_preprocessing - total_patch_labeled\n",
        "\n",
        "# ---- OUTPUT RINGKAS ----\n",
        "summary = pd.DataFrame({\n",
        "    \"Jumlah Patch\": [\n",
        "        total_patch_after_preprocessing,\n",
        "        total_patch_labeled,\n",
        "        total_patch_unused\n",
        "    ]\n",
        "}, index=[\n",
        "    \"Total patch setelah preprocessing\",\n",
        "    \"Total patch berlabel\",\n",
        "    \"Total patch tidak dipakai\"\n",
        "])\n",
        "\n",
        "print(\"ðŸ“ File preprocessing :\", PATCH_ALL)\n",
        "print(\"ðŸ“ File label final   :\", PATCH_LABELED)\n",
        "print(\"\\nðŸ“Š Ringkasan Patch:\")\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFPSXeJIb7aX"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 0.2 â€” Imports, Seed, Global Config\n",
        "# ============================================\n",
        "import random\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# ML stack (TF/Keras assumed)\n",
        "import tensorflow as tf\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Global config (KUNCI)\n",
        "PATCH_SIZE = 128\n",
        "STRIDE     = 128\n",
        "ZOOM_RGB_TO_THERMAL = 2.2  # default zoom\n",
        "\n",
        "# Thermal normalization config (will be used in Tahap 3.3)\n",
        "THERM_P01 = 1.0\n",
        "THERM_P99 = 99.0\n",
        "\n",
        "# Canopy mask QC\n",
        "MIN_CANOPY_FRAC = 0.20\n",
        "\n",
        "# Runtime check\n",
        "print(\"âœ… Imports OK\")\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", len(tf.config.list_physical_devices('GPU')) > 0)\n",
        "\n",
        "print(\"\\n--- Global Config ---\")\n",
        "print(\"SEED =\", SEED)\n",
        "print(\"PATCH_SIZE =\", PATCH_SIZE, \"| STRIDE =\", STRIDE)\n",
        "print(\"ZOOM_RGB_TO_THERMAL =\", ZOOM_RGB_TO_THERMAL)\n",
        "print(\"THERM percentiles =\", (THERM_P01, THERM_P99))\n",
        "print(\"MIN_CANOPY_FRAC =\", MIN_CANOPY_FRAC)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCckQtV_b9v2"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 0.3 â€” Helper Functions (Core Utilities)\n",
        "#   - load RGB / thermal\n",
        "#   - center-crop + zoom + resize (RGB -> thermal grid)\n",
        "#   - patch grid + extraction (thermal-domain)\n",
        "# ============================================\n",
        "\n",
        "def read_rgb(path: str | Path) -> np.ndarray:\n",
        "    \"\"\"Read RGB image (png/jpg) as uint8 RGB.\"\"\"\n",
        "    img_bgr = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
        "    if img_bgr is None:\n",
        "        raise FileNotFoundError(f\"RGB not readable: {path}\")\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    return img_rgb\n",
        "\n",
        "def read_thermal_npy(path: str | Path) -> np.ndarray:\n",
        "    \"\"\"Read thermal .npy float32 matrix (H,W).\"\"\"\n",
        "    arr = np.load(str(path))\n",
        "    if arr.ndim != 2:\n",
        "        raise ValueError(f\"Thermal must be 2D (H,W), got shape {arr.shape} from {path}\")\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "def center_crop_zoom(img: np.ndarray, zoom: float) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Center crop with zoom factor > 1.\n",
        "    zoom=2 means crop to 1/2 width & height (then resize back later).\n",
        "    \"\"\"\n",
        "    if zoom <= 1.0:\n",
        "        return img\n",
        "    h, w = img.shape[:2]\n",
        "    new_w = int(round(w / zoom))\n",
        "    new_h = int(round(h / zoom))\n",
        "    x0 = (w - new_w) // 2\n",
        "    y0 = (h - new_h) // 2\n",
        "    crop = img[y0:y0+new_h, x0:x0+new_w]\n",
        "    return crop\n",
        "\n",
        "def align_rgb_to_thermal_grid(rgb_raw: np.ndarray, thermal_hw: tuple[int,int], zoom: float = 2.2) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Align RGB to thermal domain by center-crop + zoom then resize to thermal size.\n",
        "    thermal_hw: (H_th, W_th)\n",
        "    \"\"\"\n",
        "    H_th, W_th = thermal_hw\n",
        "    crop = center_crop_zoom(rgb_raw, zoom=zoom)\n",
        "    aligned = cv2.resize(crop, (W_th, H_th), interpolation=cv2.INTER_AREA)\n",
        "    return aligned\n",
        "\n",
        "def build_patch_grid(H: int, W: int, patch: int, stride: int) -> pd.DataFrame:\n",
        "    \"\"\"Return dataframe grid with row,col,x0,y0,x1,y1. Based on thermal domain.\"\"\"\n",
        "    rows = ((H - patch) // stride) + 1\n",
        "    cols = ((W - patch) // stride) + 1\n",
        "    recs = []\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            y0 = r * stride\n",
        "            x0 = c * stride\n",
        "            recs.append({\n",
        "                \"r\": r, \"c\": c,\n",
        "                \"x0\": x0, \"y0\": y0,\n",
        "                \"x1\": x0 + patch,\n",
        "                \"y1\": y0 + patch\n",
        "            })\n",
        "    return pd.DataFrame(recs), rows, cols\n",
        "\n",
        "def extract_patch(img: np.ndarray, x0: int, y0: int, patch: int) -> np.ndarray:\n",
        "    \"\"\"Crop patch for either RGB (H,W,3) or thermal (H,W).\"\"\"\n",
        "    if img.ndim == 3:\n",
        "        return img[y0:y0+patch, x0:x0+patch, :]\n",
        "    else:\n",
        "        return img[y0:y0+patch, x0:x0+patch]\n",
        "\n",
        "def safe_json_dump(obj, path: str | Path):\n",
        "    with open(str(path), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "print(\"âœ… Helper functions loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2B0yQkOFB2X"
      },
      "source": [
        "# Tahap 1 - Data Preprocessing & Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB3x0NQ9d2So"
      },
      "source": [
        "## 1.1 Audit Pairing RGBâ€“Thermal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljbloyEUeC7X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "RAW_RGB_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"rgb\"\n",
        "RAW_TH_DIR  = PROJECT_ROOT / \"data\" / \"raw\" / \"thermal\"\n",
        "\n",
        "if not RAW_RGB_DIR.exists():\n",
        "    raise FileNotFoundError(f\"RAW_RGB_DIR tidak ditemukan: {RAW_RGB_DIR}\")\n",
        "if not RAW_TH_DIR.exists():\n",
        "    raise FileNotFoundError(f\"RAW_TH_DIR tidak ditemukan: {RAW_TH_DIR}\")\n",
        "\n",
        "rgb_exts = (\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\")\n",
        "rgb_paths = sorted([p for p in RAW_RGB_DIR.iterdir() if p.is_file() and p.name.endswith(rgb_exts)])\n",
        "th_paths  = sorted([p for p in RAW_TH_DIR.iterdir() if p.is_file() and p.suffix.lower()==\".npy\"])\n",
        "\n",
        "print(\"RAW_RGB_DIR:\", RAW_RGB_DIR)\n",
        "print(\"RAW_TH_DIR :\", RAW_TH_DIR)\n",
        "print(\"\\n--- RAW COUNTS ---\")\n",
        "print(\"Total RGB files   :\", len(rgb_paths))\n",
        "print(\"Total Thermal npy :\", len(th_paths))\n",
        "\n",
        "def parse_frame_id_from_rgb_name(rgb_filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Konsisten dengan kode lama kamu:\n",
        "    - buang ekstensi\n",
        "    - kalau ada suffix '_V' di akhir stem -> buang\n",
        "    Contoh:\n",
        "    'DJI_..._0009_V.JPG' -> 'DJI_..._0009'\n",
        "    \"\"\"\n",
        "    base = rgb_filename\n",
        "    for ext in [\".JPG\", \".JPEG\", \".PNG\", \".jpg\", \".jpeg\", \".png\"]:\n",
        "        if base.endswith(ext):\n",
        "            base = base[:-len(ext)]\n",
        "            break\n",
        "    if base.endswith(\"_V\"):\n",
        "        return base[:-2]\n",
        "    return base\n",
        "\n",
        "def thermal_name_from_frame_id(frame_id: str) -> str:\n",
        "    return f\"{frame_id}_T.npy\"\n",
        "\n",
        "rows = []\n",
        "skipped_no_thermal = 0\n",
        "\n",
        "for rgb_p in rgb_paths:\n",
        "    frame_id = parse_frame_id_from_rgb_name(rgb_p.name)\n",
        "    th_name  = thermal_name_from_frame_id(frame_id)\n",
        "    th_p     = RAW_TH_DIR / th_name\n",
        "\n",
        "    status = \"OK_PAIR\" if th_p.exists() else \"NO_THERMAL\"\n",
        "    if status != \"OK_PAIR\":\n",
        "        skipped_no_thermal += 1\n",
        "\n",
        "    rows.append({\n",
        "        \"frame_id\": frame_id,\n",
        "        \"rgb_path_raw\": str(rgb_p),\n",
        "        \"thermal_path_raw\": str(th_p) if th_p.exists() else \"\",\n",
        "        \"status\": status\n",
        "    })\n",
        "\n",
        "audit_df = pd.DataFrame(rows)\n",
        "n_ok = int((audit_df[\"status\"]==\"OK_PAIR\").sum())\n",
        "n_skip = int((audit_df[\"status\"]!=\"OK_PAIR\").sum())\n",
        "\n",
        "print(\"\\n--- PAIRING AUDIT SUMMARY ---\")\n",
        "print(\"RGB scanned        :\", len(audit_df))\n",
        "print(\"Valid pairs        :\", n_ok)\n",
        "print(\"Skipped no thermal :\", n_skip)\n",
        "\n",
        "display(audit_df.head(5))\n",
        "display(audit_df[audit_df[\"status\"]!=\"OK_PAIR\"].head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVnzpOsoeHkL"
      },
      "source": [
        "## 1.2 Alignment & Save FIX Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FpXVlktd4Zv"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "ensure_dir(FRAMES_DIR)\n",
        "ensure_dir(RGB_ALIGNED_DIR)\n",
        "ensure_dir(THERMAL_DIR)\n",
        "\n",
        "pairs_df = audit_df[audit_df[\"status\"]==\"OK_PAIR\"].copy().reset_index(drop=True)\n",
        "if len(pairs_df) == 0:\n",
        "    raise RuntimeError(\"Tidak ada pasangan valid untuk diproses. Cek naming _V dan _T.npy.\")\n",
        "\n",
        "ZOOM_USED = float(ZOOM_RGB_TO_THERMAL)\n",
        "\n",
        "# sanity overlay output (opsional)\n",
        "SANITY_DIR = OUT_DIR / \"sanity_alignment\"\n",
        "ensure_dir(SANITY_DIR)\n",
        "SANITY_N = 6\n",
        "sanity_saved = 0\n",
        "\n",
        "ok = 0\n",
        "skipped_bad_read = 0\n",
        "fail_log = []\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "for i, row in pairs_df.iterrows():\n",
        "    frame_id = row[\"frame_id\"]\n",
        "    rgb_raw_path = Path(row[\"rgb_path_raw\"])\n",
        "    th_raw_path  = Path(row[\"thermal_path_raw\"])\n",
        "\n",
        "    try:\n",
        "        # load\n",
        "        rgb_raw = read_rgb(rgb_raw_path)            # RGB uint8\n",
        "        th      = read_thermal_npy(th_raw_path)     # float32 (H,W)\n",
        "        H_th, W_th = th.shape[:2]\n",
        "\n",
        "        # align\n",
        "        rgb_aligned = align_rgb_to_thermal_grid(rgb_raw, (H_th, W_th), zoom=ZOOM_USED)\n",
        "\n",
        "        # save (nama file DISAMAKAN dgn frame_id -> penting untuk tahap2 dst)\n",
        "        out_rgb = RGB_ALIGNED_DIR / f\"{frame_id}.png\"\n",
        "        out_th  = THERMAL_DIR / f\"{frame_id}.npy\"\n",
        "\n",
        "        cv2.imwrite(str(out_rgb), cv2.cvtColor(rgb_aligned, cv2.COLOR_RGB2BGR))\n",
        "        np.save(str(out_th), th.astype(np.float32))  # copy thermal, tetap grid thermal\n",
        "\n",
        "        # update df\n",
        "        pairs_df.loc[i, \"rgb_path_aligned\"] = str(out_rgb)\n",
        "        pairs_df.loc[i, \"thermal_path_fix\"] = str(out_th)\n",
        "        pairs_df.loc[i, \"zoom_used\"] = ZOOM_USED\n",
        "        pairs_df.loc[i, \"thermal_H\"] = int(H_th)\n",
        "        pairs_df.loc[i, \"thermal_W\"] = int(W_th)\n",
        "\n",
        "        ok += 1\n",
        "\n",
        "        # sanity save beberapa sample\n",
        "        if sanity_saved < SANITY_N:\n",
        "            th_vis = th.copy()\n",
        "            lo, hi = np.nanpercentile(th_vis, 2), np.nanpercentile(th_vis, 98)\n",
        "            th_vis = np.clip((th_vis - lo) / (hi - lo + 1e-6), 0, 1)\n",
        "            th_u8  = (th_vis * 255).astype(np.uint8)\n",
        "            th_cm  = cv2.applyColorMap(th_u8, cv2.COLORMAP_JET)\n",
        "            th_cm  = cv2.cvtColor(th_cm, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            side = np.concatenate([rgb_aligned, th_cm], axis=1)\n",
        "            cv2.imwrite(str(SANITY_DIR / f\"{frame_id}_rgb_vs_th.png\"),\n",
        "                        cv2.cvtColor(side, cv2.COLOR_RGB2BGR))\n",
        "            sanity_saved += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        skipped_bad_read += 1\n",
        "        fail_log.append((frame_id, str(e)))\n",
        "        continue\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "pairs_df[\"status_fix\"] = np.where(pairs_df[\"rgb_path_aligned\"].astype(str)!=\"\", \"OK_FIX\", \"FAIL_FIX\")\n",
        "\n",
        "print(\"\\n--- ALIGNMENT SUMMARY ---\")\n",
        "print(\"Zoom used            :\", ZOOM_USED)\n",
        "print(\"Pairs processed      :\", len(pairs_df))\n",
        "print(\"OK saved             :\", ok)\n",
        "print(\"Skipped bad read/load:\", skipped_bad_read)\n",
        "print(\"Sanity saved         :\", sanity_saved, \"->\", SANITY_DIR)\n",
        "print(f\"Elapsed              : {elapsed:.1f}s\")\n",
        "\n",
        "if fail_log:\n",
        "    print(\"\\nFirst 5 fails:\")\n",
        "    for fr, err in fail_log[:5]:\n",
        "        print(\"-\", fr, \"=>\", err)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT4LlKBTd96N"
      },
      "source": [
        "## 1.3 Save pairs_index.csv + Summary Statistik"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBKE610geNu5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def _exists(p):\n",
        "    try:\n",
        "        return Path(p).exists()\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "pairs_df[\"rgb_exists\"] = pairs_df[\"rgb_path_aligned\"].apply(_exists)\n",
        "pairs_df[\"th_exists\"]  = pairs_df[\"thermal_path_fix\"].apply(_exists)\n",
        "pairs_df[\"status_fix\"] = np.where(pairs_df[\"rgb_exists\"] & pairs_df[\"th_exists\"], \"OK_FIX\", \"FAIL_FIX\")\n",
        "\n",
        "pairs_ok = pairs_df[pairs_df[\"status_fix\"]==\"OK_FIX\"].copy().reset_index(drop=True)\n",
        "pairs_fail = pairs_df[pairs_df[\"status_fix\"]!=\"OK_FIX\"].copy().reset_index(drop=True)\n",
        "\n",
        "# simpan index (kolom ringkas tapi cukup untuk tahap 2 dst)\n",
        "pairs_index = pairs_ok[[\n",
        "    \"frame_id\",\n",
        "    \"rgb_path_raw\", \"thermal_path_raw\",\n",
        "    \"rgb_path_aligned\", \"thermal_path_fix\",\n",
        "    \"zoom_used\", \"thermal_H\", \"thermal_W\"\n",
        "]].copy()\n",
        "\n",
        "pairs_index.to_csv(PAIRS_CSV, index=False)\n",
        "\n",
        "# summary json\n",
        "PREPROCESS_SUMMARY_JSON = FRAMES_DIR / \"preprocess_summary.json\"\n",
        "summary = {\n",
        "    \"project_root\": str(PROJECT_ROOT),\n",
        "    \"fix_root\": str(FIX_ROOT),\n",
        "    \"raw_rgb_dir\": str(RAW_RGB_DIR),\n",
        "    \"raw_thermal_dir\": str(RAW_TH_DIR),\n",
        "    \"out_frames_dir\": str(FRAMES_DIR),\n",
        "    \"rgb_aligned_dir\": str(RGB_ALIGNED_DIR),\n",
        "    \"thermal_fix_dir\": str(THERMAL_DIR),\n",
        "    \"pairs_index_csv\": str(PAIRS_CSV),\n",
        "    \"counts\": {\n",
        "        \"rgb_raw_total\": int(len(audit_df)),\n",
        "        \"thermal_raw_total\": int(len(list(RAW_TH_DIR.glob('*.npy')))),\n",
        "        \"pairs_valid_before_fix\": int((audit_df[\"status\"]==\"OK_PAIR\").sum()),\n",
        "        \"pairs_skipped_no_thermal\": int((audit_df[\"status\"]!=\"OK_PAIR\").sum()),\n",
        "        \"pairs_fix_ok\": int(len(pairs_ok)),\n",
        "        \"pairs_fix_fail\": int(len(pairs_fail)),\n",
        "    },\n",
        "    \"alignment\": {\n",
        "        \"zoom_used\": float(ZOOM_USED),\n",
        "        \"thermal_hw_example\": [int(pairs_ok[\"thermal_H\"].iloc[0]) if len(pairs_ok) else None,\n",
        "                              int(pairs_ok[\"thermal_W\"].iloc[0]) if len(pairs_ok) else None]\n",
        "    }\n",
        "}\n",
        "safe_json_dump(summary, PREPROCESS_SUMMARY_JSON)\n",
        "\n",
        "print(\"âœ… Saved pairs_index.csv :\", PAIRS_CSV)\n",
        "print(\"âœ… Saved summary.json    :\", PREPROCESS_SUMMARY_JSON)\n",
        "\n",
        "print(\"\\n--- FINAL FIX STATS ---\")\n",
        "print(\"Valid FIX pairs :\", len(pairs_ok))\n",
        "print(\"Fail  FIX pairs :\", len(pairs_fail))\n",
        "print(\"RGB aligned files:\", len(list(RGB_ALIGNED_DIR.glob(\"*.png\"))))\n",
        "print(\"Thermal npy files:\", len(list(THERMAL_DIR.glob(\"*.npy\"))))\n",
        "\n",
        "display(pairs_index.head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40L0VjLooZEx"
      },
      "outputs": [],
      "source": [
        "print(rgb.shape, th.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1mYCYGgmNzp"
      },
      "source": [
        "## 1.4 Qality Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hfj972bQrzI"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# FULL PIPELINE: load -> align (crop+zoom) -> overlay -> patches\n",
        "# =========================================================\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Utils\n",
        "# -----------------------------\n",
        "def normalize_to_u8(x, p1=1, p99=99):\n",
        "    \"\"\"Normalize 2D float array to uint8 for visualization.\"\"\"\n",
        "    x = x.astype(np.float32)\n",
        "    lo, hi = np.nanpercentile(x, p1), np.nanpercentile(x, p99)\n",
        "    x = np.clip((x - lo) / (hi - lo + 1e-6), 0, 1)\n",
        "    return (x * 255).astype(np.uint8)\n",
        "\n",
        "def center_crop(img, crop_h, crop_w):\n",
        "    H, W = img.shape[:2]\n",
        "    y0 = (H - crop_h) // 2\n",
        "    x0 = (W - crop_w) // 2\n",
        "    return img[y0:y0+crop_h, x0:x0+crop_w], (x0, y0, crop_w, crop_h)\n",
        "\n",
        "def rgb_to_thermal_grid(rgb_bgr, th_shape_hw, zoom=2.2):\n",
        "    \"\"\"\n",
        "    RGB wide -> crop center (zoom-in) -> resize to thermal grid.\n",
        "    zoom bigger => crop smaller => RGB looks more zoomed-in.\n",
        "    \"\"\"\n",
        "    Hth, Wth = th_shape_hw\n",
        "    Hr, Wr = rgb_bgr.shape[:2]\n",
        "    crop_h = int(Hr / zoom)\n",
        "    crop_w = int(Wr / zoom)\n",
        "\n",
        "    rgb_crop, roi = center_crop(rgb_bgr, crop_h, crop_w)\n",
        "    rgb_aligned = cv2.resize(rgb_crop, (Wth, Hth), interpolation=cv2.INTER_LINEAR)\n",
        "    return rgb_aligned, roi\n",
        "\n",
        "def show_alignment(rgb_aligned_bgr, th_2d, title_suffix=\"\"):\n",
        "    \"\"\"3-panel plot: RGB aligned, Thermal, Overlay.\"\"\"\n",
        "    th_u8 = normalize_to_u8(th_2d)\n",
        "    rgb_show = cv2.cvtColor(rgb_aligned_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1,3,1); plt.imshow(rgb_show); plt.title(f\"RGB aligned{title_suffix}\"); plt.axis(\"off\")\n",
        "    plt.subplot(1,3,2); plt.imshow(th_u8, cmap=\"inferno\"); plt.title(f\"Thermal{title_suffix}\"); plt.axis(\"off\")\n",
        "    plt.subplot(1,3,3); plt.imshow(rgb_show); plt.imshow(th_u8, cmap=\"inferno\", alpha=0.35); plt.title(f\"Overlay{title_suffix}\"); plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def extract_paired_patches(rgb_aligned_bgr, th_2d, patch=128, stride=128):\n",
        "    \"\"\"\n",
        "    Extract paired patches (RGB + Thermal) from same grid.\n",
        "    Return:\n",
        "      rgb_patches: (N, patch, patch, 3) uint8\n",
        "      th_patches : (N, patch, patch) float32\n",
        "      meta       : list dict per patch\n",
        "    \"\"\"\n",
        "    H, W = th_2d.shape[:2]\n",
        "    rgb_patches, th_patches, meta = [], [], []\n",
        "\n",
        "    for y in range(0, H - patch + 1, stride):\n",
        "        for x in range(0, W - patch + 1, stride):\n",
        "            rgb_p = rgb_aligned_bgr[y:y+patch, x:x+patch]\n",
        "            th_p  = th_2d[y:y+patch, x:x+patch]\n",
        "\n",
        "            rgb_patches.append(rgb_p)\n",
        "            th_patches.append(th_p.astype(np.float32))\n",
        "\n",
        "            meta.append({\n",
        "                \"x\": x, \"y\": y,\n",
        "                \"patch\": patch,\n",
        "                \"row\": y // stride,\n",
        "                \"col\": x // stride\n",
        "            })\n",
        "\n",
        "    return np.stack(rgb_patches, axis=0), np.stack(th_patches, axis=0), meta\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Paths (EDIT THIS)\n",
        "# -----------------------------\n",
        "rgb_path = \"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/data/raw/rgb/DJI_20251223095741_0048_V.JPG\"\n",
        "th_path  = \"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/data/raw/thermal/DJI_20251223095741_0048_T.npy\"\n",
        "\n",
        "zoom = 2.2       # <-- ganti kalau perlu (2.0 / 2.2 / 2.4 / 2.6)\n",
        "patch = 128\n",
        "stride = 128\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Load data\n",
        "# -----------------------------\n",
        "rgb = cv2.imread(rgb_path)   # BGR\n",
        "if rgb is None:\n",
        "    raise ValueError(f\"RGB gagal dibaca. Cek path: {rgb_path}\")\n",
        "\n",
        "th = np.load(th_path)\n",
        "if th.ndim != 2:\n",
        "    raise ValueError(f\"Thermal npy harus 2D. Dapat shape={th.shape}\")\n",
        "\n",
        "print(\"RGB shape:\", rgb.shape, rgb.dtype)\n",
        "print(\"TH  shape:\", th.shape, th.dtype)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Align RGB -> thermal grid\n",
        "# -----------------------------\n",
        "rgb_aligned, roi = rgb_to_thermal_grid(rgb, th.shape[:2], zoom=zoom)\n",
        "print(\"ALIGN PARAMS:\", {\"zoom\": zoom, \"roi\": roi})\n",
        "print(\"RGB aligned shape:\", rgb_aligned.shape)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Sanity check overlay\n",
        "# -----------------------------\n",
        "show_alignment(rgb_aligned, th, title_suffix=f\" (zoom={zoom})\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Extract paired patches\n",
        "# -----------------------------\n",
        "rgb_patches, th_patches, meta = extract_paired_patches(rgb_aligned, th, patch=patch, stride=stride)\n",
        "print(\"RGB patches:\", rgb_patches.shape, rgb_patches.dtype)\n",
        "print(\"TH  patches:\", th_patches.shape, th_patches.dtype)\n",
        "print(\"Example meta[0]:\", meta[0])\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 7) (Optional) Visualize a few random paired patches\n",
        "# -----------------------------\n",
        "idxs = [0, len(meta)//2, len(meta)-1] if len(meta) >= 3 else list(range(len(meta)))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, idx in enumerate(idxs, 1):\n",
        "    rgb_p = cv2.cvtColor(rgb_patches[idx], cv2.COLOR_BGR2RGB)\n",
        "    th_u8 = normalize_to_u8(th_patches[idx])\n",
        "\n",
        "    ax = plt.subplot(2, len(idxs), i)\n",
        "    ax.imshow(rgb_p); ax.set_title(f\"RGB patch #{idx}\"); ax.axis(\"off\")\n",
        "\n",
        "    ax = plt.subplot(2, len(idxs), i + len(idxs))\n",
        "    ax.imshow(th_u8, cmap=\"inferno\"); ax.set_title(f\"TH patch #{idx}\"); ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsFzqWNAgvcc"
      },
      "source": [
        "# TAHAP 2 â€” Ekstraksi Patch & Konstruksi Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o2I9X2Jg0jA"
      },
      "source": [
        "## 2.1 Definisi Patch Grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jv21EBlg3uP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "if not PAIRS_CSV.exists():\n",
        "    raise FileNotFoundError(f\"pairs_index.csv tidak ditemukan: {PAIRS_CSV}. Jalankan TAHAP 1 dulu.\")\n",
        "\n",
        "pairs_index = pd.read_csv(PAIRS_CSV)\n",
        "print(\"âœ… Loaded pairs_index.csv :\", PAIRS_CSV)\n",
        "print(\"Total FIX pairs:\", len(pairs_index))\n",
        "\n",
        "# Ambil thermal size (asumsi konsisten)\n",
        "H_th = int(pairs_index[\"thermal_H\"].iloc[0]) if \"thermal_H\" in pairs_index.columns else None\n",
        "W_th = int(pairs_index[\"thermal_W\"].iloc[0]) if \"thermal_W\" in pairs_index.columns else None\n",
        "\n",
        "# Jika kolom thermal_H/W tidak ada (index lama), infer dari 1 file thermal\n",
        "if H_th is None or W_th is None:\n",
        "    sample_th = read_thermal_npy(pairs_index[\"thermal_path_fix\"].iloc[0])\n",
        "    H_th, W_th = sample_th.shape[:2]\n",
        "\n",
        "print(\"Thermal reference HW:\", (H_th, W_th))\n",
        "\n",
        "patch = int(PATCH_SIZE)\n",
        "stride = int(STRIDE)\n",
        "\n",
        "patch_grid_df, n_rows, n_cols = build_patch_grid(H_th, W_th, patch=patch, stride=stride)\n",
        "\n",
        "print(\"\\n--- PATCH GRID SUMMARY ---\")\n",
        "print(\"PATCH_SIZE:\", patch, \"| STRIDE:\", stride)\n",
        "print(\"Rows:\", n_rows, \"| Cols:\", n_cols, \"| Patches per frame:\", len(patch_grid_df))\n",
        "\n",
        "# save grid json (untuk deploy juga)\n",
        "ensure_dir(PATCH_META_DIR)\n",
        "grid_info = {\n",
        "    \"thermal_hw\": [H_th, W_th],\n",
        "    \"patch\": patch,\n",
        "    \"stride\": stride,\n",
        "    \"rows\": n_rows,\n",
        "    \"cols\": n_cols,\n",
        "    \"patches_per_frame\": int(len(patch_grid_df))\n",
        "}\n",
        "safe_json_dump(grid_info, PATCH_GRID_JSON)\n",
        "\n",
        "print(\"âœ… Saved patch_grid.json:\", PATCH_GRID_JSON)\n",
        "display(patch_grid_df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c0L7SFkg_S5"
      },
      "source": [
        "## 2.2 Extract Paired Patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3GdH8iUhADH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# --- toggle ---\n",
        "SAVE_PATCHES = True  # âœ… untuk kamu ini masuk akal (total patch kecil); ganti False kalau mau hemat I/O\n",
        "\n",
        "ensure_dir(PATCH_ROOT)\n",
        "ensure_dir(PATCH_META_DIR)\n",
        "if SAVE_PATCHES:\n",
        "    ensure_dir(PATCH_RGB_DIR)\n",
        "    ensure_dir(PATCH_TH_DIR)\n",
        "\n",
        "pairs_index = pd.read_csv(PAIRS_CSV)\n",
        "needed_cols = [\"frame_id\", \"rgb_path_aligned\", \"thermal_path_fix\"]\n",
        "for c in needed_cols:\n",
        "    if c not in pairs_index.columns:\n",
        "        raise ValueError(f\"Kolom '{c}' tidak ada di pairs_index.csv. Kolom tersedia: {list(pairs_index.columns)}\")\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "rows = []\n",
        "fail = 0\n",
        "ok_frames = 0\n",
        "total_patches = 0\n",
        "\n",
        "for _, r in pairs_index.iterrows():\n",
        "    frame_id = str(r[\"frame_id\"])\n",
        "    rgb_path = Path(r[\"rgb_path_aligned\"])\n",
        "    th_path  = Path(r[\"thermal_path_fix\"])\n",
        "\n",
        "    if (not rgb_path.exists()) or (not th_path.exists()):\n",
        "        fail += 1\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        rgb = read_rgb(rgb_path)          # (H,W,3) RGB aligned\n",
        "        th  = read_thermal_npy(th_path)   # (H,W) float32\n",
        "\n",
        "        # sanity: thermal domain should match rgb aligned size\n",
        "        if rgb.shape[0] != th.shape[0] or rgb.shape[1] != th.shape[1]:\n",
        "            raise ValueError(f\"Size mismatch rgb {rgb.shape[:2]} vs th {th.shape[:2]} for {frame_id}\")\n",
        "\n",
        "        H, W = th.shape[:2]\n",
        "\n",
        "        # build grid once per frame (but using same params)\n",
        "        grid_df, rr, cc = build_patch_grid(H, W, patch=PATCH_SIZE, stride=STRIDE)\n",
        "\n",
        "        for j, g in grid_df.iterrows():\n",
        "            r0, c0 = int(g[\"r\"]), int(g[\"c\"])\n",
        "            x0, y0 = int(g[\"x0\"]), int(g[\"y0\"])\n",
        "\n",
        "            patch_id = f\"{frame_id}_r{r0:03d}_c{c0:03d}\"\n",
        "\n",
        "            # extract\n",
        "            rgb_patch = extract_patch(rgb, x0, y0, PATCH_SIZE)\n",
        "            th_patch  = extract_patch(th,  x0, y0, PATCH_SIZE)\n",
        "\n",
        "            # optional save patch to disk\n",
        "            rgb_patch_path = \"\"\n",
        "            th_patch_path  = \"\"\n",
        "            if SAVE_PATCHES:\n",
        "                rgb_patch_path = str(PATCH_RGB_DIR / f\"{patch_id}.png\")\n",
        "                th_patch_path  = str(PATCH_TH_DIR  / f\"{patch_id}.npy\")\n",
        "\n",
        "                cv2.imwrite(rgb_patch_path, cv2.cvtColor(rgb_patch, cv2.COLOR_RGB2BGR))\n",
        "                np.save(th_patch_path, th_patch.astype(np.float32))\n",
        "\n",
        "            rows.append({\n",
        "                \"patch_id\": patch_id,\n",
        "                \"frame_id\": frame_id,\n",
        "                \"r\": r0,\n",
        "                \"c\": c0,\n",
        "                \"x0\": x0,\n",
        "                \"y0\": y0,\n",
        "                \"patch\": int(PATCH_SIZE),\n",
        "                \"stride\": int(STRIDE),\n",
        "\n",
        "                # source frame paths (selalu disimpan)\n",
        "                \"rgb_frame_path\": str(rgb_path),\n",
        "                \"thermal_frame_path\": str(th_path),\n",
        "\n",
        "                # optional patch paths\n",
        "                \"rgb_patch_path\": rgb_patch_path,\n",
        "                \"thermal_patch_path\": th_patch_path,\n",
        "            })\n",
        "\n",
        "            total_patches += 1\n",
        "\n",
        "        ok_frames += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        fail += 1\n",
        "        continue\n",
        "\n",
        "patches_all = pd.DataFrame(rows)\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "print(\"\\n--- PATCH EXTRACTION SUMMARY ---\")\n",
        "print(\"SAVE_PATCHES:\", SAVE_PATCHES)\n",
        "print(\"Frames OK    :\", ok_frames)\n",
        "print(\"Frames FAIL  :\", fail)\n",
        "print(\"Total patches:\", len(patches_all))\n",
        "print(f\"Elapsed      : {elapsed:.1f}s\")\n",
        "\n",
        "display(patches_all.head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcSSdfq_hLjT"
      },
      "source": [
        "## 2.3 Save patches_all.csv + Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca5yL0pThJSI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "ensure_dir(PATCH_META_DIR)\n",
        "\n",
        "# basic checks\n",
        "if len(patches_all) == 0:\n",
        "    raise RuntimeError(\"patches_all kosong. Cek pairs_index.csv dan folder frames/ output Tahap 1.\")\n",
        "\n",
        "# Save CSV\n",
        "patches_all.to_csv(PATCHES_ALL_CSV, index=False)\n",
        "\n",
        "# summary\n",
        "patch_extract_summary = {\n",
        "    \"pairs_index_csv\": str(PAIRS_CSV),\n",
        "    \"patches_all_csv\": str(PATCHES_ALL_CSV),\n",
        "    \"save_patches\": bool(SAVE_PATCHES),\n",
        "    \"patch\": int(PATCH_SIZE),\n",
        "    \"stride\": int(STRIDE),\n",
        "    \"counts\": {\n",
        "        \"frames_in_index\": int(len(pd.read_csv(PAIRS_CSV))),\n",
        "        \"patch_rows\": int(len(patches_all)),\n",
        "        \"unique_frames\": int(patches_all[\"frame_id\"].nunique()),\n",
        "        \"patches_per_frame_mode\": int(patches_all.groupby(\"frame_id\").size().mode().iloc[0]),\n",
        "    },\n",
        "    \"paths\": {\n",
        "        \"patch_root\": str(PATCH_ROOT),\n",
        "        \"patch_rgb_dir\": str(PATCH_RGB_DIR) if SAVE_PATCHES else \"\",\n",
        "        \"patch_th_dir\": str(PATCH_TH_DIR) if SAVE_PATCHES else \"\",\n",
        "        \"patch_meta_dir\": str(PATCH_META_DIR),\n",
        "        \"patch_grid_json\": str(PATCH_GRID_JSON)\n",
        "    }\n",
        "}\n",
        "\n",
        "PATCH_SUMMARY_JSON = PATCH_META_DIR / \"patch_extract_summary.json\"\n",
        "safe_json_dump(patch_extract_summary, PATCH_SUMMARY_JSON)\n",
        "\n",
        "print(\"âœ… Saved patches_all.csv         :\", PATCHES_ALL_CSV)\n",
        "print(\"âœ… Saved patch_extract_summary.json:\", PATCH_SUMMARY_JSON)\n",
        "\n",
        "print(\"\\n--- QUICK STATS ---\")\n",
        "print(\"Unique frames :\", patches_all['frame_id'].nunique())\n",
        "print(\"Total patches :\", len(patches_all))\n",
        "print(\"Label columns present? (belum) ->\", any(c.startswith(\"label\") or c==\"y\" for c in patches_all.columns))\n",
        "\n",
        "# Optional: show patch path completeness\n",
        "if SAVE_PATCHES:\n",
        "    n_rgb_paths = int((patches_all[\"rgb_patch_path\"].astype(str) != \"\").sum())\n",
        "    n_th_paths  = int((patches_all[\"thermal_patch_path\"].astype(str) != \"\").sum())\n",
        "    print(\"Saved RGB patch files rows:\", n_rgb_paths)\n",
        "    print(\"Saved TH  patch files rows:\", n_th_paths)\n",
        "\n",
        "display(patches_all.sample(5, random_state=SEED))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euEuR112ofZj"
      },
      "outputs": [],
      "source": [
        "patches_all.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiEKJd8vj3Bg"
      },
      "source": [
        "# TAHAP 3 â€” Feature Extraction & Labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZNGPORmj4BA"
      },
      "source": [
        "## 3.1 Template Pool Labeling Manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sZx9NAxj49s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "if not PATCHES_ALL_CSV.exists():\n",
        "    raise FileNotFoundError(f\"patches_all.csv tidak ditemukan: {PATCHES_ALL_CSV}. Jalankan TAHAP 2 dulu.\")\n",
        "\n",
        "ensure_dir(LABEL_GOLD_DIR)\n",
        "\n",
        "patches_all = pd.read_csv(PATCHES_ALL_CSV)\n",
        "print(\"âœ… Loaded patches_all.csv:\", PATCHES_ALL_CSV)\n",
        "print(\"Rows:\", len(patches_all), \"| Unique frames:\", patches_all[\"frame_id\"].nunique())\n",
        "\n",
        "# --- konfigurasi pool manual ---\n",
        "POOL_SIZE = 2000  # target pool manual (ubah sesuai kebutuhan)\n",
        "PER_FRAME_CAP = 20  # batasi max patch per frame agar tidak bias 1 frame\n",
        "\n",
        "# pastikan ada path rgb patch untuk memudahkan anotasi\n",
        "# kalau SAVE_PATCHES=False di Tahap 2, kita buat \"virtual view path\" dari rgb_frame_path + coords\n",
        "has_rgb_patch_files = \"rgb_patch_path\" in patches_all.columns and (patches_all[\"rgb_patch_path\"].astype(str) != \"\").any()\n",
        "\n",
        "df = patches_all.copy()\n",
        "\n",
        "# sampling: balanced coverage by frame\n",
        "pool_rows = []\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "for frame_id, g in df.groupby(\"frame_id\"):\n",
        "    g = g.sample(min(len(g), PER_FRAME_CAP), random_state=SEED)\n",
        "    pool_rows.append(g)\n",
        "\n",
        "pool_df = pd.concat(pool_rows, ignore_index=True)\n",
        "\n",
        "# kalau masih lebih besar dari POOL_SIZE -> sample lagi\n",
        "if len(pool_df) > POOL_SIZE:\n",
        "    pool_df = pool_df.sample(POOL_SIZE, random_state=SEED).reset_index(drop=True)\n",
        "else:\n",
        "    pool_df = pool_df.reset_index(drop=True)\n",
        "\n",
        "# template columns (manual annotation)\n",
        "template = pd.DataFrame({\n",
        "    \"patch_id\": pool_df[\"patch_id\"].astype(str),\n",
        "    \"frame_id\": pool_df[\"frame_id\"].astype(str),\n",
        "    \"r\": pool_df[\"r\"].astype(int),\n",
        "    \"c\": pool_df[\"c\"].astype(int),\n",
        "    \"x0\": pool_df[\"x0\"].astype(int),\n",
        "    \"y0\": pool_df[\"y0\"].astype(int),\n",
        "    # untuk anotasi cepat: pakai patch file kalau ada, kalau tidak pakai frame path (nanti dipotong on-the-fly)\n",
        "    \"rgb_view_path\": pool_df[\"rgb_patch_path\"].astype(str) if has_rgb_patch_files else pool_df[\"rgb_frame_path\"].astype(str),\n",
        "    \"label_manual\": \"\",          # isi: 0 atau 1\n",
        "    \"manual_conf\": \"\",           # opsional: low/med/high atau 0-1\n",
        "    \"annotator\": \"\",             # nama/initial\n",
        "    \"notes\": \"\"                  # catatan\n",
        "})\n",
        "\n",
        "LABEL_GOLD_TEMPLATE = LABEL_GOLD_DIR / \"labels_gold_template.csv\"\n",
        "template.to_csv(LABEL_GOLD_TEMPLATE, index=False)\n",
        "\n",
        "print(\"\\n--- GOLD POOL SUMMARY ---\")\n",
        "print(\"Template saved:\", LABEL_GOLD_TEMPLATE)\n",
        "print(\"Pool size:\", len(template))\n",
        "print(\"Has rgb patch files:\", has_rgb_patch_files)\n",
        "\n",
        "display(template.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su8azgTUxiyp"
      },
      "source": [
        "## 3.2 â€” UI Manual Labeling (RGB-only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRIpacjT7J4A"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 3.2 â€” UI Manual Labeling (GOLD) [autosave + resume]\n",
        "#   Input : labels_gold_template.csv\n",
        "#   Output: labels_gold.csv\n",
        "#   Buttons: Back | Save & Next | Skip | Cancel\n",
        "#   Columns output (minimal): patch_id, label_manual, notes\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from PIL import Image\n",
        "\n",
        "LABEL_TEMPLATE = LABEL_GOLD_DIR / \"labels_gold_template.csv\"\n",
        "LABEL_GOLD_OUT = LABEL_GOLD_DIR / \"labels_gold.csv\"\n",
        "\n",
        "if not LABEL_TEMPLATE.exists():\n",
        "    raise FileNotFoundError(f\"Template tidak ditemukan: {LABEL_TEMPLATE}. Jalankan TAHAP 3.1 dulu.\")\n",
        "\n",
        "pool = pd.read_csv(LABEL_TEMPLATE)\n",
        "pool[\"patch_id\"] = pool[\"patch_id\"].astype(str)\n",
        "\n",
        "# Output minimal sesuai preferensi kamu\n",
        "OUT_COLS = [\"patch_id\", \"label_manual\", \"notes\"]\n",
        "\n",
        "if not LABEL_GOLD_OUT.exists():\n",
        "    pd.DataFrame(columns=OUT_COLS).to_csv(LABEL_GOLD_OUT, index=False)\n",
        "\n",
        "done = pd.read_csv(LABEL_GOLD_OUT)\n",
        "done_ids = set(done[\"patch_id\"].astype(str).tolist()) if len(done) else set()\n",
        "\n",
        "before = len(pool)\n",
        "pool = pool[~pool[\"patch_id\"].isin(done_ids)].reset_index(drop=True)\n",
        "\n",
        "print(\"âœ… Template:\", LABEL_TEMPLATE, \"| rows:\", before)\n",
        "print(\"âœ… Output  :\", LABEL_GOLD_OUT)\n",
        "print(\"Already labeled:\", len(done_ids))\n",
        "print(\"Remaining:\", len(pool))\n",
        "\n",
        "idx = 0\n",
        "total = len(pool)\n",
        "is_cancelled = False\n",
        "\n",
        "title = widgets.HTML()\n",
        "img_out = widgets.Output()\n",
        "\n",
        "# Rubric (opsional)\n",
        "c1 = widgets.Checkbox(value=False, description=\"K1: warna stres (pucat/kekuningan/tidak merata)\")\n",
        "c2 = widgets.Checkbox(value=False, description=\"K2: kanopi tidak seragam (bercak/patchy)\")\n",
        "c3 = widgets.Checkbox(value=False, description=\"K3: kanopi jarang (celah banyak)\")\n",
        "c4 = widgets.Checkbox(value=False, description=\"K4: layu/kerusakan (daun abnormal/terkulai)\")\n",
        "\n",
        "score_lbl = widgets.HTML()\n",
        "auto_lbl  = widgets.HTML()\n",
        "\n",
        "radio = widgets.ToggleButtons(\n",
        "    options=[(\"Normal (0)\", 0), (\"Stress (1)\", 1)],\n",
        "    value=0,\n",
        "    description=\"Label:\"\n",
        ")\n",
        "\n",
        "note  = widgets.Text(value=\"\", placeholder=\"catatan opsional (blur, non-padi, ragu)\", description=\"Note:\")\n",
        "\n",
        "# Buttons sesuai request\n",
        "btn_back  = widgets.Button(description=\"Back\")\n",
        "btn_save  = widgets.Button(description=\"Save & Next\", button_style=\"success\")\n",
        "btn_skip  = widgets.Button(description=\"Skip\", button_style=\"warning\")\n",
        "btn_cancel= widgets.Button(description=\"Cancel\", button_style=\"danger\")\n",
        "\n",
        "msg = widgets.HTML()\n",
        "\n",
        "def compute_score():\n",
        "    score = int(c1.value) + int(c2.value) + int(c3.value) + int(c4.value)\n",
        "    auto  = 1 if score >= 2 else 0\n",
        "    return score, auto\n",
        "\n",
        "def reset_inputs():\n",
        "    c1.value=False; c2.value=False; c3.value=False; c4.value=False\n",
        "    note.value=\"\"\n",
        "    radio.value=0\n",
        "    msg.value=\"\"\n",
        "\n",
        "def append_and_autosave(rec):\n",
        "    df_old = pd.read_csv(LABEL_GOLD_OUT)\n",
        "    df_new = pd.concat([df_old, pd.DataFrame([rec])], ignore_index=True)\n",
        "    # jaga kolom\n",
        "    for c in OUT_COLS:\n",
        "        if c not in df_new.columns:\n",
        "            df_new[c] = \"\"\n",
        "    df_new = df_new[OUT_COLS]\n",
        "    df_new.to_csv(LABEL_GOLD_OUT, index=False)\n",
        "\n",
        "def render():\n",
        "    global idx, total, is_cancelled\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    if is_cancelled:\n",
        "        display(widgets.HTML(f\"<b>Dibatalkan.</b> Data aman tersimpan di: <code>{LABEL_GOLD_OUT}</code>\"))\n",
        "        return\n",
        "\n",
        "    if total == 0:\n",
        "        display(widgets.HTML(\"<b>Selesai.</b> Tidak ada patch tersisa (atau semuanya sudah dilabel).\"))\n",
        "        display(widgets.HTML(f\"Output: <code>{LABEL_GOLD_OUT}</code>\"))\n",
        "        return\n",
        "\n",
        "    idx = max(0, min(idx, total-1))\n",
        "    row = pool.iloc[idx]\n",
        "\n",
        "    patch_id = str(row[\"patch_id\"])\n",
        "    frame_id = str(row.get(\"frame_id\",\"\"))\n",
        "    r = row.get(\"r\",\"\"); c = row.get(\"c\",\"\")\n",
        "    p = str(row[\"rgb_view_path\"])\n",
        "\n",
        "    score, auto = compute_score()\n",
        "    title.value = f\"\"\"\n",
        "    <b>GOLD Labeling {idx+1}/{total}</b><br>\n",
        "    patch_id: <code>{patch_id}</code><br>\n",
        "    frame_id: <code>{frame_id}</code> | r,c=({r},{c})\n",
        "    \"\"\"\n",
        "\n",
        "    score_lbl.value = f\"<b>Rubric score:</b> {score} (auto>=2 â†’ Stress={auto})\"\n",
        "    auto_lbl.value  = f\"<b>Rubric suggestion:</b> <span style='font-size:16px'>{'Stress' if auto==1 else 'Normal'}</span>\"\n",
        "\n",
        "    img_out.clear_output()\n",
        "    with img_out:\n",
        "        try:\n",
        "            im = Image.open(p).convert(\"RGB\")\n",
        "            display(im)\n",
        "        except Exception as e:\n",
        "            display(widgets.HTML(f\"<b style='color:red'>Gagal load image:</b><br><code>{p}</code><br>{e}\"))\n",
        "\n",
        "    display(title)\n",
        "    display(img_out)\n",
        "    display(widgets.VBox([c1,c2,c3,c4,score_lbl,auto_lbl]))\n",
        "    display(widgets.VBox([radio, note]))\n",
        "    display(widgets.HBox([btn_back, btn_save, btn_skip, btn_cancel]))\n",
        "    display(msg)\n",
        "    display(widgets.HTML(f\"<small>Autosave ke Drive: <code>{LABEL_GOLD_OUT}</code></small>\"))\n",
        "\n",
        "def on_save(_):\n",
        "    global idx\n",
        "    if total == 0:\n",
        "        return\n",
        "\n",
        "    row = pool.iloc[idx]\n",
        "    patch_id = str(row[\"patch_id\"])\n",
        "\n",
        "    rec = {\n",
        "        \"patch_id\": patch_id,\n",
        "        \"label_manual\": int(radio.value),\n",
        "        \"notes\": note.value.strip()\n",
        "    }\n",
        "    append_and_autosave(rec)\n",
        "    msg.value = \"<span style='color:green'><b>Saved.</b> (autosaved)</span>\"\n",
        "    idx += 1\n",
        "    reset_inputs()\n",
        "    render()\n",
        "\n",
        "def on_skip(_):\n",
        "    global idx\n",
        "    if total == 0:\n",
        "        return\n",
        "\n",
        "    row = pool.iloc[idx]\n",
        "    patch_id = str(row[\"patch_id\"])\n",
        "    n = note.value.strip()\n",
        "\n",
        "    rec = {\n",
        "        \"patch_id\": patch_id,\n",
        "        \"label_manual\": \"\",  # kosong => dianggap tidak dilabel\n",
        "        \"notes\": (\"[SKIP] \" + n).strip() if n else \"[SKIP]\"\n",
        "    }\n",
        "    append_and_autosave(rec)\n",
        "    msg.value = \"<span style='color:orange'><b>Skipped.</b> (autosaved)</span>\"\n",
        "    idx += 1\n",
        "    reset_inputs()\n",
        "    render()\n",
        "\n",
        "def on_back(_):\n",
        "    global idx\n",
        "    idx -= 1\n",
        "    reset_inputs()\n",
        "    render()\n",
        "\n",
        "def on_cancel(_):\n",
        "    global is_cancelled\n",
        "    is_cancelled = True\n",
        "    render()\n",
        "\n",
        "btn_save.on_click(on_save)\n",
        "btn_skip.on_click(on_skip)\n",
        "btn_back.on_click(on_back)\n",
        "btn_cancel.on_click(on_cancel)\n",
        "\n",
        "render()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCKj6x34CLMR"
      },
      "outputs": [],
      "source": [
        "import shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "LABEL_GOLD_OUT = FIX_ROOT / \"labels\" / \"gold\" / \"labels_gold.csv\"\n",
        "bak = LABEL_GOLD_OUT.parent / f\"labels_gold_BACKUP_{time.strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "shutil.copy2(LABEL_GOLD_OUT, bak)\n",
        "print(\"âœ… Backup saved:\", bak)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvP3AOspCQn4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "LABEL_TEMPLATE = FIX_ROOT / \"labels\" / \"gold\" / \"labels_gold_template.csv\"\n",
        "LABEL_GOLD_OUT = FIX_ROOT / \"labels\" / \"gold\" / \"labels_gold.csv\"\n",
        "\n",
        "N_RESET = 700\n",
        "\n",
        "tpl = pd.read_csv(LABEL_TEMPLATE)\n",
        "gold = pd.read_csv(LABEL_GOLD_OUT)\n",
        "\n",
        "# patch_id yg mau diulang = 1..700 sesuai urutan template\n",
        "reset_ids = set(tpl[\"patch_id\"].astype(str).iloc[:N_RESET].tolist())\n",
        "\n",
        "before = len(gold)\n",
        "gold[\"patch_id\"] = gold[\"patch_id\"].astype(str)\n",
        "\n",
        "# hapus label untuk patch_id tsb\n",
        "gold2 = gold[~gold[\"patch_id\"].isin(reset_ids)].copy()\n",
        "\n",
        "after = len(gold2)\n",
        "gold2.to_csv(LABEL_GOLD_OUT, index=False)\n",
        "\n",
        "print(\"âœ… RESET DONE\")\n",
        "print(\"Template:\", LABEL_TEMPLATE)\n",
        "print(\"Labels  :\", LABEL_GOLD_OUT)\n",
        "print(\"Reset N patch:\", len(reset_ids))\n",
        "print(\"Rows before:\", before, \"| after:\", after, \"| removed:\", before-after)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRfOm8x2FbBT"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 3.2R â€” UI Manual Labeling (RE-LABEL 1â€“700)\n",
        "#   Purpose : ulang labeling untuk template index 1..700 (urut template)\n",
        "#   Input   : labels_gold_template.csv  (sumber urutan)\n",
        "#   Output  : labels_gold_relabel_1_700.csv  (BARU, urut 1..700)\n",
        "#   Notes   : autosave + resume + tombol Back/Save/Skip/Cancel\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from PIL import Image\n",
        "\n",
        "# ---- CONFIG ----\n",
        "LABEL_TEMPLATE = LABEL_GOLD_DIR / \"labels_gold_template.csv\"\n",
        "LABEL_BASE_GOLD = LABEL_GOLD_DIR / \"labels_gold.csv\"  # hanya untuk info/cek (optional)\n",
        "LABEL_OUT = LABEL_GOLD_DIR / \"labels_gold_relabel_1_700.csv\"\n",
        "\n",
        "N_RELAB = 700  # relabel template #1..#700 (1-based)\n",
        "\n",
        "if not LABEL_TEMPLATE.exists():\n",
        "    raise FileNotFoundError(f\"Template tidak ditemukan: {LABEL_TEMPLATE}. Jalankan TAHAP 3.1 dulu.\")\n",
        "\n",
        "tpl = pd.read_csv(LABEL_TEMPLATE)\n",
        "tpl[\"patch_id\"] = tpl[\"patch_id\"].astype(str)\n",
        "\n",
        "# Ambil hanya 1..700 sesuai urutan template\n",
        "pool = tpl.iloc[:N_RELAB].copy().reset_index(drop=True)\n",
        "\n",
        "# Output minimal sesuai preferensi kamu\n",
        "OUT_COLS = [\"patch_id\", \"label_manual\", \"notes\"]\n",
        "\n",
        "# ----- RESUME LOGIC -----\n",
        "# Kalau file output belum ada, buat baru (kosong)\n",
        "if not LABEL_OUT.exists():\n",
        "    pd.DataFrame(columns=OUT_COLS).to_csv(LABEL_OUT, index=False)\n",
        "\n",
        "done = pd.read_csv(LABEL_OUT)\n",
        "done[\"patch_id\"] = done[\"patch_id\"].astype(str) if len(done) else done.get(\"patch_id\", pd.Series([], dtype=str))\n",
        "done_ids = set(done[\"patch_id\"].tolist()) if len(done) else set()\n",
        "\n",
        "# Filter patch yang sudah dilabel di sesi relabel ini\n",
        "pool = pool[~pool[\"patch_id\"].isin(done_ids)].reset_index(drop=True)\n",
        "\n",
        "print(\"âœ… TEMPLATE:\", LABEL_TEMPLATE)\n",
        "print(\"âœ… OUTPUT  :\", LABEL_OUT, \"(file relabel khusus)\")\n",
        "print(\"Relabel target:\", N_RELAB, \"patch (template #1..#700)\")\n",
        "print(\"Already relabeled:\", len(done_ids))\n",
        "print(\"Remaining:\", len(pool))\n",
        "if len(pool) > 0:\n",
        "    print(\"First remaining:\", pool.iloc[0][\"patch_id\"])\n",
        "    print(\"Last remaining :\", pool.iloc[-1][\"patch_id\"])\n",
        "\n",
        "# ---- UI STATE ----\n",
        "idx = 0\n",
        "total = len(pool)\n",
        "is_cancelled = False\n",
        "\n",
        "title = widgets.HTML()\n",
        "img_out = widgets.Output()\n",
        "\n",
        "# Rubric (opsional, bantu konsistensi)\n",
        "c1 = widgets.Checkbox(value=False, description=\"K1: warna stres (pucat/kekuningan/tidak merata)\")\n",
        "c2 = widgets.Checkbox(value=False, description=\"K2: kanopi tidak seragam (bercak/patchy)\")\n",
        "c3 = widgets.Checkbox(value=False, description=\"K3: kanopi jarang (celah banyak)\")\n",
        "c4 = widgets.Checkbox(value=False, description=\"K4: layu/kerusakan (daun abnormal/terkulai)\")\n",
        "\n",
        "score_lbl = widgets.HTML()\n",
        "auto_lbl  = widgets.HTML()\n",
        "\n",
        "radio = widgets.ToggleButtons(\n",
        "    options=[(\"Normal (0)\", 0), (\"Stress (1)\", 1)],\n",
        "    value=0,\n",
        "    description=\"Label:\"\n",
        ")\n",
        "\n",
        "note  = widgets.Text(value=\"\", placeholder=\"catatan opsional (blur, non-padi, ragu)\", description=\"Note:\")\n",
        "\n",
        "# Buttons\n",
        "btn_back   = widgets.Button(description=\"Back\")\n",
        "btn_save   = widgets.Button(description=\"Save & Next\", button_style=\"success\")\n",
        "btn_skip   = widgets.Button(description=\"Skip\", button_style=\"warning\")\n",
        "btn_cancel = widgets.Button(description=\"Cancel\", button_style=\"danger\")\n",
        "\n",
        "msg = widgets.HTML()\n",
        "\n",
        "def compute_score():\n",
        "    score = int(c1.value) + int(c2.value) + int(c3.value) + int(c4.value)\n",
        "    auto  = 1 if score >= 2 else 0\n",
        "    return score, auto\n",
        "\n",
        "def reset_inputs():\n",
        "    c1.value=False; c2.value=False; c3.value=False; c4.value=False\n",
        "    note.value=\"\"\n",
        "    radio.value=0\n",
        "    msg.value=\"\"\n",
        "\n",
        "def append_and_autosave(rec):\n",
        "    df_old = pd.read_csv(LABEL_OUT)\n",
        "    df_new = pd.concat([df_old, pd.DataFrame([rec])], ignore_index=True)\n",
        "\n",
        "    # jaga kolom (minimal)\n",
        "    for c in OUT_COLS:\n",
        "        if c not in df_new.columns:\n",
        "            df_new[c] = \"\"\n",
        "    df_new = df_new[OUT_COLS]\n",
        "    df_new.to_csv(LABEL_OUT, index=False)\n",
        "\n",
        "def render():\n",
        "    global idx, total, is_cancelled\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    if is_cancelled:\n",
        "        display(widgets.HTML(f\"<b>Dibatalkan.</b> Data aman tersimpan di: <code>{LABEL_OUT}</code>\"))\n",
        "        return\n",
        "\n",
        "    if total == 0:\n",
        "        display(widgets.HTML(\"<b>Selesai.</b> Tidak ada patch tersisa untuk relabel 1â€“700 (semua sudah dilabel ulang).\"))\n",
        "        display(widgets.HTML(f\"Output: <code>{LABEL_OUT}</code>\"))\n",
        "        # info tambahan (optional)\n",
        "        if LABEL_BASE_GOLD.exists():\n",
        "            display(widgets.HTML(f\"<small>Catatan: file label utama tetap: <code>{LABEL_BASE_GOLD}</code></small>\"))\n",
        "        return\n",
        "\n",
        "    idx = max(0, min(idx, total-1))\n",
        "    row = pool.iloc[idx]\n",
        "\n",
        "    patch_id = str(row[\"patch_id\"])\n",
        "    frame_id = str(row.get(\"frame_id\",\"\"))\n",
        "    r = row.get(\"r\",\"\"); c = row.get(\"c\",\"\")\n",
        "    p = str(row.get(\"rgb_view_path\",\"\"))\n",
        "\n",
        "    score, auto = compute_score()\n",
        "    title.value = f\"\"\"\n",
        "    <b>RE-LABEL 1â€“700: {idx+1}/{total}</b><br>\n",
        "    patch_id: <code>{patch_id}</code><br>\n",
        "    frame_id: <code>{frame_id}</code> | r,c=({r},{c})\n",
        "    \"\"\"\n",
        "\n",
        "    score_lbl.value = f\"<b>Rubric score:</b> {score} (auto>=2 â†’ Stress={auto})\"\n",
        "    auto_lbl.value  = f\"<b>Rubric suggestion:</b> <span style='font-size:16px'>{'Stress' if auto==1 else 'Normal'}</span>\"\n",
        "\n",
        "    img_out.clear_output()\n",
        "    with img_out:\n",
        "        try:\n",
        "            im = Image.open(p).convert(\"RGB\")\n",
        "            display(im)\n",
        "        except Exception as e:\n",
        "            display(widgets.HTML(f\"<b style='color:red'>Gagal load image:</b><br><code>{p}</code><br>{e}\"))\n",
        "\n",
        "    display(title)\n",
        "    display(img_out)\n",
        "    display(widgets.VBox([c1,c2,c3,c4,score_lbl,auto_lbl]))\n",
        "    display(widgets.VBox([radio, note]))\n",
        "    display(widgets.HBox([btn_back, btn_save, btn_skip, btn_cancel]))\n",
        "    display(msg)\n",
        "    display(widgets.HTML(f\"<small>Autosave ke Drive: <code>{LABEL_OUT}</code></small>\"))\n",
        "\n",
        "def on_save(_):\n",
        "    global idx\n",
        "    if total == 0:\n",
        "        return\n",
        "\n",
        "    row = pool.iloc[idx]\n",
        "    patch_id = str(row[\"patch_id\"])\n",
        "\n",
        "    rec = {\n",
        "        \"patch_id\": patch_id,\n",
        "        \"label_manual\": int(radio.value),\n",
        "        \"notes\": note.value.strip()\n",
        "    }\n",
        "    append_and_autosave(rec)\n",
        "    msg.value = \"<span style='color:green'><b>Saved.</b> (autosaved)</span>\"\n",
        "    idx += 1\n",
        "    reset_inputs()\n",
        "    render()\n",
        "\n",
        "def on_skip(_):\n",
        "    global idx\n",
        "    if total == 0:\n",
        "        return\n",
        "\n",
        "    row = pool.iloc[idx]\n",
        "    patch_id = str(row[\"patch_id\"])\n",
        "    n = note.value.strip()\n",
        "\n",
        "    rec = {\n",
        "        \"patch_id\": patch_id,\n",
        "        \"label_manual\": \"\",  # kosong => dianggap tidak dilabel (skip)\n",
        "        \"notes\": (\"[SKIP] \" + n).strip() if n else \"[SKIP]\"\n",
        "    }\n",
        "    append_and_autosave(rec)\n",
        "    msg.value = \"<span style='color:orange'><b>Skipped.</b> (autosaved)</span>\"\n",
        "    idx += 1\n",
        "    reset_inputs()\n",
        "    render()\n",
        "\n",
        "def on_back(_):\n",
        "    global idx\n",
        "    idx -= 1\n",
        "    reset_inputs()\n",
        "    render()\n",
        "\n",
        "def on_cancel(_):\n",
        "    global is_cancelled\n",
        "    is_cancelled = True\n",
        "    render()\n",
        "\n",
        "btn_save.on_click(on_save)\n",
        "btn_skip.on_click(on_skip)\n",
        "btn_back.on_click(on_back)\n",
        "btn_cancel.on_click(on_cancel)\n",
        "\n",
        "render()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywZjN_mwAfYn"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# MERGE RELABEL 1â€“700 -> labels_gold.csv\n",
        "# + SORT hasil akhir mengikuti urutan labels_gold_template.csv\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "import shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "LABEL_TEMPLATE = LABEL_GOLD_DIR / \"labels_gold_template.csv\"\n",
        "LABEL_MAIN     = LABEL_GOLD_DIR / \"labels_gold.csv\"\n",
        "LABEL_RELAB    = LABEL_GOLD_DIR / \"labels_gold_relabel_1_700.csv\"\n",
        "\n",
        "N_RELAB = 700  # harus sama seperti sesi relabel\n",
        "\n",
        "# ---- cek file ----\n",
        "for p in [LABEL_TEMPLATE, LABEL_MAIN, LABEL_RELAB]:\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"File tidak ditemukan: {p}\")\n",
        "\n",
        "# ---- backup labels_gold.csv dulu ----\n",
        "bak = LABEL_MAIN.parent / f\"labels_gold_BACKUP_before_merge_{time.strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "shutil.copy2(LABEL_MAIN, bak)\n",
        "print(\"âœ… Backup saved:\", bak)\n",
        "\n",
        "# ---- load ----\n",
        "tpl = pd.read_csv(LABEL_TEMPLATE)\n",
        "tpl[\"patch_id\"] = tpl[\"patch_id\"].astype(str)\n",
        "\n",
        "main = pd.read_csv(LABEL_MAIN)\n",
        "main[\"patch_id\"] = main[\"patch_id\"].astype(str)\n",
        "\n",
        "relab = pd.read_csv(LABEL_RELAB)\n",
        "relab[\"patch_id\"] = relab[\"patch_id\"].astype(str)\n",
        "\n",
        "# ---- patch_id target = template #1..#700 ----\n",
        "target_ids = set(tpl[\"patch_id\"].iloc[:N_RELAB].tolist())\n",
        "\n",
        "# ---- validasi: relabel harus mencakup patch target ----\n",
        "relab_ids = set(relab[\"patch_id\"].tolist())\n",
        "missing = sorted(list(target_ids - relab_ids))\n",
        "if len(missing) > 0:\n",
        "    print(\"âš ï¸ WARNING: Ada patch target yang belum ada di relabel file (belum kelabel / belum tersentuh):\", len(missing))\n",
        "    print(\"Contoh missing:\", missing[:5])\n",
        "    print(\"Tetap lanjut merge, tapi patch missing akan tetap memakai label lama (kalau ada) / kosong.\")\n",
        "else:\n",
        "    print(\"âœ… Relabel coverage OK: semua patch 1â€“700 ada di relabel file.\")\n",
        "\n",
        "# ---- samakan kolom: pakai kolom minimal yang kamu pakai (patch_id, label_manual, notes)\n",
        "# Jika di main ada kolom lain, biarkan tetap ada (kita merge kolom minimal lalu pertahankan sisanya).\n",
        "needed_cols = [\"patch_id\", \"label_manual\", \"notes\"]\n",
        "for c in needed_cols:\n",
        "    if c not in main.columns:\n",
        "        main[c] = \"\"\n",
        "    if c not in relab.columns:\n",
        "        relab[c] = \"\"\n",
        "\n",
        "# ---- 1) buang dulu record 1â€“700 dari main ----\n",
        "before_main = len(main)\n",
        "main_keep = main[~main[\"patch_id\"].isin(target_ids)].copy()\n",
        "removed = before_main - len(main_keep)\n",
        "\n",
        "# ---- 2) gabungkan main_keep + relab (relab jadi sumber terbaru untuk 1â€“700) ----\n",
        "merged = pd.concat([main_keep, relab[needed_cols]], ignore_index=True)\n",
        "\n",
        "# ---- 3) jika ada duplikat patch_id (harusnya tidak), keep yang terakhir (relab) ----\n",
        "merged = merged.drop_duplicates(subset=[\"patch_id\"], keep=\"last\").copy()\n",
        "\n",
        "print(\"\\n--- MERGE SUMMARY ---\")\n",
        "print(\"Main rows before :\", before_main)\n",
        "print(\"Removed 1â€“700     :\", removed)\n",
        "print(\"Relabel rows add  :\", len(relab))\n",
        "print(\"Merged rows now   :\", len(merged))\n",
        "\n",
        "# ---- 4) Sort sesuai urutan template ----\n",
        "# Buat mapping patch_id -> urutan di template\n",
        "tpl_order = {pid: i for i, pid in enumerate(tpl[\"patch_id\"].astype(str).tolist())}\n",
        "\n",
        "def order_key(pid: str):\n",
        "    # patch yang ada di template akan diurutkan sesuai template\n",
        "    # patch yang tidak ada di template taruh di bawah (akhir)\n",
        "    return tpl_order.get(pid, 10**12)\n",
        "\n",
        "merged[\"_order\"] = merged[\"patch_id\"].astype(str).map(order_key)\n",
        "\n",
        "# sort by template order\n",
        "merged_sorted = merged.sort_values([\"_order\", \"patch_id\"], ascending=[True, True]).drop(columns=[\"_order\"])\n",
        "\n",
        "# ---- 5) simpan balik ke labels_gold.csv ----\n",
        "merged_sorted.to_csv(LABEL_MAIN, index=False)\n",
        "\n",
        "print(\"\\nâœ… DONE: labels_gold.csv sudah digabung + diurutkan sesuai template\")\n",
        "print(\"Saved:\", LABEL_MAIN)\n",
        "\n",
        "# sanity check: print first 5, last 5 patch_id\n",
        "print(\"\\nFirst 5 patch_id in labels_gold.csv:\")\n",
        "print(merged_sorted[\"patch_id\"].head(5).tolist())\n",
        "print(\"\\nLast 5 patch_id in labels_gold.csv:\")\n",
        "print(merged_sorted[\"patch_id\"].tail(5).tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWAzuy1okJpW"
      },
      "source": [
        "## 3.3 â€” Canopy Masking & Thermal Feature Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "farb5ajmkLCS"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 3.3 â€” Canopy Masking & Thermal Feature Prep\n",
        "#   SAFE VERSION: progress + autosave + resume\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "THERM_DIR = FIX_ROOT / \"features\" / \"thermal\"\n",
        "ensure_dir(THERM_DIR)\n",
        "\n",
        "THERM_FEAT_CSV = THERM_DIR / \"thermal_features.csv\"\n",
        "PATCHES_ENRICHED_CSV = FIX_ROOT / \"patches\" / \"meta\" / \"patches_all_enriched.csv\"\n",
        "\n",
        "MIN_CANOPY_FRAC = 0.25\n",
        "BATCH_SAVE = 100   # autosave tiap 100 patch\n",
        "\n",
        "def load_rgb_patch_u8(p):\n",
        "    img = cv2.imread(str(p), cv2.IMREAD_COLOR)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(p)\n",
        "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def load_th_patch(p):\n",
        "    arr = np.load(str(p)).astype(np.float32)\n",
        "    if arr.ndim != 2:\n",
        "        raise ValueError(arr.shape)\n",
        "    return arr\n",
        "\n",
        "def canopy_mask_v2_from_rgb(rgb_u8):\n",
        "    rgb = rgb_u8.astype(np.float32)\n",
        "    r, g, b = rgb[...,0], rgb[...,1], rgb[...,2]\n",
        "    exg = 2*g - r - b\n",
        "\n",
        "    lo, hi = np.percentile(exg, 2), np.percentile(exg, 98)\n",
        "    exg_norm = np.clip((exg - lo) / (hi - lo + 1e-6), 0, 1)\n",
        "\n",
        "    hsv = cv2.cvtColor(rgb_u8, cv2.COLOR_RGB2HSV)\n",
        "    H,S,V = hsv[...,0], hsv[...,1], hsv[...,2]\n",
        "    green = (H>=25)&(H<=95)&(S>=30)&(V>=30)\n",
        "\n",
        "    mask = green & (exg_norm > 0.35)\n",
        "\n",
        "    k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))\n",
        "    mask = cv2.morphologyEx(mask.astype(np.uint8)*255, cv2.MORPH_OPEN, k, 1)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, k, 2)\n",
        "\n",
        "    return mask > 0\n",
        "\n",
        "\n",
        "# ---- LOAD PATCH META ----\n",
        "patches = pd.read_csv(PATCHES_ALL_CSV)\n",
        "patches[\"patch_id\"] = patches[\"patch_id\"].astype(str)\n",
        "print(\"Total patches:\", len(patches))\n",
        "\n",
        "# ---- RESUME LOGIC ----\n",
        "if THERM_FEAT_CSV.exists():\n",
        "    done = pd.read_csv(THERM_FEAT_CSV)\n",
        "    done_ids = set(done[\"patch_id\"].astype(str).tolist())\n",
        "    print(\"Resume mode: already processed:\", len(done_ids))\n",
        "else:\n",
        "    done = pd.DataFrame()\n",
        "    done_ids = set()\n",
        "    print(\"Fresh run\")\n",
        "\n",
        "todo = patches[~patches[\"patch_id\"].isin(done_ids)].reset_index(drop=True)\n",
        "print(\"Remaining to process:\", len(todo))\n",
        "\n",
        "rows = []\n",
        "bad = 0\n",
        "\n",
        "# ---- MAIN LOOP ----\n",
        "for i, rr in enumerate(tqdm(todo.itertuples(), total=len(todo))):\n",
        "    pid = rr.patch_id\n",
        "    try:\n",
        "        rgb = load_rgb_patch_u8(rr.rgb_patch_path)\n",
        "        th  = load_th_patch(rr.thermal_patch_path)\n",
        "\n",
        "        mask = canopy_mask_v2_from_rgb(rgb)\n",
        "        canopy_frac = float(mask.mean())\n",
        "\n",
        "        th_can = th.copy()\n",
        "        th_can[~mask] = np.nan\n",
        "\n",
        "        if np.isfinite(np.nanmean(th_can)):\n",
        "            tc_mean = float(np.nanmean(th_can))\n",
        "            tc_p90  = float(np.nanpercentile(th_can, 90))\n",
        "            tc_p95  = float(np.nanpercentile(th_can, 95))\n",
        "        else:\n",
        "            tc_mean = tc_p90 = tc_p95 = np.nan\n",
        "\n",
        "        rows.append({\n",
        "            \"patch_id\": pid,\n",
        "            \"canopy_frac\": canopy_frac,\n",
        "            \"tc_mean_canopy\": tc_mean,\n",
        "            \"tc_p90_canopy\": tc_p90,\n",
        "            \"tc_p95_canopy\": tc_p95,\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        bad += 1\n",
        "        continue\n",
        "\n",
        "    # ---- AUTOSAVE ----\n",
        "    if len(rows) >= BATCH_SAVE:\n",
        "        df_new = pd.DataFrame(rows)\n",
        "        df_all = pd.concat([done, df_new], ignore_index=True)\n",
        "        df_all.to_csv(THERM_FEAT_CSV, index=False)\n",
        "        done = df_all\n",
        "        rows = []\n",
        "        print(f\"ðŸ’¾ autosave @ {len(done)} rows\")\n",
        "\n",
        "# ---- FINAL SAVE (sisa) ----\n",
        "if rows:\n",
        "    df_new = pd.DataFrame(rows)\n",
        "    df_all = pd.concat([done, df_new], ignore_index=True)\n",
        "    df_all.to_csv(THERM_FEAT_CSV, index=False)\n",
        "    done = df_all\n",
        "\n",
        "print(\"âœ… Saved thermal features:\", THERM_FEAT_CSV)\n",
        "print(\"Rows:\", len(done), \"| Bad:\", bad)\n",
        "\n",
        "# ---- QC + ENRICH ----\n",
        "done[\"qc_canopy_ok\"] = done[\"canopy_frac\"] >= MIN_CANOPY_FRAC\n",
        "\n",
        "enriched = patches.merge(\n",
        "    done[[\"patch_id\",\"canopy_frac\",\"qc_canopy_ok\"]],\n",
        "    on=\"patch_id\", how=\"left\"\n",
        ")\n",
        "enriched[\"keep_for_model\"] = enriched[\"qc_canopy_ok\"].fillna(False)\n",
        "\n",
        "enriched.to_csv(PATCHES_ENRICHED_CSV, index=False)\n",
        "\n",
        "print(\"âœ… Saved enriched patches:\", PATCHES_ENRICHED_CSV)\n",
        "print(enriched[\"keep_for_model\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOEZezkhjDm4"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# EVIDENCE CELL â€” Canopy Masking Proof (Patch-level)\n",
        "#   Show: RGB, Mask, Thermal raw, Thermal masked, Overlay + stats\n",
        "#   Optional: save PNG evidence to Drive\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# --------- CONFIG ----------\n",
        "PATCHES_ALL_CSV = FIX_ROOT / \"patches\" / \"meta\" / \"patches_all.csv\"\n",
        "EVID_DIR = FIX_ROOT / \"reports\" / \"evidence_canopy_mask\"\n",
        "ensure_dir(EVID_DIR)\n",
        "\n",
        "N_SHOW = 6                 # tampilkan berapa contoh\n",
        "SAVE_PNG = True            # True = simpan ke Drive\n",
        "SEED_LOCAL = 42\n",
        "\n",
        "# Kalau kamu mau pilih patch_id spesifik, isi list ini (kalau kosong -> random)\n",
        "SELECT_PATCH_IDS = []  # contoh: [\"DJI_..._r000_c000\", \"DJI_..._r001_c003\"]\n",
        "\n",
        "# --------- FUNCTIONS ----------\n",
        "def load_rgb_patch_u8(p: str) -> np.ndarray:\n",
        "    img = cv2.imread(str(p), cv2.IMREAD_COLOR)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(p)\n",
        "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def load_th_patch(p: str) -> np.ndarray:\n",
        "    arr = np.load(str(p)).astype(np.float32)\n",
        "    if arr.ndim != 2:\n",
        "        raise ValueError(f\"thermal patch not 2D: {arr.shape}\")\n",
        "    return arr\n",
        "\n",
        "def canopy_mask_v2_from_rgb(rgb_u8: np.ndarray) -> np.ndarray:\n",
        "    rgb = rgb_u8.astype(np.float32)\n",
        "    r, g, b = rgb[...,0], rgb[...,1], rgb[...,2]\n",
        "    exg = 2*g - r - b\n",
        "\n",
        "    lo = np.percentile(exg, 2)\n",
        "    hi = np.percentile(exg, 98)\n",
        "    exg_norm = (exg - lo) / (hi - lo + 1e-6)\n",
        "    exg_norm = np.clip(exg_norm, 0, 1)\n",
        "\n",
        "    hsv = cv2.cvtColor(rgb_u8, cv2.COLOR_RGB2HSV)\n",
        "    H = hsv[...,0]; S = hsv[...,1]; V = hsv[...,2]\n",
        "    green_hsv = (H >= 25) & (H <= 95) & (S >= 30) & (V >= 30)\n",
        "    green_exg = exg_norm > 0.35\n",
        "\n",
        "    mask = green_hsv & green_exg\n",
        "\n",
        "    # morph clean\n",
        "    mask_u8 = (mask.astype(np.uint8) * 255)\n",
        "    k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
        "    mask_u8 = cv2.morphologyEx(mask_u8, cv2.MORPH_OPEN, k, iterations=1)\n",
        "    mask_u8 = cv2.morphologyEx(mask_u8, cv2.MORPH_CLOSE, k, iterations=2)\n",
        "    return mask_u8 > 0\n",
        "\n",
        "def compute_thermal_stats(th: np.ndarray, mask: np.ndarray):\n",
        "    th_can = th.copy()\n",
        "    th_can[~mask] = np.nan\n",
        "    canopy_frac = float(mask.mean())\n",
        "    if np.isfinite(np.nanmean(th_can)):\n",
        "        tc_mean = float(np.nanmean(th_can))\n",
        "        tc_p90  = float(np.nanpercentile(th_can, 90))\n",
        "        tc_p95  = float(np.nanpercentile(th_can, 95))\n",
        "    else:\n",
        "        tc_mean = tc_p90 = tc_p95 = np.nan\n",
        "    return canopy_frac, tc_mean, tc_p90, tc_p95, th_can\n",
        "\n",
        "def overlay_mask_on_rgb(rgb: np.ndarray, mask: np.ndarray, alpha=0.35):\n",
        "    # overlay hijau pada area mask\n",
        "    overlay = rgb.copy().astype(np.float32)\n",
        "    green = np.zeros_like(overlay); green[...,1] = 255\n",
        "    overlay[mask] = (1-alpha)*overlay[mask] + alpha*green[mask]\n",
        "    return overlay.astype(np.uint8)\n",
        "\n",
        "# --------- LOAD META ----------\n",
        "df = pd.read_csv(PATCHES_ALL_CSV)\n",
        "df[\"patch_id\"] = df[\"patch_id\"].astype(str)\n",
        "\n",
        "if SELECT_PATCH_IDS:\n",
        "    show_df = df[df[\"patch_id\"].isin(SELECT_PATCH_IDS)].copy()\n",
        "    if len(show_df) == 0:\n",
        "        raise ValueError(\"SELECT_PATCH_IDS tidak ditemukan di patches_all.csv\")\n",
        "else:\n",
        "    show_df = df.sample(n=min(N_SHOW, len(df)), random_state=SEED_LOCAL).copy()\n",
        "\n",
        "show_df = show_df.reset_index(drop=True)\n",
        "\n",
        "print(\"Showing:\", len(show_df), \"patches\")\n",
        "print(\"Evidence save dir:\", EVID_DIR)\n",
        "\n",
        "# --------- VISUALIZE ----------\n",
        "for i, rr in show_df.iterrows():\n",
        "    pid = rr[\"patch_id\"]\n",
        "    rgbp = rr[\"rgb_patch_path\"]\n",
        "    thp  = rr[\"thermal_patch_path\"]\n",
        "\n",
        "    rgb = load_rgb_patch_u8(rgbp)\n",
        "    th  = load_th_patch(thp)\n",
        "    mask = canopy_mask_v2_from_rgb(rgb)\n",
        "    canopy_frac, tc_mean, tc_p90, tc_p95, th_can = compute_thermal_stats(th, mask)\n",
        "    rgb_ov = overlay_mask_on_rgb(rgb, mask, alpha=0.35)\n",
        "\n",
        "    # plot\n",
        "    fig = plt.figure(figsize=(14, 8))\n",
        "    fig.suptitle(f\"{pid} | canopy_frac={canopy_frac:.3f} | Tc_mean={tc_mean:.2f} | p90={tc_p90:.2f} | p95={tc_p95:.2f}\", fontsize=12)\n",
        "\n",
        "    ax1 = plt.subplot(2,3,1); ax1.imshow(rgb); ax1.set_title(\"RGB patch\"); ax1.axis(\"off\")\n",
        "    ax2 = plt.subplot(2,3,2); ax2.imshow(mask, cmap=\"gray\"); ax2.set_title(\"Canopy mask\"); ax2.axis(\"off\")\n",
        "    ax3 = plt.subplot(2,3,3); ax3.imshow(rgb_ov); ax3.set_title(\"RGB + mask overlay\"); ax3.axis(\"off\")\n",
        "\n",
        "    ax4 = plt.subplot(2,3,4); ax4.imshow(th, cmap=\"inferno\"); ax4.set_title(\"Thermal raw\"); ax4.axis(\"off\")\n",
        "    ax5 = plt.subplot(2,3,5); ax5.imshow(th_can, cmap=\"inferno\"); ax5.set_title(\"Thermal masked (canopy only)\"); ax5.axis(\"off\")\n",
        "    ax6 = plt.subplot(2,3,6); ax6.hist(th[np.isfinite(th)].ravel(), bins=30); ax6.set_title(\"Thermal raw histogram\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # save evidence PNG (optional)\n",
        "    if SAVE_PNG:\n",
        "        out_png = EVID_DIR / f\"{pid}_evidence.png\"\n",
        "        fig.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
        "        plt.close(fig)\n",
        "        print(\"âœ… Saved:\", out_png)\n",
        "\n",
        "print(\"DONE. Evidence ready for report.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILWhOl5wj-9l"
      },
      "source": [
        "## 3.4 Auto-labeling + Merge Rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUv2KRaAniJD"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CHECK GOLD LABEL SUMMARY\n",
        "#   - total labeled rows\n",
        "#   - normal / stress / skip / empty\n",
        "#   - optional: cek coverage terhadap template (jika ada)\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "GOLD_CSV = FIX_ROOT / \"labels\" / \"gold\" / \"labels_gold.csv\"\n",
        "TEMPLATE_CSV = FIX_ROOT / \"labels\" / \"gold\" / \"labels_gold_template.csv\"  # optional\n",
        "\n",
        "if not GOLD_CSV.exists():\n",
        "    raise FileNotFoundError(f\"Tidak ditemukan: {GOLD_CSV}\")\n",
        "\n",
        "gold = pd.read_csv(GOLD_CSV)\n",
        "print(\"âœ… Loaded:\", GOLD_CSV)\n",
        "print(\"Rows:\", len(gold))\n",
        "print(\"Columns:\", list(gold.columns))\n",
        "\n",
        "# pastikan patch_id string\n",
        "if \"patch_id\" in gold.columns:\n",
        "    gold[\"patch_id\"] = gold[\"patch_id\"].astype(str)\n",
        "\n",
        "# label_manual bisa kosong / string / angka -> normalize\n",
        "if \"label_manual\" not in gold.columns:\n",
        "    raise ValueError(\"Kolom 'label_manual' tidak ada di labels_gold.csv\")\n",
        "\n",
        "gold[\"label_manual_num\"] = pd.to_numeric(gold[\"label_manual\"], errors=\"coerce\")\n",
        "\n",
        "# definisi skip:\n",
        "# - label_manual kosong/NaN -> dianggap skip/empty\n",
        "# - atau notes mengandung \"[SKIP]\"\n",
        "notes = gold[\"notes\"].fillna(\"\").astype(str) if \"notes\" in gold.columns else pd.Series([\"\"]*len(gold))\n",
        "is_skip_note = notes.str.contains(r\"\\[SKIP\\]\", case=False, regex=True)\n",
        "\n",
        "is_empty_label = gold[\"label_manual_num\"].isna()\n",
        "is_stress = gold[\"label_manual_num\"] == 1\n",
        "is_normal = gold[\"label_manual_num\"] == 0\n",
        "\n",
        "# hitung\n",
        "n_total = len(gold)\n",
        "n_stress = int(is_stress.sum())\n",
        "n_normal = int(is_normal.sum())\n",
        "n_empty = int(is_empty_label.sum())\n",
        "\n",
        "# jika kamu pakai tombol skip, biasanya label_manual kosong + notes [SKIP]\n",
        "n_skip = int((is_empty_label | is_skip_note).sum())\n",
        "n_empty_no_skipflag = int((is_empty_label & ~is_skip_note).sum())\n",
        "\n",
        "# unik patch\n",
        "n_unique = gold[\"patch_id\"].nunique() if \"patch_id\" in gold.columns else np.nan\n",
        "n_dup = int(n_total - n_unique) if n_unique is not np.nan else 0\n",
        "\n",
        "print(\"\\n--- GOLD LABEL SUMMARY ---\")\n",
        "print(\"Total rows (saved):\", n_total)\n",
        "print(\"Unique patch_id    :\", n_unique)\n",
        "print(\"Duplicates         :\", n_dup)\n",
        "\n",
        "print(\"\\nLabel counts:\")\n",
        "print(\"  Stress (1)       :\", n_stress)\n",
        "print(\"  Normal (0)       :\", n_normal)\n",
        "print(\"  Empty label      :\", n_empty)\n",
        "print(\"  Skip (empty OR [SKIP]) :\", n_skip)\n",
        "print(\"  Empty but no [SKIP]    :\", n_empty_no_skipflag)\n",
        "\n",
        "# sanity: hanya label valid (0/1)\n",
        "valid = gold[gold[\"label_manual_num\"].isin([0,1])].copy()\n",
        "print(\"\\nValid labeled (0/1):\", len(valid))\n",
        "if len(valid) > 0:\n",
        "    print(\"Valid distribution:\")\n",
        "    print(valid[\"label_manual_num\"].value_counts().rename({0:\"Normal(0)\",1:\"Stress(1)\"}))\n",
        "\n",
        "# optional: coverage vs template\n",
        "if TEMPLATE_CSV.exists() and \"patch_id\" in gold.columns:\n",
        "    tpl = pd.read_csv(TEMPLATE_CSV)\n",
        "    tpl[\"patch_id\"] = tpl[\"patch_id\"].astype(str)\n",
        "\n",
        "    tpl_ids = set(tpl[\"patch_id\"].tolist())\n",
        "    gold_ids = set(gold[\"patch_id\"].tolist())\n",
        "\n",
        "    in_tpl = len(gold_ids & tpl_ids)\n",
        "    out_tpl = len(gold_ids - tpl_ids)\n",
        "\n",
        "    print(\"\\n--- COVERAGE VS TEMPLATE ---\")\n",
        "    print(\"Template rows:\", len(tpl))\n",
        "    print(\"Gold patch_id in template:\", in_tpl)\n",
        "    print(\"Gold patch_id NOT in template:\", out_tpl)\n",
        "\n",
        "    # progress relabel 1..700 (jika template memang 2000 pool)\n",
        "    first700 = set(tpl[\"patch_id\"].iloc[:700].tolist())\n",
        "    labeled_first700 = len(gold_ids & first700)\n",
        "    print(\"Labeled among template #1..#700:\", labeled_first700, \"/ 700\")\n",
        "else:\n",
        "    print(\"\\n(Info) Template tidak ditemukan, skip coverage check:\", TEMPLATE_CSV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDc1kj1yToow"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "\n",
        "# ---------- PATHS (sesuaikan kalau strukturmu beda) ----------\n",
        "ENRICHED_CSV   = FIX_ROOT / \"patches\" / \"meta\" / \"patches_all_enriched.csv\"         # dari 3.3\n",
        "THERM_FEAT_CSV = FIX_ROOT / \"features\" / \"thermal\" / \"thermal_features.csv\"        # dari 3.3\n",
        "GOLD_CSV       = FIX_ROOT / \"labels\" / \"gold\" / \"labels_gold.csv\"                  # dari 3.2\n",
        "SILVER_DIR     = FIX_ROOT / \"labels\" / \"silver\"\n",
        "SILVER_AUTO_CSV= SILVER_DIR / \"labels_silver_auto.csv\"                             # output autolabel confident (akan dibuat jika belum ada)\n",
        "BASEMODEL_PATH = SILVER_DIR / \"rgb_baseline_for_autolabel.keras\"                   # model baseline RGB (akan dibuat jika belum ada)\n",
        "\n",
        "FINAL_DIR      = FIX_ROOT / \"labels\" / \"final\"\n",
        "FINAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUT_FINAL_CSV  = FINAL_DIR / \"patches_all_final_labeled.csv\"\n",
        "\n",
        "# ---------- PARAMS ----------\n",
        "USE_KEEP_FOR_MODEL = True     # Opsi A: hanya patch padi\n",
        "IMG_SIZE = 128\n",
        "BATCH = 64\n",
        "EPOCHS = 6\n",
        "SEED = 42\n",
        "\n",
        "# Auto-confident thresholds (ketat)\n",
        "THR_POS = 0.95\n",
        "THR_NEG = 0.05\n",
        "\n",
        "# Fallback thermal-relative threshold (Â°C) untuk semua sisanya\n",
        "DELTA_C = 0.5\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "def ensure_exists(p: Path, name=\"file\"):\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"{name} tidak ditemukan: {p}\")\n",
        "\n",
        "def make_ds_rgb(paths, labels=None, training=False):\n",
        "    def _load_rgb(p, y=None):\n",
        "        img = tf.io.read_file(p)\n",
        "        # patch RGB kamu PNG hasil extract; kalau JPG juga aman decode_jpeg\n",
        "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "        img = tf.cast(img, tf.float32) / 255.0\n",
        "        if y is None:\n",
        "            return img\n",
        "        return img, tf.cast(y, tf.float32)\n",
        "\n",
        "    if labels is None:\n",
        "        ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "        ds = ds.map(lambda p: _load_rgb(p, None), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "        ds = ds.map(lambda p, y: _load_rgb(p, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if training:\n",
        "        ds = ds.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
        "\n",
        "    return ds.batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "def build_rgb_baseline():\n",
        "    base = tf.keras.applications.MobileNetV2(\n",
        "        include_top=False, weights=\"imagenet\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        "    )\n",
        "    base.trainable = False\n",
        "    inp = tf.keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    x = tf.keras.applications.mobilenet_v2.preprocess_input(inp*255.0)\n",
        "    x = base(x, training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = tf.keras.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[tf.keras.metrics.AUC(name=\"roc_auc\"), tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\")]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ---------- LOAD REQUIRED ----------\n",
        "ensure_exists(ENRICHED_CSV, \"ENRICHED_CSV\")\n",
        "ensure_exists(THERM_FEAT_CSV, \"THERM_FEAT_CSV\")\n",
        "ensure_exists(GOLD_CSV, \"GOLD_CSV\")\n",
        "\n",
        "df = pd.read_csv(ENRICHED_CSV)\n",
        "df[\"patch_id\"] = df[\"patch_id\"].astype(str)\n",
        "\n",
        "feat = pd.read_csv(THERM_FEAT_CSV)\n",
        "feat[\"patch_id\"] = feat[\"patch_id\"].astype(str)\n",
        "\n",
        "gold = pd.read_csv(GOLD_CSV)\n",
        "gold[\"patch_id\"] = gold[\"patch_id\"].astype(str)\n",
        "gold[\"label_manual_num\"] = pd.to_numeric(gold[\"label_manual\"], errors=\"coerce\")  # 0/1 atau NaN\n",
        "\n",
        "# ---------- WORK PATCHES (Opsi A) ----------\n",
        "if USE_KEEP_FOR_MODEL and \"keep_for_model\" in df.columns:\n",
        "    work = df[df[\"keep_for_model\"] == True].copy()\n",
        "else:\n",
        "    work = df.copy()\n",
        "\n",
        "print(\"All patches:\", len(df), \"| Work patches (padi):\", len(work))\n",
        "\n",
        "# merge manual\n",
        "work = work.merge(gold[[\"patch_id\",\"label_manual_num\"]], on=\"patch_id\", how=\"left\")\n",
        "manual_valid = work[\"label_manual_num\"].isin([0,1]).sum()\n",
        "\n",
        "print(\"Manual labeled in work (valid 0/1):\", int(manual_valid))\n",
        "\n",
        "# ---------- TRAIN / LOAD RGB BASELINE FOR AUTO-LABEL ----------\n",
        "# train only from manual valid (0/1)\n",
        "train_gold = work[work[\"label_manual_num\"].isin([0,1])].copy()\n",
        "train_gold[\"y\"] = train_gold[\"label_manual_num\"].astype(int)\n",
        "\n",
        "print(\"\\n--- GOLD (work) distribution ---\")\n",
        "print(train_gold[\"y\"].value_counts())\n",
        "\n",
        "if len(train_gold) < 200:\n",
        "    print(\"âš ï¸ WARNING: GOLD valid < 200, autolabel bisa kurang stabil.\")\n",
        "\n",
        "# load model if exists else train\n",
        "if BASEMODEL_PATH.exists():\n",
        "    model = tf.keras.models.load_model(BASEMODEL_PATH)\n",
        "    print(\"\\nâœ… Loaded baseline model:\", BASEMODEL_PATH)\n",
        "else:\n",
        "    print(\"\\nTraining baseline RGB model for autolabel...\")\n",
        "    paths = train_gold[\"rgb_patch_path\"].astype(str).tolist()\n",
        "    y = train_gold[\"y\"].astype(int).tolist()\n",
        "\n",
        "    n = len(paths)\n",
        "    idxs = np.arange(n)\n",
        "    np.random.shuffle(idxs)\n",
        "    cut = max(1, int(0.85*n))\n",
        "    tr_idx, va_idx = idxs[:cut], idxs[cut:]\n",
        "\n",
        "    ds_tr = make_ds_rgb([paths[i] for i in tr_idx], [y[i] for i in tr_idx], training=True)\n",
        "    ds_va = make_ds_rgb([paths[i] for i in va_idx], [y[i] for i in va_idx], training=False)\n",
        "\n",
        "    model = build_rgb_baseline()\n",
        "    model.fit(ds_tr, validation_data=ds_va, epochs=EPOCHS, verbose=1)\n",
        "\n",
        "    SILVER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    model.save(BASEMODEL_PATH)\n",
        "    print(\"âœ… Saved baseline model:\", BASEMODEL_PATH)\n",
        "\n",
        "# ---------- PREDICT ALL WORK PATCHES ----------\n",
        "all_paths = work[\"rgb_patch_path\"].astype(str).tolist()\n",
        "ds_all = make_ds_rgb(all_paths, labels=None, training=False)\n",
        "probs = model.predict(ds_all, verbose=1).reshape(-1)\n",
        "\n",
        "work[\"p_auto_rgb\"] = probs\n",
        "work[\"label_auto\"] = np.nan\n",
        "work.loc[work[\"p_auto_rgb\"] >= THR_POS, \"label_auto\"] = 1\n",
        "work.loc[work[\"p_auto_rgb\"] <= THR_NEG, \"label_auto\"] = 0\n",
        "work[\"auto_confident\"] = work[\"label_auto\"].notna()\n",
        "\n",
        "# save SILVER confident only (optional artifact)\n",
        "silver = work[work[\"auto_confident\"]][[\"patch_id\",\"label_auto\",\"p_auto_rgb\"]].copy()\n",
        "silver.to_csv(SILVER_AUTO_CSV, index=False)\n",
        "print(\"âœ… Saved silver auto confident:\", SILVER_AUTO_CSV, \"| rows:\", len(silver))\n",
        "\n",
        "# ---------- THERMAL-RELATIVE FALLBACK ----------\n",
        "# join thermal feature (tc_mean_canopy) + compute tc_rel per frame\n",
        "work = work.merge(feat[[\"patch_id\",\"tc_mean_canopy\"]], on=\"patch_id\", how=\"left\")\n",
        "\n",
        "# safety: frame_id must exist\n",
        "if \"frame_id\" not in work.columns:\n",
        "    raise ValueError(\"Kolom frame_id tidak ditemukan di enriched meta. Pastikan patches_all.csv/enriched menyimpan frame_id.\")\n",
        "\n",
        "work[\"tc_med_frame\"] = work.groupby(\"frame_id\")[\"tc_mean_canopy\"].transform(\"median\")\n",
        "work[\"tc_rel\"] = work[\"tc_mean_canopy\"] - work[\"tc_med_frame\"]\n",
        "\n",
        "# ---------- MERGE LABELS: manual > auto > fallback ----------\n",
        "work[\"y_stage1\"] = work[\"label_manual_num\"]\n",
        "mask_stage1_nan = work[\"y_stage1\"].isna()\n",
        "work.loc[mask_stage1_nan, \"y_stage1\"] = work.loc[mask_stage1_nan, \"label_auto\"]\n",
        "\n",
        "# fallback for remaining NaN\n",
        "mask_need_fallback = work[\"y_stage1\"].isna()\n",
        "\n",
        "# fallback rule: tc_rel >= DELTA_C => Stress else Normal\n",
        "# if tc_rel NaN (should be rare) => default Normal (0) to avoid injecting stress noise\n",
        "fallback = (work.loc[mask_need_fallback, \"tc_rel\"] >= DELTA_C).astype(float)\n",
        "fallback = fallback.fillna(0.0)\n",
        "\n",
        "work.loc[mask_need_fallback, \"y_fallback\"] = fallback\n",
        "work[\"y_final\"] = work[\"y_stage1\"]\n",
        "work.loc[work[\"y_final\"].isna(), \"y_final\"] = work.loc[work[\"y_final\"].isna(), \"y_fallback\"]\n",
        "\n",
        "# label source\n",
        "work[\"label_source\"] = np.where(work[\"label_manual_num\"].isin([0,1]), \"manual\",\n",
        "                        np.where(work[\"label_auto\"].notna(), \"auto_confident\", \"fallback_thermal\"))\n",
        "\n",
        "# ---------- FINAL CHECKS ----------\n",
        "n_total = len(work)\n",
        "n_nan = int(pd.isna(work[\"y_final\"]).sum())\n",
        "print(\"\\n--- FINAL LABEL SUMMARY (Work patches only / Opsi A) ---\")\n",
        "print(\"Total work patches:\", n_total)\n",
        "print(\"NaN y_final:\", n_nan, \"(harus 0)\")\n",
        "\n",
        "print(\"\\nLabel distribution (y_final):\")\n",
        "print(work[\"y_final\"].value_counts(dropna=False))\n",
        "\n",
        "print(\"\\nLabel source counts:\")\n",
        "print(work[\"label_source\"].value_counts())\n",
        "\n",
        "# ---------- SAVE FINAL DATASET ----------\n",
        "work.to_csv(OUT_FINAL_CSV, index=False)\n",
        "print(\"\\nâœ… Saved FINAL labeled dataset:\", OUT_FINAL_CSV)\n",
        "print(\"Columns include: patch_id, frame_id, rgb_patch_path, thermal_patch_path, keep_for_model, canopy_frac, tc_mean_canopy, tc_rel, p_auto_rgb, y_final, label_source\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glVbRtNtwTWY"
      },
      "source": [
        "# TAHAP 4 â€” Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-J-h6QNwXy-"
      },
      "source": [
        "## 4.1 Dataset Builder + Splits (GroupKFold by frame_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spFt6zaoVun3"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 4.1 â€” Dataset Builder + Splits (GroupKFold)\n",
        "# - Load master final labeled dataset\n",
        "# - Build df_train\n",
        "# - Create folds with GroupKFold (group = frame_id)\n",
        "# - Save fold assignment to CSV (reproducible)\n",
        "# - Build tf.data dual-input loader:\n",
        "#     X_rgb: (128,128,3)\n",
        "#     X_th : (128,128,1) thermal masked+normalized (mask from RGB patch)\n",
        "#     y    : 0/1\n",
        "# - Print sanity checks (counts, imbalance, leakage)\n",
        "# ============================================\n",
        "\n",
        "import os, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "# ---------- PATHS ----------\n",
        "MASTER_FINAL = FIX_ROOT / \"labels\" / \"final\" / \"patches_all_final_labeled.csv\"  # master (debug-friendly)\n",
        "assert MASTER_FINAL.exists(), f\"Not found: {MASTER_FINAL}\"\n",
        "\n",
        "FOLD_DIR = FIX_ROOT / \"models\" / \"folds\"\n",
        "FOLD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FOLDS_CSV = FOLD_DIR / \"patch_folds.csv\"\n",
        "\n",
        "# ---------- PARAMS ----------\n",
        "IMG_SIZE = 128\n",
        "BATCH = 64\n",
        "N_SPLITS = 5\n",
        "SEED = 42\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ---------- LOAD ----------\n",
        "df = pd.read_csv(MASTER_FINAL)\n",
        "print(\"âœ… Loaded:\", MASTER_FINAL)\n",
        "print(\"Rows:\", len(df), \"| Unique frames:\", df[\"frame_id\"].nunique())\n",
        "\n",
        "need = [\"patch_id\",\"frame_id\",\"rgb_patch_path\",\"thermal_patch_path\",\"y_final\"]\n",
        "miss = [c for c in need if c not in df.columns]\n",
        "if miss:\n",
        "    raise ValueError(f\"Kolom wajib hilang di master final: {miss}\")\n",
        "\n",
        "# Pastikan y_final 0/1\n",
        "df[\"y_final\"] = pd.to_numeric(df[\"y_final\"], errors=\"coerce\").astype(\"Int64\")\n",
        "if df[\"y_final\"].isna().any():\n",
        "    raise ValueError(\"Masih ada NaN di y_final. Master final harus full-labeled untuk Opsi A.\")\n",
        "\n",
        "# ---------- RECONSTRUCT label_source (kalau tidak ada) ----------\n",
        "if \"label_source\" not in df.columns:\n",
        "    # logic: manual if label_manual_num valid; else auto_confident if label_auto exists; else fallback_thermal\n",
        "    if \"label_manual_num\" in df.columns:\n",
        "        manual_mask = df[\"label_manual_num\"].isin([0,1])\n",
        "    else:\n",
        "        manual_mask = pd.Series(False, index=df.index)\n",
        "\n",
        "    if \"label_auto\" in df.columns:\n",
        "        auto_mask = df[\"label_auto\"].notna()\n",
        "    else:\n",
        "        auto_mask = pd.Series(False, index=df.index)\n",
        "\n",
        "    df[\"label_source\"] = np.where(manual_mask, \"manual\",\n",
        "                          np.where(auto_mask, \"auto_confident\", \"fallback_thermal\"))\n",
        "    print(\"â„¹ï¸ label_source tidak ada â†’ dibuat ulang dari kolom yang tersedia.\")\n",
        "\n",
        "# ---------- BUILD df_train (untuk training model, kita pakai subset sesuai mode nanti) ----------\n",
        "# di tahap 4.1 kita simpan fold assignment untuk semua patch kerja\n",
        "df_train = df.copy()\n",
        "\n",
        "# ---------- GROUPKFOLD ----------\n",
        "groups = df_train[\"frame_id\"].astype(str).values\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "\n",
        "df_train[\"fold\"] = -1\n",
        "for fold_id, (_, val_idx) in enumerate(gkf.split(df_train, df_train[\"y_final\"].values, groups=groups)):\n",
        "    df_train.loc[val_idx, \"fold\"] = fold_id\n",
        "\n",
        "assert (df_train[\"fold\"] >= 0).all(), \"Ada fold yang belum terisi.\"\n",
        "\n",
        "# Save fold assignment (reproducible)\n",
        "df_train.to_csv(FOLDS_CSV, index=False)\n",
        "print(\"\\nâœ… Saved folds:\", FOLDS_CSV)\n",
        "print(\"Fold counts:\")\n",
        "display(df_train[\"fold\"].value_counts().sort_index())\n",
        "\n",
        "# ---------- SANITY CHECK: no frame overlap between train/val for each fold ----------\n",
        "def check_leakage(df_in, fold_id):\n",
        "    tr = df_in[df_in[\"fold\"] != fold_id]\n",
        "    va = df_in[df_in[\"fold\"] == fold_id]\n",
        "    tr_frames = set(tr[\"frame_id\"].astype(str))\n",
        "    va_frames = set(va[\"frame_id\"].astype(str))\n",
        "    inter = tr_frames.intersection(va_frames)\n",
        "    return len(inter)\n",
        "\n",
        "print(\"\\nLeakage check (frame_id overlap):\")\n",
        "for f in range(N_SPLITS):\n",
        "    inter = check_leakage(df_train, f)\n",
        "    print(f\"  Fold {f}: overlap frames =\", inter)\n",
        "\n",
        "# ---------- Label distribution per fold (y_final) ----------\n",
        "print(\"\\nLabel distribution per fold (y_final):\")\n",
        "for f in range(N_SPLITS):\n",
        "    va = df_train[df_train[\"fold\"] == f]\n",
        "    counts = va[\"y_final\"].value_counts().sort_index()\n",
        "    n0 = int(counts.get(0,0)); n1 = int(counts.get(1,0))\n",
        "    print(f\"  Fold {f} | val size={len(va)} | Normal={n0} | Stress={n1}\")\n",
        "\n",
        "# ---------- Dual-input loader: RGB + Thermal (masked+normalized) ----------\n",
        "def canopy_mask_v2_from_rgb(rgb_u8: np.ndarray) -> np.ndarray:\n",
        "    # RGB uint8 (H,W,3)\n",
        "    rgb = rgb_u8.astype(np.float32)\n",
        "    r, g, b = rgb[...,0], rgb[...,1], rgb[...,2]\n",
        "    exg = 2*g - r - b\n",
        "\n",
        "    lo = np.percentile(exg, 2)\n",
        "    hi = np.percentile(exg, 98)\n",
        "    exg_norm = (exg - lo) / (hi - lo + 1e-6)\n",
        "    exg_norm = np.clip(exg_norm, 0, 1)\n",
        "\n",
        "    hsv = cv2.cvtColor(rgb_u8, cv2.COLOR_RGB2HSV)\n",
        "    H = hsv[...,0]; S = hsv[...,1]; V = hsv[...,2]\n",
        "    green_hsv = (H >= 25) & (H <= 95) & (S >= 30) & (V >= 30)\n",
        "    green_exg = exg_norm > 0.35\n",
        "\n",
        "    mask = green_hsv & green_exg\n",
        "\n",
        "    mask_u8 = (mask.astype(np.uint8) * 255)\n",
        "    k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
        "    mask_u8 = cv2.morphologyEx(mask_u8, cv2.MORPH_OPEN, k, iterations=1)\n",
        "    mask_u8 = cv2.morphologyEx(mask_u8, cv2.MORPH_CLOSE, k, iterations=2)\n",
        "    return (mask_u8 > 0)\n",
        "\n",
        "def _load_one(rgb_path_b, th_path_b, y):\n",
        "    rgb_path = rgb_path_b.decode(\"utf-8\")\n",
        "    th_path  = th_path_b.decode(\"utf-8\")\n",
        "\n",
        "    # RGB\n",
        "    bgr = cv2.imread(rgb_path, cv2.IMREAD_COLOR)\n",
        "    if bgr is None:\n",
        "        raise FileNotFoundError(rgb_path)\n",
        "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "    if rgb.shape[0] != IMG_SIZE or rgb.shape[1] != IMG_SIZE:\n",
        "        rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n",
        "    rgb_f = rgb.astype(np.float32) / 255.0  # (128,128,3)\n",
        "\n",
        "    # Thermal\n",
        "    th = np.load(th_path).astype(np.float32)\n",
        "    if th.ndim != 2:\n",
        "        raise ValueError(f\"Thermal patch not 2D: {th.shape}\")\n",
        "    if th.shape != (IMG_SIZE, IMG_SIZE):\n",
        "        th = cv2.resize(th, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Mask canopy from RGB\n",
        "    mask = canopy_mask_v2_from_rgb((rgb_f*255).astype(np.uint8))  # bool (128,128)\n",
        "    th_can = th.copy()\n",
        "    th_can[~mask] = np.nan\n",
        "\n",
        "    # Normalize thermal within canopy (percentile 1-99), fallback to global if all NaN\n",
        "    vals = th_can[np.isfinite(th_can)]\n",
        "    if vals.size < 10:\n",
        "        vals = th[np.isfinite(th)]\n",
        "    p1 = np.percentile(vals, 1) if vals.size else 0.0\n",
        "    p99= np.percentile(vals, 99) if vals.size else 1.0\n",
        "    th_norm = (th - p1) / (p99 - p1 + 1e-6)\n",
        "    th_norm = np.clip(th_norm, 0.0, 1.0).astype(np.float32)  # (128,128)\n",
        "    th_norm = th_norm[..., None]  # (128,128,1)\n",
        "\n",
        "    return rgb_f.astype(np.float32), th_norm, np.float32(y)\n",
        "\n",
        "def tf_load(rgb_path, th_path, y):\n",
        "    rgb, th, yy = tf.numpy_function(\n",
        "        func=_load_one,\n",
        "        inp=[rgb_path, th_path, y],\n",
        "        Tout=[tf.float32, tf.float32, tf.float32]\n",
        "    )\n",
        "    rgb.set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "    th.set_shape((IMG_SIZE, IMG_SIZE, 1))\n",
        "    yy.set_shape(())\n",
        "    return (rgb, th), yy\n",
        "\n",
        "def make_ds(df_part, training=False):\n",
        "    rgb_paths = df_part[\"rgb_patch_path\"].astype(str).values\n",
        "    th_paths  = df_part[\"thermal_patch_path\"].astype(str).values\n",
        "    y         = df_part[\"y_final\"].astype(int).values\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((rgb_paths, th_paths, y))\n",
        "    if training:\n",
        "        ds = ds.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(tf_load, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "print(\"\\nâœ… 4.1 DONE: folds ready + data loader functions defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPHqF1o4wih8"
      },
      "source": [
        "## 4.2 Model Definition (Late Fusion: RGB MobileNetV2 + Thermal CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC_hywbpXZhm"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 4.2 â€” Model Definition (Late Fusion)\n",
        "# RGB: MobileNetV2 pretrained\n",
        "# Thermal: small CNN (1-channel)\n",
        "# Fusion: concat â†’ dense â†’ sigmoid\n",
        "# Compile: ROC-AUC, PR-AUC, Recall, Precision\n",
        "# ============================================\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_late_fusion_model(img_size=128, lr=1e-4):\n",
        "    # ---- RGB branch ----\n",
        "    inp_rgb = tf.keras.Input((img_size, img_size, 3), name=\"rgb\")\n",
        "    base_rgb = tf.keras.applications.MobileNetV2(\n",
        "        include_top=False, weights=\"imagenet\",\n",
        "        input_shape=(img_size, img_size, 3)\n",
        "    )\n",
        "    base_rgb.trainable = False  # start frozen; bisa fine-tune nanti\n",
        "    x_rgb = tf.keras.applications.mobilenet_v2.preprocess_input(inp_rgb * 255.0)\n",
        "    x_rgb = base_rgb(x_rgb, training=False)\n",
        "    x_rgb = tf.keras.layers.GlobalAveragePooling2D(name=\"gap_rgb\")(x_rgb)\n",
        "\n",
        "    # ---- Thermal branch ----\n",
        "    inp_th = tf.keras.Input((img_size, img_size, 1), name=\"th\")\n",
        "    x_th = tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\")(inp_th)\n",
        "    x_th = tf.keras.layers.MaxPool2D()(x_th)\n",
        "    x_th = tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(x_th)\n",
        "    x_th = tf.keras.layers.MaxPool2D()(x_th)\n",
        "    x_th = tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x_th)\n",
        "    x_th = tf.keras.layers.GlobalAveragePooling2D(name=\"gap_th\")(x_th)\n",
        "\n",
        "    # ---- Fusion ----\n",
        "    x = tf.keras.layers.Concatenate(name=\"fusion\")([x_rgb, x_th])\n",
        "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"p_stress\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[inp_rgb, inp_th], outputs=out, name=\"late_fusion_rgb_th\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\n",
        "            tf.keras.metrics.AUC(name=\"roc_auc\"),\n",
        "            tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\"),\n",
        "            tf.keras.metrics.Recall(name=\"recall\"),\n",
        "            tf.keras.metrics.Precision(name=\"precision\"),\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_late_fusion_model(img_size=IMG_SIZE, lr=1e-4)\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nTotal params:\", model.count_params())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c87HPcQwsJu"
      },
      "source": [
        "## 4.3 Train Mode A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52K2nKD1H0Ji"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 4.3 â€” Train Mode A (Scientific: GOLD-only)\n",
        "# Train: label_source == manual\n",
        "# Val  : pilih 1 fold (mis. fold=0)\n",
        "# Save : model_gold.keras\n",
        "# ============================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "MODEL_DIR = FIX_ROOT / \"models\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUT_GOLD = MODEL_DIR / \"model_gold.keras\"\n",
        "\n",
        "VAL_FOLD = 0\n",
        "\n",
        "# Filter GOLD-only\n",
        "df_gold = df_train[df_train[\"label_source\"] == \"manual\"].copy()\n",
        "print(\"GOLD-only rows:\", len(df_gold), \"| frames:\", df_gold[\"frame_id\"].nunique())\n",
        "print(\"Label dist (GOLD):\")\n",
        "display(df_gold[\"y_final\"].value_counts())\n",
        "\n",
        "# Split by fold\n",
        "tr = df_gold[df_gold[\"fold\"] != VAL_FOLD].copy()\n",
        "va = df_gold[df_gold[\"fold\"] == VAL_FOLD].copy()\n",
        "\n",
        "print(f\"\\nFold {VAL_FOLD} splits (GOLD-only):\")\n",
        "print(\"Train:\", len(tr), \"| Val:\", len(va))\n",
        "print(\"Train label dist:\")\n",
        "display(tr[\"y_final\"].value_counts())\n",
        "print(\"Val label dist:\")\n",
        "display(va[\"y_final\"].value_counts())\n",
        "\n",
        "# Leakage check\n",
        "inter = set(tr[\"frame_id\"]).intersection(set(va[\"frame_id\"]))\n",
        "print(\"Frame overlap:\", len(inter))\n",
        "\n",
        "ds_tr = make_ds(tr, training=True)\n",
        "ds_va = make_ds(va, training=False)\n",
        "\n",
        "model_gold = build_late_fusion_model(img_size=IMG_SIZE, lr=1e-4)\n",
        "\n",
        "cb = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_pr_auc\", mode=\"max\",\n",
        "        patience=3, restore_best_weights=True, verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(OUT_GOLD),\n",
        "        monitor=\"val_pr_auc\", mode=\"max\",\n",
        "        save_best_only=True, verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "hist = model_gold.fit(ds_tr, validation_data=ds_va, epochs=25, callbacks=cb, verbose=1)\n",
        "\n",
        "# ringkas best epoch\n",
        "best = np.argmax(hist.history[\"val_pr_auc\"]) + 1\n",
        "best_val = np.max(hist.history[\"val_pr_auc\"])\n",
        "print(f\"\\nâœ… Best epoch (by val_pr_auc): {best} | val_pr_auc={best_val:.4f}\")\n",
        "print(\"âœ… Saved best model:\", OUT_GOLD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5BFBRksnPuj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "import numpy as np\n",
        "\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "\n",
        "df_gold = df_train[df_train[\"label_source\"]==\"manual\"].copy()\n",
        "groups = df_gold[\"frame_id\"].astype(str).values\n",
        "y = df_gold[\"y_final\"].astype(int).values\n",
        "\n",
        "fold_metrics = []\n",
        "\n",
        "for f, (tr_idx, va_idx) in enumerate(gkf.split(df_gold, y, groups=groups)):\n",
        "    tr = df_gold.iloc[tr_idx].copy()\n",
        "    va = df_gold.iloc[va_idx].copy()\n",
        "\n",
        "    ds_tr = make_ds(tr, training=True)\n",
        "    ds_va = make_ds(va, training=False)\n",
        "\n",
        "    model = build_late_fusion_model(img_size=IMG_SIZE, lr=1e-4)\n",
        "    model.fit(ds_tr, validation_data=ds_va, epochs=25, callbacks=cb, verbose=0)\n",
        "\n",
        "    # ambil metrik val terakhir / atau evaluasi ulang\n",
        "    res = model.evaluate(ds_va, verbose=0)\n",
        "    fold_metrics.append(res)\n",
        "\n",
        "# ringkas mean/std\n",
        "fold_metrics = np.array(fold_metrics)\n",
        "print(\"Mean metrics:\", fold_metrics.mean(axis=0))\n",
        "print(\"Std  metrics:\", fold_metrics.std(axis=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heXADVrpRWt_"
      },
      "source": [
        "## 4.4 Train Mode B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfv7DrhXRYK7"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 4.4 â€” Train Mode B (Deployment: GOLD + SILVER confident)\n",
        "# Train: label_source in {manual, auto_confident}\n",
        "# Val  : pilih 1 fold (mis. fold=0)\n",
        "# Save : model_deploy.keras\n",
        "# Print: data count per label_source\n",
        "# ============================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_DEPLOY = MODEL_DIR / \"model_deploy.keras\"\n",
        "VAL_FOLD = 0\n",
        "\n",
        "df_deploy = df_train[df_train[\"label_source\"].isin([\"manual\",\"auto_confident\"])].copy()\n",
        "\n",
        "print(\"Deploy-train rows:\", len(df_deploy), \"| frames:\", df_deploy[\"frame_id\"].nunique())\n",
        "print(\"\\nCounts per label_source:\")\n",
        "display(df_deploy[\"label_source\"].value_counts())\n",
        "\n",
        "print(\"\\nLabel dist (deploy set):\")\n",
        "display(df_deploy[\"y_final\"].value_counts())\n",
        "\n",
        "tr = df_deploy[df_deploy[\"fold\"] != VAL_FOLD].copy()\n",
        "va = df_deploy[df_deploy[\"fold\"] == VAL_FOLD].copy()\n",
        "\n",
        "print(f\"\\nFold {VAL_FOLD} splits (deploy):\")\n",
        "print(\"Train:\", len(tr), \"| Val:\", len(va))\n",
        "print(\"Train label dist:\")\n",
        "display(tr[\"y_final\"].value_counts())\n",
        "print(\"Val label dist:\")\n",
        "display(va[\"y_final\"].value_counts())\n",
        "\n",
        "# Leakage check\n",
        "inter = set(tr[\"frame_id\"]).intersection(set(va[\"frame_id\"]))\n",
        "print(\"Frame overlap:\", len(inter))\n",
        "\n",
        "ds_tr = make_ds(tr, training=True)\n",
        "ds_va = make_ds(va, training=False)\n",
        "\n",
        "model_deploy = build_late_fusion_model(img_size=IMG_SIZE, lr=1e-4)\n",
        "\n",
        "cb = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_pr_auc\", mode=\"max\",\n",
        "        patience=3, restore_best_weights=True, verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(OUT_DEPLOY),\n",
        "        monitor=\"val_pr_auc\", mode=\"max\",\n",
        "        save_best_only=True, verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "hist = model_deploy.fit(ds_tr, validation_data=ds_va, epochs=25, callbacks=cb, verbose=1)\n",
        "\n",
        "best = np.argmax(hist.history[\"val_pr_auc\"]) + 1\n",
        "best_val = np.max(hist.history[\"val_pr_auc\"])\n",
        "print(f\"\\nâœ… Best epoch (by val_pr_auc): {best} | val_pr_auc={best_val:.4f}\")\n",
        "print(\"âœ… Saved best model:\", OUT_DEPLOY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57jG25CAnots"
      },
      "outputs": [],
      "source": [
        "df_deploy = df_train[df_train[\"label_source\"].isin([\"manual\",\"auto_confident\"])].copy()\n",
        "groups = df_deploy[\"frame_id\"].astype(str).values\n",
        "y = df_deploy[\"y_final\"].astype(int).values\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "scores = []\n",
        "\n",
        "for f, (tr_idx, va_idx) in enumerate(gkf.split(df_deploy, y, groups=groups)):\n",
        "    tr = df_deploy.iloc[tr_idx].copy()\n",
        "    va_all = df_deploy.iloc[va_idx].copy()\n",
        "\n",
        "    # eval hanya manual di fold tsb\n",
        "    va = va_all[va_all[\"label_source\"]==\"manual\"].copy()\n",
        "    if len(va) == 0:\n",
        "        print(\"Fold\", f, \"skip: no manual in val\")\n",
        "        continue\n",
        "\n",
        "    ds_tr = make_ds(tr, training=True)\n",
        "    ds_va = make_ds(va, training=False)\n",
        "\n",
        "    model = build_late_fusion_model(img_size=IMG_SIZE, lr=1e-4)\n",
        "    model.fit(ds_tr, validation_data=ds_va, epochs=25, callbacks=cb, verbose=0)\n",
        "\n",
        "    res = model.evaluate(ds_va, verbose=0)\n",
        "    scores.append(res)\n",
        "\n",
        "scores = np.array(scores)\n",
        "print(\"Mean metrics (eval manual-only):\", scores.mean(axis=0))\n",
        "print(\"Std  metrics:\", scores.std(axis=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8qgyBk1W6RY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "candidates = []\n",
        "for k, v in globals().items():\n",
        "    if isinstance(v, tf.keras.Model):\n",
        "        candidates.append(k)\n",
        "\n",
        "print(\"Keras model objects in memory:\")\n",
        "for c in candidates:\n",
        "    print(\"-\", c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHbxlu2AXiVi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def inspect_model(name, mdl):\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Inputs:\")\n",
        "    for i, inp in enumerate(mdl.inputs):\n",
        "        print(f\"  input[{i}]: shape={inp.shape}\")\n",
        "    print(\"Outputs:\", mdl.outputs[0].shape)\n",
        "\n",
        "inspect_model(\"model\", model)\n",
        "inspect_model(\"model_final\", model_final)\n",
        "inspect_model(\"model_ft\", model_ft)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYYSqC2-XSNq"
      },
      "source": [
        "# TAHAP 5 - Evaluation & Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfhBZouZXdZ2"
      },
      "source": [
        "## 5.1 Setup Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvMWmehNXuS_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix\n",
        ")\n",
        "\n",
        "THR = 0.5\n",
        "N_SPLITS = df_train[\"fold\"].nunique()\n",
        "\n",
        "def compute_metrics(y_true, y_prob, thr=0.5):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "        \"roc_auc\": roc_auc_score(y_true, y_prob),\n",
        "        \"pr_auc\": average_precision_score(y_true, y_prob),\n",
        "        \"cm\": confusion_matrix(y_true, y_pred)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiHdCo8PXr18"
      },
      "source": [
        "## 5.2 Dataset Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCToVtCUgUiP"
      },
      "outputs": [],
      "source": [
        "df_gold = df_train[df_train[\"label_source\"] == \"manual\"].copy()\n",
        "\n",
        "print(\"GOLD rows:\", len(df_gold))\n",
        "print(\"Frames:\", df_gold[\"frame_id\"].nunique())\n",
        "print(\"Label dist:\")\n",
        "display(df_gold[\"y_final\"].value_counts())\n",
        "\n",
        "print(\"\\nLabel dist per fold:\")\n",
        "for f in sorted(df_gold[\"fold\"].unique()):\n",
        "    sub = df_gold[df_gold[\"fold\"] == f]\n",
        "    print(f\"Fold {f}:\")\n",
        "    display(sub[\"y_final\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqzOh31XuBYU"
      },
      "source": [
        "## 5.3 RGB-only Model + RGB-only Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFtVjtmpuJRS"
      },
      "source": [
        "### 5.3A Model RGB-only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7O23SYrkzK9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def build_rgb_only_model(img_size=128, lr=1e-4):\n",
        "    inp = tf.keras.Input((img_size, img_size, 3), name=\"rgb\")\n",
        "    base = tf.keras.applications.MobileNetV2(\n",
        "        include_top=False, weights=\"imagenet\",\n",
        "        input_shape=(img_size, img_size, 3)\n",
        "    )\n",
        "    base.trainable = False\n",
        "\n",
        "    x = tf.keras.applications.mobilenet_v2.preprocess_input(inp * 255.0)\n",
        "    x = base(x, training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inp, outputs=out, name=\"rgb_only\")\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\n",
        "            tf.keras.metrics.AUC(name=\"roc_auc\"),\n",
        "            tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\")\n",
        "        ]\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWfuTN3MuOst"
      },
      "source": [
        "### 5.3B RGB-only Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MquAoB_iuQ5Z"
      },
      "outputs": [],
      "source": [
        "def make_ds_rgb(df_part, training=False):\n",
        "    rgb_paths = df_part[\"rgb_patch_path\"].astype(str).values\n",
        "    y = df_part[\"y_final\"].astype(int).values\n",
        "\n",
        "    def _load_rgb(rgb_path_b, y):\n",
        "        rgb_path = rgb_path_b.decode(\"utf-8\")\n",
        "        bgr = cv2.imread(rgb_path, cv2.IMREAD_COLOR)\n",
        "        if bgr is None:\n",
        "            raise FileNotFoundError(rgb_path)\n",
        "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "        if rgb.shape[:2] != (IMG_SIZE, IMG_SIZE):\n",
        "            rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE))\n",
        "        rgb = rgb.astype(np.float32) / 255.0\n",
        "        return rgb, np.float32(y)\n",
        "\n",
        "    def tf_load(rgb_path, y):\n",
        "        rgb, yy = tf.numpy_function(\n",
        "            _load_rgb, [rgb_path, y], [tf.float32, tf.float32]\n",
        "        )\n",
        "        rgb.set_shape((IMG_SIZE, IMG_SIZE, 3))\n",
        "        yy.set_shape(())\n",
        "        return rgb, yy\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((rgb_paths, y))\n",
        "    if training:\n",
        "        ds = ds.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(tf_load, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ODZLSDBuS_2"
      },
      "source": [
        "## 5.4 K-Fold Evaluation: RGB-only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFbQqp1VuUeQ"
      },
      "outputs": [],
      "source": [
        "rgb_oof_rows = []\n",
        "rgb_fold_metrics = []\n",
        "rgb_cm_sum = np.zeros((2,2), dtype=int)\n",
        "\n",
        "for f in range(N_SPLITS):\n",
        "    tr = df_gold[df_gold[\"fold\"] != f].copy()\n",
        "    va = df_gold[df_gold[\"fold\"] == f].copy()\n",
        "\n",
        "    ds_tr = make_ds_rgb(tr, training=True)\n",
        "    ds_va = make_ds_rgb(va, training=False)\n",
        "\n",
        "    model = build_rgb_only_model(img_size=IMG_SIZE, lr=1e-4)\n",
        "    model.fit(ds_tr, epochs=15, verbose=0)\n",
        "\n",
        "    y_prob = model.predict(ds_va).ravel()\n",
        "    y_true = va[\"y_final\"].values\n",
        "\n",
        "    m = compute_metrics(y_true, y_prob, THR)\n",
        "    rgb_fold_metrics.append({k:v for k,v in m.items() if k!=\"cm\"})\n",
        "    rgb_cm_sum += m[\"cm\"]\n",
        "\n",
        "    for pid, fid, yt, yp in zip(va[\"patch_id\"], va[\"frame_id\"], y_true, y_prob):\n",
        "        rgb_oof_rows.append({\n",
        "            \"patch_id\": pid, \"frame_id\": fid, \"fold\": f,\n",
        "            \"y_true\": yt, \"y_prob\": yp, \"y_pred\": int(yp>=THR)\n",
        "        })\n",
        "\n",
        "rgb_metrics_df = pd.DataFrame(rgb_fold_metrics)\n",
        "display(rgb_metrics_df)\n",
        "print(\"\\nRGB-only mean Â± std:\")\n",
        "display(rgb_metrics_df.agg([\"mean\",\"std\"]))\n",
        "print(\"RGB-only Confusion Matrix (aggregated):\\n\", rgb_cm_sum)\n",
        "\n",
        "pd.DataFrame(rgb_oof_rows).to_csv(\"pred_rgb_oof.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydm_0XVAuXhV"
      },
      "source": [
        "## 5.5 K-Fold Evaluation: Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk8FoA-VuY09"
      },
      "outputs": [],
      "source": [
        "fusion_oof_rows = []\n",
        "fusion_fold_metrics = []\n",
        "fusion_cm_sum = np.zeros((2,2), dtype=int)\n",
        "\n",
        "for f in range(N_SPLITS):\n",
        "    tr = df_gold[df_gold[\"fold\"] != f].copy()\n",
        "    va = df_gold[df_gold[\"fold\"] == f].copy()\n",
        "\n",
        "    ds_tr = make_ds(tr, training=True)\n",
        "    ds_va = make_ds(va, training=False)\n",
        "\n",
        "    model = build_late_fusion_model(img_size=IMG_SIZE, lr=1e-4)\n",
        "    model.fit(ds_tr, epochs=15, verbose=0)\n",
        "\n",
        "    y_prob = model.predict(ds_va).ravel()\n",
        "    y_true = va[\"y_final\"].values\n",
        "\n",
        "    m = compute_metrics(y_true, y_prob, THR)\n",
        "    fusion_fold_metrics.append({k:v for k,v in m.items() if k!=\"cm\"})\n",
        "    fusion_cm_sum += m[\"cm\"]\n",
        "\n",
        "    for pid, fid, yt, yp in zip(va[\"patch_id\"], va[\"frame_id\"], y_true, y_prob):\n",
        "        fusion_oof_rows.append({\n",
        "            \"patch_id\": pid, \"frame_id\": fid, \"fold\": f,\n",
        "            \"y_true\": yt, \"y_prob\": yp, \"y_pred\": int(yp>=THR)\n",
        "        })\n",
        "\n",
        "fusion_metrics_df = pd.DataFrame(fusion_fold_metrics)\n",
        "display(fusion_metrics_df)\n",
        "print(\"\\nFusion mean Â± std:\")\n",
        "display(fusion_metrics_df.agg([\"mean\",\"std\"]))\n",
        "print(\"Fusion Confusion Matrix (aggregated):\\n\", fusion_cm_sum)\n",
        "\n",
        "pd.DataFrame(fusion_oof_rows).to_csv(\"pred_fusion_oof.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWQX5Sgjf3ex"
      },
      "source": [
        "## 5.6 Deploy Evaluation (YANG DIPAKAI YANG INI YA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIzTh5je1FSn"
      },
      "source": [
        "### 5.6.1 RGB ONLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNOph__7f3Hj"
      },
      "outputs": [],
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Reproducibility\n",
        "# ---------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# ---------------------------\n",
        "# 1) PATHS (sesuaikan kalau FIX_ROOT kamu beda)\n",
        "# ---------------------------\n",
        "# Root project kamu (sesuai yang selama ini dipakai)\n",
        "FIX_ROOT = Path(\"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/DatasetTerbaruFIX\")\n",
        "\n",
        "# FINAL labeled dataset hasil autolabel config topk_10pct\n",
        "FINAL_LABELED_CSV = FIX_ROOT / \"labels\" / \"final\" / \"patches_all_final_labeled_topk_10pct.csv\"\n",
        "\n",
        "# Export output ke Drive\n",
        "OUT_DIR = FIX_ROOT / \"exports\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUT_OOF = OUT_DIR / \"pred_rgb_oof_deployset_manual_autoconf_topk_10pct.csv\"\n",
        "OUT_MET = OUT_DIR / \"metrics_rgb_deployset_manual_autoconf_topk_10pct.csv\"\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PARAMS\n",
        "# ---------------------------\n",
        "IMG_SIZE = 128\n",
        "BATCH = 64\n",
        "N_SPLITS = 5\n",
        "\n",
        "EPOCHS = 30          # cukup besar, tapi akan dipotong EarlyStopping\n",
        "LR_HEAD = 1e-4       # LR head (warm-up)\n",
        "LR_FT   = 5e-5       # LR finetune\n",
        "PATIENCE = 4\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Load dataset FINAL (topk_10pct)\n",
        "# ---------------------------\n",
        "if not FINAL_LABELED_CSV.exists():\n",
        "    raise FileNotFoundError(f\"FINAL labeled CSV tidak ditemukan:\\n{FINAL_LABELED_CSV}\")\n",
        "\n",
        "df = pd.read_csv(FINAL_LABELED_CSV)\n",
        "print(\"Loaded:\", FINAL_LABELED_CSV, \"shape:\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "# Pastikan kolom penting ada\n",
        "need_cols = [\"patch_id\",\"frame_id\",\"rgb_patch_path\",\"y_final\",\"label_source\"]\n",
        "for c in need_cols:\n",
        "    if c not in df.columns:\n",
        "        raise ValueError(f\"Kolom wajib '{c}' tidak ada di CSV. Kolom tersedia: {list(df.columns)}\")\n",
        "\n",
        "# (Opsional tapi disarankan) filter hanya patch padi (kalau kolom ada)\n",
        "for col in [\"keep_for_model\", \"qc_canopy_ok\"]:\n",
        "    if col in df.columns:\n",
        "        before = len(df)\n",
        "        df = df[df[col] == True].copy()\n",
        "        print(f\"Filter {col}=True: {before} -> {len(df)}\")\n",
        "\n",
        "# Ambil hanya manual + auto_confident (target kamu = 2069)\n",
        "df_eval = df[df[\"label_source\"].isin([\"manual\",\"auto_confident\"])].copy()\n",
        "\n",
        "# pastikan label numeric 0/1\n",
        "df_eval[\"y_final\"] = pd.to_numeric(df_eval[\"y_final\"], errors=\"coerce\")\n",
        "df_eval = df_eval[df_eval[\"y_final\"].isin([0,1])].copy()\n",
        "df_eval[\"y_final\"] = df_eval[\"y_final\"].astype(int)\n",
        "\n",
        "df_eval = df_eval.reset_index(drop=True)\n",
        "print(\"\\nEVAL rows (manual+auto_confident):\", len(df_eval))\n",
        "print(df_eval[\"label_source\"].value_counts())\n",
        "print(\"Label dist:\", df_eval[\"y_final\"].value_counts().to_dict())\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Buat fold by frame_id (anti leakage)\n",
        "# ---------------------------\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "df_eval[\"fold\"] = -1\n",
        "for fold, (_, va_idx) in enumerate(gkf.split(df_eval, df_eval[\"y_final\"], groups=df_eval[\"frame_id\"])):\n",
        "    df_eval.loc[va_idx, \"fold\"] = fold\n",
        "\n",
        "assert (df_eval[\"fold\"] >= 0).all()\n",
        "print(\"\\nFold sizes:\", df_eval[\"fold\"].value_counts().sort_index().to_dict())\n",
        "\n",
        "# ---------------------------\n",
        "# 5) tf.data loader RGB\n",
        "# ---------------------------\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def _load_rgb(path, y=None):\n",
        "    # path: tf.string\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    if y is None:\n",
        "        return img\n",
        "    y = tf.cast(y, tf.float32)\n",
        "    return img, y\n",
        "\n",
        "def make_ds_rgb(df_part: pd.DataFrame, training: bool):\n",
        "    paths = df_part[\"rgb_patch_path\"].astype(str).values\n",
        "    labels = df_part[\"y_final\"].astype(int).values\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    ds = ds.map(lambda p, y: _load_rgb(p, y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if training:\n",
        "        ds = ds.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
        "\n",
        "    ds = ds.batch(BATCH).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Model RGB-only (MobileNetV2) + optional finetune\n",
        "# ---------------------------\n",
        "def build_rgb_only_model(lr=LR_HEAD, train_base=False):\n",
        "    base = tf.keras.applications.MobileNetV2(\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        "    )\n",
        "    base.trainable = bool(train_base)\n",
        "\n",
        "    inp = tf.keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    x = tf.keras.applications.mobilenet_v2.preprocess_input(inp * 255.0)\n",
        "    x = base(x, training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.25)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\n",
        "            tf.keras.metrics.AUC(name=\"roc_auc\"),\n",
        "            tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\"),\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Metrics helper + threshold search (maximize accuracy)\n",
        "# ---------------------------\n",
        "def best_thr_for_accuracy(y_true, y_prob):\n",
        "    ths = np.linspace(0.05, 0.95, 181)\n",
        "    best_t, best_acc = 0.5, -1\n",
        "    for t in ths:\n",
        "        acc = accuracy_score(y_true, (y_prob >= t).astype(int))\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_t = float(t)\n",
        "    return best_t, best_acc\n",
        "\n",
        "def compute_metrics(y_true, y_prob, thr):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "        \"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "        \"pr_auc\": float(average_precision_score(y_true, y_prob)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "        \"cm\": confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# 8) K-Fold Training + OOF preds\n",
        "# ---------------------------\n",
        "rgb_oof_rows = []\n",
        "fold_metrics = []\n",
        "cm_sum = np.zeros((2,2), dtype=int)\n",
        "\n",
        "for f in range(N_SPLITS):\n",
        "    tr = df_eval[df_eval[\"fold\"] != f].copy()\n",
        "    va = df_eval[df_eval[\"fold\"] == f].copy()\n",
        "\n",
        "    ds_tr = make_ds_rgb(tr, training=True)\n",
        "    ds_va = make_ds_rgb(va, training=False)\n",
        "\n",
        "    # class_weight (lebih stabil)\n",
        "    y_tr = tr[\"y_final\"].values\n",
        "    classes = np.array([0,1])\n",
        "    cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
        "    class_weight = {0: float(cw[0]), 1: float(cw[1])}\n",
        "\n",
        "    # --- stage A: train head (base frozen)\n",
        "    model = build_rgb_only_model(lr=LR_HEAD, train_base=False)\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6),\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        ds_tr,\n",
        "        validation_data=ds_va,\n",
        "        epochs=EPOCHS,\n",
        "        verbose=0,\n",
        "        class_weight=class_weight,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    # --- stage B: light finetune (unfreeze last N layers of base)\n",
        "    # (Ini sering ngangkat sedikit performa tanpa overfit terlalu cepat)\n",
        "    base = None\n",
        "    for lyr in model.layers:\n",
        "        if isinstance(lyr, tf.keras.Model) and \"mobilenetv2\" in lyr.name.lower():\n",
        "            base = lyr\n",
        "            break\n",
        "    # kalau tidak ketemu via loop di atas (struktur bisa beda), cari via model.get_layer\n",
        "    if base is None:\n",
        "        try:\n",
        "            base = model.get_layer(index=1)\n",
        "        except:\n",
        "            base = None\n",
        "\n",
        "    if base is not None:\n",
        "        base.trainable = True\n",
        "        # freeze awal, unfreeze bagian akhir saja\n",
        "        # (angka 30 bisa kamu ubah: 20/30/40)\n",
        "        for lyr in base.layers[:-30]:\n",
        "            lyr.trainable = False\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(LR_FT),\n",
        "            loss=\"binary_crossentropy\",\n",
        "            metrics=[tf.keras.metrics.AUC(name=\"roc_auc\"),\n",
        "                     tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\")]\n",
        "        )\n",
        "\n",
        "        model.fit(\n",
        "            ds_tr,\n",
        "            validation_data=ds_va,\n",
        "            epochs=10,\n",
        "            verbose=0,\n",
        "            class_weight=class_weight,\n",
        "            callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)],\n",
        "        )\n",
        "\n",
        "    # Predict fold\n",
        "    y_prob = model.predict(ds_va, verbose=0).ravel()\n",
        "    y_true = va[\"y_final\"].values.astype(int)\n",
        "\n",
        "    thr_fold, acc_fold = best_thr_for_accuracy(y_true, y_prob)\n",
        "    m = compute_metrics(y_true, y_prob, thr_fold)\n",
        "    m_out = {k:v for k,v in m.items() if k != \"cm\"}\n",
        "    m_out[\"thr_used\"] = float(thr_fold)\n",
        "    m_out[\"fold\"] = int(f)\n",
        "    m_out[\"n_val\"] = int(len(va))\n",
        "    fold_metrics.append(m_out)\n",
        "    cm_sum += m[\"cm\"]\n",
        "\n",
        "    for pid, fid, yt, yp in zip(va[\"patch_id\"].astype(str).values,\n",
        "                                va[\"frame_id\"].astype(str).values,\n",
        "                                y_true, y_prob):\n",
        "        rgb_oof_rows.append({\n",
        "            \"patch_id\": pid,\n",
        "            \"frame_id\": fid,\n",
        "            \"fold\": int(f),\n",
        "            \"y_true\": int(yt),\n",
        "            \"y_prob\": float(yp),\n",
        "            \"thr_used\": float(thr_fold),\n",
        "            \"y_pred\": int(yp >= thr_fold),\n",
        "            \"label_source\": \"manual_or_autoconf\"  # penanda, biar ga ketuker file\n",
        "        })\n",
        "\n",
        "# ---------------------------\n",
        "# 9) Report + Save to Drive\n",
        "# ---------------------------\n",
        "met_df = pd.DataFrame(fold_metrics).sort_values(\"fold\").reset_index(drop=True)\n",
        "print(\"\\nRGB-only (manual+auto_confident) per-fold:\")\n",
        "display(met_df)\n",
        "\n",
        "print(\"\\nMean Â± Std:\")\n",
        "display(met_df.drop(columns=[\"fold\"]).agg([\"mean\",\"std\"]))\n",
        "\n",
        "print(\"\\nConfusion Matrix (aggregated):\\n\", cm_sum)\n",
        "\n",
        "oof_df = pd.DataFrame(rgb_oof_rows)\n",
        "oof_df.to_csv(OUT_OOF, index=False)\n",
        "met_df.to_csv(OUT_MET, index=False)\n",
        "\n",
        "print(\"\\nâœ… Saved OOF:\", OUT_OOF)\n",
        "print(\"âœ… Saved metrics:\", OUT_MET)\n",
        "print(\"OOF shape:\", oof_df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b7bF_lS1Nle"
      },
      "source": [
        "### 5.6.2 Thermal ONLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRVwR-rGggSC"
      },
      "outputs": [],
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Reproducibility\n",
        "# ---------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# ---------------------------\n",
        "# 1) PATHS\n",
        "# ---------------------------\n",
        "FIX_ROOT = Path(\"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/DatasetTerbaruFIX\")\n",
        "FINAL_LABELED_CSV = FIX_ROOT / \"labels\" / \"final\" / \"patches_all_final_labeled_topk_10pct.csv\"\n",
        "\n",
        "OUT_DIR = FIX_ROOT / \"exports\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUT_OOF = OUT_DIR / \"pred_thermal_oof_deployset_manual_autoconf_topk_10pct.csv\"\n",
        "OUT_MET = OUT_DIR / \"metrics_thermal_deployset_manual_autoconf_topk_10pct.csv\"\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PARAMS\n",
        "# ---------------------------\n",
        "IMG_SIZE = 128\n",
        "BATCH = 64\n",
        "N_SPLITS = 5\n",
        "\n",
        "EPOCHS = 30\n",
        "LR_HEAD = 1e-4\n",
        "LR_FT = 5e-5\n",
        "PATIENCE = 4\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Load dataset FINAL\n",
        "# ---------------------------\n",
        "if not FINAL_LABELED_CSV.exists():\n",
        "    raise FileNotFoundError(f\"FINAL labeled CSV tidak ditemukan:\\n{FINAL_LABELED_CSV}\")\n",
        "\n",
        "df = pd.read_csv(FINAL_LABELED_CSV)\n",
        "print(\"Loaded:\", FINAL_LABELED_CSV, \"shape:\", df.shape)\n",
        "\n",
        "need_cols = [\"patch_id\",\"frame_id\",\"thermal_patch_path\",\"y_final\",\"label_source\"]\n",
        "for c in need_cols:\n",
        "    if c not in df.columns:\n",
        "        raise ValueError(f\"Kolom wajib '{c}' tidak ada. Kolom tersedia: {list(df.columns)}\")\n",
        "\n",
        "# opsional filter kualitas (kalau kolom ada)\n",
        "for col in [\"keep_for_model\", \"qc_canopy_ok\"]:\n",
        "    if col in df.columns:\n",
        "        before = len(df)\n",
        "        df = df[df[col] == True].copy()\n",
        "        print(f\"Filter {col}=True: {before} -> {len(df)}\")\n",
        "\n",
        "# hanya manual + auto_confident\n",
        "df_eval = df[df[\"label_source\"].isin([\"manual\",\"auto_confident\"])].copy()\n",
        "df_eval[\"y_final\"] = pd.to_numeric(df_eval[\"y_final\"], errors=\"coerce\")\n",
        "df_eval = df_eval[df_eval[\"y_final\"].isin([0,1])].copy()\n",
        "df_eval[\"y_final\"] = df_eval[\"y_final\"].astype(int)\n",
        "df_eval = df_eval.reset_index(drop=True)\n",
        "\n",
        "print(\"\\nEVAL rows (manual+auto_confident):\", len(df_eval))\n",
        "print(df_eval[\"label_source\"].value_counts())\n",
        "print(\"Label dist:\", df_eval[\"y_final\"].value_counts().to_dict())\n",
        "\n",
        "# pastikan file npy exist minimal sampling (biar cepat ketahuan path salah)\n",
        "sample_paths = df_eval[\"thermal_patch_path\"].astype(str).head(5).tolist()\n",
        "for p in sample_paths:\n",
        "    if not Path(p).exists():\n",
        "        raise FileNotFoundError(f\"Contoh thermal_patch_path tidak ditemukan:\\n{p}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Fold by frame_id\n",
        "# ---------------------------\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "df_eval[\"fold\"] = -1\n",
        "for fold, (_, va_idx) in enumerate(gkf.split(df_eval, df_eval[\"y_final\"], groups=df_eval[\"frame_id\"])):\n",
        "    df_eval.loc[va_idx, \"fold\"] = fold\n",
        "assert (df_eval[\"fold\"] >= 0).all()\n",
        "print(\"\\nFold sizes:\", df_eval[\"fold\"].value_counts().sort_index().to_dict())\n",
        "\n",
        "# ---------------------------\n",
        "# 5) tf.data loader THERMAL (.npy)\n",
        "# ---------------------------\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def _np_load_npy(path_bytes):\n",
        "    # path_bytes: bytes\n",
        "    p = path_bytes.decode(\"utf-8\")\n",
        "    arr = np.load(p).astype(np.float32)  # (H,W) = (128,128) atau sesuai patch\n",
        "    return arr\n",
        "\n",
        "def _load_th(path, y=None):\n",
        "    # pakai numpy_function agar bisa np.load\n",
        "    th = tf.numpy_function(_np_load_npy, [path], tf.float32)  # shape unknown\n",
        "    th.set_shape((IMG_SIZE, IMG_SIZE))  # WAJIB biar graph stabil\n",
        "    th = tf.expand_dims(th, axis=-1)    # (H,W,1)\n",
        "\n",
        "    # normalisasi sederhana: per-patch z-score (lebih stabil daripada raw)\n",
        "    mean = tf.reduce_mean(th)\n",
        "    std = tf.math.reduce_std(th)\n",
        "    th = (th - mean) / (std + 1e-6)\n",
        "\n",
        "    if y is None:\n",
        "        return th\n",
        "    return th, tf.cast(y, tf.float32)\n",
        "\n",
        "def make_ds_th(df_part: pd.DataFrame, training: bool):\n",
        "    paths = df_part[\"thermal_patch_path\"].astype(str).values\n",
        "    labels = df_part[\"y_final\"].astype(int).values\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    ds = ds.map(lambda p, y: _load_th(p, y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if training:\n",
        "        ds = ds.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
        "\n",
        "    ds = ds.batch(BATCH).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Model THERMAL-only (small CNN)\n",
        "# ---------------------------\n",
        "def build_thermal_only_model(lr=LR_HEAD):\n",
        "    inp = tf.keras.Input((IMG_SIZE, IMG_SIZE, 1))\n",
        "    x = inp\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.MaxPool2D()(x)\n",
        "    x = tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.MaxPool2D()(x)\n",
        "    x = tf.keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\n",
        "            tf.keras.metrics.AUC(name=\"roc_auc\"),\n",
        "            tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\"),\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Metrics helper + threshold search\n",
        "# ---------------------------\n",
        "def best_thr_for_accuracy(y_true, y_prob):\n",
        "    ths = np.linspace(0.05, 0.95, 181)\n",
        "    best_t, best_acc = 0.5, -1\n",
        "    for t in ths:\n",
        "        acc = accuracy_score(y_true, (y_prob >= t).astype(int))\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_t = float(t)\n",
        "    return best_t, best_acc\n",
        "\n",
        "def compute_metrics(y_true, y_prob, thr):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "        \"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "        \"pr_auc\": float(average_precision_score(y_true, y_prob)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "        \"cm\": confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# 8) K-Fold Training + OOF\n",
        "# ---------------------------\n",
        "th_oof_rows = []\n",
        "fold_metrics = []\n",
        "cm_sum = np.zeros((2,2), dtype=int)\n",
        "\n",
        "for f in range(N_SPLITS):\n",
        "    tr = df_eval[df_eval[\"fold\"] != f].copy()\n",
        "    va = df_eval[df_eval[\"fold\"] == f].copy()\n",
        "\n",
        "    ds_tr = make_ds_th(tr, training=True)\n",
        "    ds_va = make_ds_th(va, training=False)\n",
        "\n",
        "    # class_weight\n",
        "    y_tr = tr[\"y_final\"].values\n",
        "    classes = np.array([0,1])\n",
        "    cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
        "    class_weight = {0: float(cw[0]), 1: float(cw[1])}\n",
        "\n",
        "    model = build_thermal_only_model(lr=LR_HEAD)\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6),\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        ds_tr,\n",
        "        validation_data=ds_va,\n",
        "        epochs=EPOCHS,\n",
        "        verbose=0,\n",
        "        class_weight=class_weight,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    y_prob = model.predict(ds_va, verbose=0).ravel()\n",
        "    y_true = va[\"y_final\"].values.astype(int)\n",
        "\n",
        "    thr_fold, _ = best_thr_for_accuracy(y_true, y_prob)\n",
        "\n",
        "    m = compute_metrics(y_true, y_prob, thr_fold)\n",
        "    m_out = {k:v for k,v in m.items() if k != \"cm\"}\n",
        "    m_out[\"thr_used\"] = float(thr_fold)\n",
        "    m_out[\"fold\"] = int(f)\n",
        "    m_out[\"n_val\"] = int(len(va))\n",
        "    fold_metrics.append(m_out)\n",
        "    cm_sum += m[\"cm\"]\n",
        "\n",
        "    for pid, fid, yt, yp in zip(va[\"patch_id\"].astype(str).values,\n",
        "                                va[\"frame_id\"].astype(str).values,\n",
        "                                y_true, y_prob):\n",
        "        th_oof_rows.append({\n",
        "            \"patch_id\": pid,\n",
        "            \"frame_id\": fid,\n",
        "            \"fold\": int(f),\n",
        "            \"y_true\": int(yt),\n",
        "            \"y_prob\": float(yp),\n",
        "            \"thr_used\": float(thr_fold),\n",
        "            \"y_pred\": int(yp >= thr_fold),\n",
        "            \"label_source\": \"manual_or_autoconf\"\n",
        "        })\n",
        "\n",
        "# ---------------------------\n",
        "# 9) Report + Save\n",
        "# ---------------------------\n",
        "met_df = pd.DataFrame(fold_metrics).sort_values(\"fold\").reset_index(drop=True)\n",
        "print(\"\\nTHERMAL-only (manual+auto_confident) per-fold:\")\n",
        "display(met_df)\n",
        "\n",
        "print(\"\\nMean Â± Std:\")\n",
        "display(met_df.drop(columns=[\"fold\"]).agg([\"mean\",\"std\"]))\n",
        "\n",
        "print(\"\\nConfusion Matrix (aggregated):\\n\", cm_sum)\n",
        "\n",
        "oof_df = pd.DataFrame(th_oof_rows)\n",
        "oof_df.to_csv(OUT_OOF, index=False)\n",
        "met_df.to_csv(OUT_MET, index=False)\n",
        "\n",
        "print(\"\\nâœ… Saved OOF:\", OUT_OOF)\n",
        "print(\"âœ… Saved metrics:\", OUT_MET)\n",
        "print(\"OOF shape:\", oof_df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k66EiSvZ1WBF"
      },
      "source": [
        "### 5.6.3 FUSION (RGB + THERMAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwEM1XMsg7UT"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# FUSION (RGB+THERMAL) EVAL (manual + auto_confident) = dari FINAL thr_090_010\n",
        "# - input: patches_all_final_labeled_thr_090_010.csv\n",
        "# - output: metrics per-fold + OOF preds CSV (SAVE ke Drive)\n",
        "# ============================================\n",
        "\n",
        "import os, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Reproducibility\n",
        "# ---------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# ---------------------------\n",
        "# 1) PATHS\n",
        "# ---------------------------\n",
        "FIX_ROOT = Path(\"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/DatasetTerbaruFIX\")\n",
        "FINAL_LABELED_CSV = FIX_ROOT / \"labels\" / \"final\" / \"patches_all_final_labeled_topk_10pct.csv\"\n",
        "\n",
        "OUT_DIR = FIX_ROOT / \"exports\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUT_OOF = OUT_DIR / \"pred_fusion_oof_deployset_manual_autoconf_thr_topk_10pct.csv\"\n",
        "OUT_MET = OUT_DIR / \"metrics_fusion_deployset_manual_autoconf_thr_topk_10pct.csv\"\n",
        "\n",
        "# ---------------------------\n",
        "# 2) PARAMS\n",
        "# ---------------------------\n",
        "IMG_SIZE = 128\n",
        "BATCH = 32          # fusion lebih berat, amanin RAM/VRAM\n",
        "N_SPLITS = 5\n",
        "EPOCHS = 30\n",
        "LR = 1e-4\n",
        "PATIENCE = 4\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Load dataset FINAL\n",
        "# ---------------------------\n",
        "if not FINAL_LABELED_CSV.exists():\n",
        "    raise FileNotFoundError(f\"FINAL labeled CSV tidak ditemukan:\\n{FINAL_LABELED_CSV}\")\n",
        "\n",
        "df = pd.read_csv(FINAL_LABELED_CSV)\n",
        "print(\"Loaded:\", FINAL_LABELED_CSV, \"shape:\", df.shape)\n",
        "\n",
        "need_cols = [\"patch_id\",\"frame_id\",\"rgb_patch_path\",\"thermal_patch_path\",\"y_final\",\"label_source\"]\n",
        "for c in need_cols:\n",
        "    if c not in df.columns:\n",
        "        raise ValueError(f\"Kolom wajib '{c}' tidak ada. Kolom tersedia: {list(df.columns)}\")\n",
        "\n",
        "# opsional filter kualitas (kalau kolom ada)\n",
        "for col in [\"keep_for_model\", \"qc_canopy_ok\"]:\n",
        "    if col in df.columns:\n",
        "        before = len(df)\n",
        "        df = df[df[col] == True].copy()\n",
        "        print(f\"Filter {col}=True: {before} -> {len(df)}\")\n",
        "\n",
        "# hanya manual + auto_confident\n",
        "df_eval = df[df[\"label_source\"].isin([\"manual\",\"auto_confident\"])].copy()\n",
        "df_eval[\"y_final\"] = pd.to_numeric(df_eval[\"y_final\"], errors=\"coerce\")\n",
        "df_eval = df_eval[df_eval[\"y_final\"].isin([0,1])].copy()\n",
        "df_eval[\"y_final\"] = df_eval[\"y_final\"].astype(int)\n",
        "df_eval = df_eval.reset_index(drop=True)\n",
        "\n",
        "print(\"\\nEVAL rows (manual+auto_confident):\", len(df_eval))\n",
        "print(df_eval[\"label_source\"].value_counts())\n",
        "print(\"Label dist:\", df_eval[\"y_final\"].value_counts().to_dict())\n",
        "\n",
        "# sanity check path sample\n",
        "for p in df_eval[\"rgb_patch_path\"].astype(str).head(3).tolist():\n",
        "    if not Path(p).exists():\n",
        "        raise FileNotFoundError(f\"Contoh rgb_patch_path tidak ditemukan:\\n{p}\")\n",
        "for p in df_eval[\"thermal_patch_path\"].astype(str).head(3).tolist():\n",
        "    if not Path(p).exists():\n",
        "        raise FileNotFoundError(f\"Contoh thermal_patch_path tidak ditemukan:\\n{p}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Fold by frame_id\n",
        "# ---------------------------\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "df_eval[\"fold\"] = -1\n",
        "for fold, (_, va_idx) in enumerate(gkf.split(df_eval, df_eval[\"y_final\"], groups=df_eval[\"frame_id\"])):\n",
        "    df_eval.loc[va_idx, \"fold\"] = fold\n",
        "assert (df_eval[\"fold\"] >= 0).all()\n",
        "print(\"\\nFold sizes:\", df_eval[\"fold\"].value_counts().sort_index().to_dict())\n",
        "\n",
        "# ---------------------------\n",
        "# 5) tf.data loaders\n",
        "# ---------------------------\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def _np_load_npy(path_bytes):\n",
        "    p = path_bytes.decode(\"utf-8\")\n",
        "    arr = np.load(p).astype(np.float32)\n",
        "    return arr\n",
        "\n",
        "def _load_rgb(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "def _load_th(path):\n",
        "    th = tf.numpy_function(_np_load_npy, [path], tf.float32)\n",
        "    th.set_shape((IMG_SIZE, IMG_SIZE))\n",
        "    th = tf.expand_dims(th, axis=-1)\n",
        "    # per-patch z-score\n",
        "    mean = tf.reduce_mean(th)\n",
        "    std = tf.math.reduce_std(th)\n",
        "    th = (th - mean) / (std + 1e-6)\n",
        "    return th\n",
        "\n",
        "def _load_fusion(rgb_path, th_path, y):\n",
        "    rgb = _load_rgb(rgb_path)\n",
        "    th  = _load_th(th_path)\n",
        "    return (rgb, th), tf.cast(y, tf.float32)\n",
        "\n",
        "def make_ds_fusion(df_part: pd.DataFrame, training: bool):\n",
        "    rgb_paths = df_part[\"rgb_patch_path\"].astype(str).values\n",
        "    th_paths  = df_part[\"thermal_patch_path\"].astype(str).values\n",
        "    labels    = df_part[\"y_final\"].astype(int).values\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((rgb_paths, th_paths, labels))\n",
        "    ds = ds.map(lambda rp, tp, y: _load_fusion(rp, tp, y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if training:\n",
        "        ds = ds.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
        "\n",
        "    ds = ds.batch(BATCH).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Late-fusion model\n",
        "# ---------------------------\n",
        "def build_late_fusion_model(lr=LR):\n",
        "    # RGB branch (MobileNetV2 feature extractor)\n",
        "    rgb_in = tf.keras.Input((IMG_SIZE, IMG_SIZE, 3), name=\"rgb\")\n",
        "    x_rgb = tf.keras.applications.mobilenet_v2.preprocess_input(rgb_in * 255.0)\n",
        "    base_rgb = tf.keras.applications.MobileNetV2(\n",
        "        include_top=False, weights=\"imagenet\", input_tensor=x_rgb\n",
        "    )\n",
        "    base_rgb.trainable = False\n",
        "    x_rgb = base_rgb.output\n",
        "    x_rgb = tf.keras.layers.GlobalAveragePooling2D()(x_rgb)\n",
        "\n",
        "    # Thermal branch (small CNN)\n",
        "    th_in = tf.keras.Input((IMG_SIZE, IMG_SIZE, 1), name=\"th\")\n",
        "    x_th = tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(th_in)\n",
        "    x_th = tf.keras.layers.MaxPool2D()(x_th)\n",
        "    x_th = tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x_th)\n",
        "    x_th = tf.keras.layers.MaxPool2D()(x_th)\n",
        "    x_th = tf.keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x_th)\n",
        "    x_th = tf.keras.layers.GlobalAveragePooling2D()(x_th)\n",
        "\n",
        "    # Fusion head\n",
        "    x = tf.keras.layers.Concatenate()([x_rgb, x_th])\n",
        "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.35)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[rgb_in, th_in], outputs=out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\n",
        "            tf.keras.metrics.AUC(name=\"roc_auc\"),\n",
        "            tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\"),\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Metrics helper + threshold search\n",
        "# ---------------------------\n",
        "def best_thr_for_accuracy(y_true, y_prob):\n",
        "    ths = np.linspace(0.05, 0.95, 181)\n",
        "    best_t, best_acc = 0.5, -1\n",
        "    for t in ths:\n",
        "        acc = accuracy_score(y_true, (y_prob >= t).astype(int))\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_t = float(t)\n",
        "    return best_t, best_acc\n",
        "\n",
        "def compute_metrics(y_true, y_prob, thr):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "        \"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "        \"pr_auc\": float(average_precision_score(y_true, y_prob)) if len(np.unique(y_true)) > 1 else float(\"nan\"),\n",
        "        \"cm\": confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# 8) K-Fold Training + OOF\n",
        "# ---------------------------\n",
        "fusion_oof_rows = []\n",
        "fold_metrics = []\n",
        "cm_sum = np.zeros((2,2), dtype=int)\n",
        "\n",
        "for f in range(N_SPLITS):\n",
        "    tr = df_eval[df_eval[\"fold\"] != f].copy()\n",
        "    va = df_eval[df_eval[\"fold\"] == f].copy()\n",
        "\n",
        "    ds_tr = make_ds_fusion(tr, training=True)\n",
        "    ds_va = make_ds_fusion(va, training=False)\n",
        "\n",
        "    # class_weight\n",
        "    y_tr = tr[\"y_final\"].values\n",
        "    classes = np.array([0,1])\n",
        "    cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
        "    class_weight = {0: float(cw[0]), 1: float(cw[1])}\n",
        "\n",
        "    model = build_late_fusion_model(lr=LR)\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6),\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        ds_tr,\n",
        "        validation_data=ds_va,\n",
        "        epochs=EPOCHS,\n",
        "        verbose=0,\n",
        "        class_weight=class_weight,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    y_prob = model.predict(ds_va, verbose=0).ravel()\n",
        "    y_true = va[\"y_final\"].values.astype(int)\n",
        "\n",
        "    thr_fold, _ = best_thr_for_accuracy(y_true, y_prob)\n",
        "\n",
        "    m = compute_metrics(y_true, y_prob, thr_fold)\n",
        "    m_out = {k:v for k,v in m.items() if k != \"cm\"}\n",
        "    m_out[\"thr_used\"] = float(thr_fold)\n",
        "    m_out[\"fold\"] = int(f)\n",
        "    m_out[\"n_val\"] = int(len(va))\n",
        "    fold_metrics.append(m_out)\n",
        "    cm_sum += m[\"cm\"]\n",
        "\n",
        "    for pid, fid, yt, yp in zip(va[\"patch_id\"].astype(str).values,\n",
        "                                va[\"frame_id\"].astype(str).values,\n",
        "                                y_true, y_prob):\n",
        "        fusion_oof_rows.append({\n",
        "            \"patch_id\": pid,\n",
        "            \"frame_id\": fid,\n",
        "            \"fold\": int(f),\n",
        "            \"y_true\": int(yt),\n",
        "            \"y_prob\": float(yp),\n",
        "            \"thr_used\": float(thr_fold),\n",
        "            \"y_pred\": int(yp >= thr_fold),\n",
        "            \"label_source\": \"manual_or_autoconf\"\n",
        "        })\n",
        "\n",
        "# ---------------------------\n",
        "# 9) Report + Save\n",
        "# ---------------------------\n",
        "met_df = pd.DataFrame(fold_metrics).sort_values(\"fold\").reset_index(drop=True)\n",
        "print(\"\\nFUSION (manual+auto_confident) per-fold:\")\n",
        "display(met_df)\n",
        "\n",
        "print(\"\\nMean Â± Std:\")\n",
        "display(met_df.drop(columns=[\"fold\"]).agg([\"mean\",\"std\"]))\n",
        "\n",
        "print(\"\\nConfusion Matrix (aggregated):\\n\", cm_sum)\n",
        "\n",
        "oof_df = pd.DataFrame(fusion_oof_rows)\n",
        "oof_df.to_csv(OUT_OOF, index=False)\n",
        "met_df.to_csv(OUT_MET, index=False)\n",
        "\n",
        "print(\"\\nâœ… Saved OOF:\", OUT_OOF)\n",
        "print(\"âœ… Saved metrics:\", OUT_MET)\n",
        "print(\"OOF shape:\", oof_df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsGpqmhg44aA"
      },
      "source": [
        "# TAHAP 6 - Final Training & Packaging untuk Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJQ6sVo847dT"
      },
      "source": [
        "## 6.1 Siapkan Dataset Final untuk Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukiVn33X4_Hz"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 6.1 â€” FINAL DEPLOY SET (manual + auto_confident)\n",
        "# ============================================\n",
        "\n",
        "DEPLOY_SOURCES = [\"manual\", \"auto_confident\"]\n",
        "\n",
        "df_final = df_train[df_train[\"label_source\"].isin(DEPLOY_SOURCES)].copy()\n",
        "\n",
        "print(\"FINAL DEPLOY rows:\", len(df_final))\n",
        "print(\"Frames:\", df_final[\"frame_id\"].nunique())\n",
        "print(\"Counts per label_source:\")\n",
        "display(df_final[\"label_source\"].value_counts())\n",
        "print(\"Label dist:\")\n",
        "display(df_final[\"y_final\"].value_counts())\n",
        "\n",
        "# dataset final (training only)\n",
        "ds_final = make_ds(df_final, training=True)\n",
        "print(\"âœ… ds_final ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3qVXTY65D1L"
      },
      "source": [
        "## 6.2 Final Training (Train sekali, pakai semua data deploy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOhEAQZqxEOZ"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 6 â€” FINAL TRAIN & SAVE (RGB-only)\n",
        "# Dataset: patches_all_final_labeled_topk_10pct.csv\n",
        "# Subset : manual + auto_confident\n",
        "# Split  : GroupShuffleSplit by frame_id (anti leakage)\n",
        "# Output : 1 file .keras untuk deployment\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "FIX_ROOT = Path(\"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/DatasetTerbaruFIX\")\n",
        "FINAL_LABELED_CSV = FIX_ROOT / \"labels\" / \"final\" / \"patches_all_final_labeled_topk_10pct.csv\"\n",
        "\n",
        "MODEL_DIR = FIX_ROOT / \"models\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUT_RGB = MODEL_DIR / \"model_rgb_final_topk10_manual_autoconf.keras\"\n",
        "TMP_BEST = MODEL_DIR / \"tmp_best_rgb.keras\"\n",
        "\n",
        "IMG_SIZE = 128\n",
        "BATCH = 64\n",
        "LR_HEAD = 1e-4\n",
        "LR_FT   = 5e-5\n",
        "EPOCHS_HEAD = 30\n",
        "EPOCHS_FT   = 10\n",
        "PATIENCE_HEAD = 4\n",
        "PATIENCE_FT   = 3\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# -------- load df --------\n",
        "df = pd.read_csv(FINAL_LABELED_CSV)\n",
        "\n",
        "for col in [\"keep_for_model\", \"qc_canopy_ok\"]:\n",
        "    if col in df.columns:\n",
        "        df = df[df[col] == True].copy()\n",
        "\n",
        "df = df[df[\"label_source\"].isin([\"manual\",\"auto_confident\"])].copy()\n",
        "df[\"y_final\"] = pd.to_numeric(df[\"y_final\"], errors=\"coerce\")\n",
        "df = df[df[\"y_final\"].isin([0,1])].copy()\n",
        "df[\"y_final\"] = df[\"y_final\"].astype(int)\n",
        "\n",
        "# -------- split by frame_id (80/20) --------\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\n",
        "tr_idx, va_idx = next(gss.split(df, df[\"y_final\"], groups=df[\"frame_id\"]))\n",
        "tr = df.iloc[tr_idx].copy().reset_index(drop=True)\n",
        "va = df.iloc[va_idx].copy().reset_index(drop=True)\n",
        "\n",
        "print(\"Train:\", len(tr), \"Val:\", len(va))\n",
        "print(\"Train dist:\", tr[\"y_final\"].value_counts().to_dict())\n",
        "print(\"Val dist:\", va[\"y_final\"].value_counts().to_dict())\n",
        "\n",
        "# -------- class_weight --------\n",
        "y_tr = tr[\"y_final\"].values\n",
        "cw = compute_class_weight(class_weight=\"balanced\", classes=np.array([0,1]), y=y_tr)\n",
        "class_weight = {0: float(cw[0]), 1: float(cw[1])}\n",
        "print(\"class_weight:\", class_weight)\n",
        "\n",
        "# -------- tf.data RGB --------\n",
        "def tf_load_rgb(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "def make_ds_rgb(df_part, training):\n",
        "    paths = df_part[\"rgb_patch_path\"].astype(str).values\n",
        "    labels = df_part[\"y_final\"].astype(int).values\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "\n",
        "    def _map(p, y):\n",
        "        img = tf_load_rgb(p)\n",
        "        return img, tf.cast(y, tf.float32)\n",
        "\n",
        "    ds = ds.map(_map, num_parallel_calls=AUTOTUNE)\n",
        "    if training:\n",
        "        ds = ds.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
        "    return ds.batch(BATCH).prefetch(AUTOTUNE)\n",
        "\n",
        "# -------- model RGB-only --------\n",
        "def build_rgb_only_model(lr, train_base=False):\n",
        "    base = tf.keras.applications.MobileNetV2(\n",
        "        include_top=False, weights=\"imagenet\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        "    )\n",
        "    base.trainable = bool(train_base)\n",
        "\n",
        "    inp = tf.keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    x = tf.keras.applications.mobilenet_v2.preprocess_input(inp * 255.0)\n",
        "    x = base(x, training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.25)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[tf.keras.metrics.AUC(name=\"roc_auc\"),\n",
        "                 tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\")]\n",
        "    )\n",
        "    return model, base\n",
        "\n",
        "ds_tr = make_ds_rgb(tr, training=True)\n",
        "ds_va = make_ds_rgb(va, training=False)\n",
        "\n",
        "# Stage A: head\n",
        "model, base = build_rgb_only_model(LR_HEAD, train_base=False)\n",
        "cb_head = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(str(TMP_BEST), monitor=\"val_loss\", save_best_only=True),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE_HEAD, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6),\n",
        "]\n",
        "model.fit(ds_tr, validation_data=ds_va, epochs=EPOCHS_HEAD, verbose=1,\n",
        "          class_weight=class_weight, callbacks=cb_head)\n",
        "\n",
        "# Stage B: finetune last 30 layers\n",
        "base.trainable = True\n",
        "for lyr in base.layers[:-30]:\n",
        "    lyr.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(LR_FT),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[tf.keras.metrics.AUC(name=\"roc_auc\"),\n",
        "             tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\")]\n",
        ")\n",
        "cb_ft = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(str(TMP_BEST), monitor=\"val_loss\", save_best_only=True),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE_FT, restore_best_weights=True),\n",
        "]\n",
        "model.fit(ds_tr, validation_data=ds_va, epochs=EPOCHS_FT, verbose=1,\n",
        "          class_weight=class_weight, callbacks=cb_ft)\n",
        "\n",
        "best = tf.keras.models.load_model(str(TMP_BEST))\n",
        "best.save(str(OUT_RGB))\n",
        "print(\"âœ… Saved RGB final:\", OUT_RGB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIW_lp0SH_Zu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from sklearn.utils.class_weight import compute_class_weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FUXoChyyxqt"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 6 â€” FINAL TRAIN & SAVE (FUSION)\n",
        "# Dataset: patches_all_final_labeled_topk_10pct.csv\n",
        "# Subset : manual + auto_confident\n",
        "# Split  : GroupShuffleSplit by frame_id\n",
        "# Output : 1 file .keras untuk deployment (fusion)\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# ---------------- config ----------------\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "FIX_ROOT = Path(\"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/DatasetTerbaruFIX\")\n",
        "FINAL_LABELED_CSV = FIX_ROOT / \"labels\" / \"final\" / \"patches_all_final_labeled_topk_10pct.csv\"\n",
        "\n",
        "MODEL_DIR = FIX_ROOT / \"models\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUT_FUSION = MODEL_DIR / \"model_fusion_final_topk10_manual_autoconf.keras\"\n",
        "TMP_BEST   = MODEL_DIR / \"tmp_best_fusion.keras\"\n",
        "\n",
        "IMG_SIZE = 128\n",
        "BATCH = 64\n",
        "LR_HEAD = 1e-4\n",
        "LR_FT   = 5e-5\n",
        "EPOCHS_HEAD = 30\n",
        "EPOCHS_FT   = 10\n",
        "PATIENCE_HEAD = 4\n",
        "PATIENCE_FT   = 3\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# ---------------- load & filter df ----------------\n",
        "df = pd.read_csv(FINAL_LABELED_CSV)\n",
        "\n",
        "for col in [\"keep_for_model\", \"qc_canopy_ok\"]:\n",
        "    if col in df.columns:\n",
        "        df = df[df[col] == True].copy()\n",
        "\n",
        "df = df[df[\"label_source\"].isin([\"manual\", \"auto_confident\"])].copy()\n",
        "df[\"y_final\"] = pd.to_numeric(df[\"y_final\"], errors=\"coerce\")\n",
        "df = df[df[\"y_final\"].isin([0, 1])].copy()\n",
        "df[\"y_final\"] = df[\"y_final\"].astype(int)\n",
        "\n",
        "# ---------------- split by frame_id ----------------\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\n",
        "tr_idx, va_idx = next(gss.split(df, df[\"y_final\"], groups=df[\"frame_id\"]))\n",
        "tr = df.iloc[tr_idx].reset_index(drop=True)\n",
        "va = df.iloc[va_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"Train:\", len(tr), \"Val:\", len(va))\n",
        "print(\"Train dist:\", tr[\"y_final\"].value_counts().to_dict())\n",
        "print(\"Val dist:\", va[\"y_final\"].value_counts().to_dict())\n",
        "\n",
        "# ---------------- class weight ----------------\n",
        "cw = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.array([0, 1]),\n",
        "    y=tr[\"y_final\"].values\n",
        ")\n",
        "class_weight = {0: float(cw[0]), 1: float(cw[1])}\n",
        "print(\"class_weight:\", class_weight)\n",
        "\n",
        "# ---------------- tf.data loaders ----------------\n",
        "def tf_load_rgb(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "def tf_load_thermal_npy(path):\n",
        "    th = tf.numpy_function(\n",
        "        lambda p: np.load(p.decode(\"utf-8\")).astype(np.float32),\n",
        "        [path],\n",
        "        tf.float32\n",
        "    )\n",
        "    th.set_shape([None, None])\n",
        "    th = tf.expand_dims(th, -1)\n",
        "    th = tf.image.resize(th, (IMG_SIZE, IMG_SIZE))\n",
        "    th = (th - tf.reduce_min(th)) / (tf.reduce_max(th) - tf.reduce_min(th) + 1e-6)\n",
        "    return th\n",
        "\n",
        "def make_ds_fusion(df_part, training):\n",
        "    rgb_paths = df_part[\"rgb_patch_path\"].astype(str).values\n",
        "    th_paths  = df_part[\"thermal_patch_path\"].astype(str).values\n",
        "    labels    = df_part[\"y_final\"].values\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((rgb_paths, th_paths, labels))\n",
        "\n",
        "    def _map(rp, tp, y):\n",
        "        rgb = tf_load_rgb(rp)          # (H,W,3)\n",
        "        th  = tf_load_thermal_npy(tp)  # (H,W,1)\n",
        "        x   = tf.concat([rgb, th], axis=-1)  # (H,W,4)\n",
        "        return x, tf.cast(y, tf.float32)\n",
        "\n",
        "    ds = ds.map(_map, num_parallel_calls=AUTOTUNE)\n",
        "    if training:\n",
        "        ds = ds.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
        "    return ds.batch(BATCH).prefetch(AUTOTUNE)\n",
        "\n",
        "ds_tr = make_ds_fusion(tr, training=True)\n",
        "ds_va = make_ds_fusion(va, training=False)\n",
        "\n",
        "# ---------------- fusion model ----------------\n",
        "def build_fusion_model(lr, train_base=False):\n",
        "    inp = tf.keras.Input((IMG_SIZE, IMG_SIZE, 4))\n",
        "\n",
        "    # adaptor 4 â†’ 3 (biar bisa pakai ImageNet)\n",
        "    x = tf.keras.layers.Conv2D(3, 1, padding=\"same\", name=\"adapt_4to3\")(inp)\n",
        "    x = tf.keras.applications.mobilenet_v2.preprocess_input(x * 255.0)\n",
        "\n",
        "    base = tf.keras.applications.MobileNetV2(\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        "    )\n",
        "    base.trainable = bool(train_base)\n",
        "\n",
        "    x = base(x, training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.25)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\n",
        "            tf.keras.metrics.AUC(name=\"roc_auc\"),\n",
        "            tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\")\n",
        "        ]\n",
        "    )\n",
        "    return model, base\n",
        "\n",
        "# ---------------- Stage A: head ----------------\n",
        "model, base = build_fusion_model(LR_HEAD, train_base=False)\n",
        "\n",
        "cb_head = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(str(TMP_BEST), monitor=\"val_loss\", save_best_only=True),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE_HEAD, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6),\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    ds_tr,\n",
        "    validation_data=ds_va,\n",
        "    epochs=EPOCHS_HEAD,\n",
        "    verbose=1,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=cb_head\n",
        ")\n",
        "\n",
        "# ---------------- Stage B: finetune ----------------\n",
        "base.trainable = True\n",
        "for lyr in base.layers[:-30]:\n",
        "    lyr.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(LR_FT),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\n",
        "        tf.keras.metrics.AUC(name=\"roc_auc\"),\n",
        "        tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "cb_ft = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(str(TMP_BEST), monitor=\"val_loss\", save_best_only=True),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE_FT, restore_best_weights=True),\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    ds_tr,\n",
        "    validation_data=ds_va,\n",
        "    epochs=EPOCHS_FT,\n",
        "    verbose=1,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=cb_ft\n",
        ")\n",
        "\n",
        "# ---------------- save final ----------------\n",
        "best = tf.keras.models.load_model(str(TMP_BEST))\n",
        "best.save(str(OUT_FUSION))\n",
        "print(\"âœ… Saved FUSION final:\", OUT_FUSION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxCuzCZd5Fhm"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 6.2 â€” FINAL TRAINING (Late Fusion) for DEPLOY\n",
        "# ============================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_FINAL = MODEL_DIR / \"model_final_deploy_flask_fix.keras\"\n",
        "\n",
        "# model baru\n",
        "model_final = build_late_fusion_model(img_size=IMG_SIZE, lr=1e-4)\n",
        "\n",
        "# callback: simpan model terbaik berdasar training loss (karena tidak ada val)\n",
        "cb = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(OUT_FINAL),\n",
        "        monitor=\"loss\", mode=\"min\",\n",
        "        save_best_only=True, verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# training\n",
        "EPOCHS = 20\n",
        "hist = model_final.fit(ds_final, epochs=EPOCHS, callbacks=cb, verbose=1)\n",
        "\n",
        "print(\"âœ… Saved final deploy model:\", OUT_FINAL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8OyIm2W5Iw6"
      },
      "source": [
        "## 6.3 Fine-tuning ringan MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-08zKmw5KkJ"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 6.3 â€” OPTIONAL: Fine-tune last layers (small LR)\n",
        "# ============================================\n",
        "\n",
        "# load kembali best model dari 6.2 (biar starting point yang bagus)\n",
        "model_ft = tf.keras.models.load_model(str(OUT_FINAL))\n",
        "\n",
        "# buka sebagian layer backbone mobilenet (rgb branch)\n",
        "# cari layer mobilenetv2 (nama bisa beda, tapi di summary biasanya \"mobilenetv2_1.00_128\")\n",
        "for layer in model_ft.layers:\n",
        "    if \"mobilenetv2\" in layer.name.lower():\n",
        "        backbone = layer\n",
        "        break\n",
        "else:\n",
        "    backbone = None\n",
        "\n",
        "print(\"Backbone found:\", backbone.name if backbone else \"NOT FOUND\")\n",
        "\n",
        "if backbone is not None:\n",
        "    backbone.trainable = True\n",
        "    # freeze sebagian besar, unfreeze ~20 layer terakhir\n",
        "    for l in backbone.layers[:-20]:\n",
        "        l.trainable = False\n",
        "\n",
        "    model_ft.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\n",
        "            tf.keras.metrics.AUC(name=\"roc_auc\"),\n",
        "            tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\"),\n",
        "            tf.keras.metrics.Recall(name=\"recall\"),\n",
        "            tf.keras.metrics.Precision(name=\"precision\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    OUT_FINAL_FT = MODEL_DIR / \"model_final_deploy_ft.keras\"\n",
        "    cb_ft = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=str(OUT_FINAL_FT),\n",
        "            monitor=\"loss\", mode=\"min\",\n",
        "            save_best_only=True, verbose=1\n",
        "        )\n",
        "    ]\n",
        "    model_ft.fit(ds_final, epochs=10, callbacks=cb_ft, verbose=1)\n",
        "    print(\"âœ… Saved fine-tuned deploy model:\", OUT_FINAL_FT)\n",
        "else:\n",
        "    print(\"Skip fine-tune (backbone not detected).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv_xKMfk5Nhx"
      },
      "source": [
        "## 6.4 Simpan â€œInference Configâ€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr5JTs_o5Pc1"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 6.4 â€” SAVE INFERENCE CONFIG (json)\n",
        "# ============================================\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "INFER_CFG = {\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "    \"img_size\": int(IMG_SIZE),\n",
        "    \"batch\": int(BATCH),\n",
        "    \"threshold\": 0.5,              # default classification threshold\n",
        "    \"label_sources_deploy\": DEPLOY_SOURCES,\n",
        "    \"thermal_norm\": {\n",
        "        \"method\": \"percentile_minmax\",\n",
        "        \"p1\": 1,\n",
        "        \"p99\": 99,\n",
        "        \"within\": \"canopy_mask_v2\"\n",
        "    },\n",
        "    \"canopy_mask\": {\n",
        "        \"name\": \"canopy_mask_v2_from_rgb\",\n",
        "        \"exg_p2_p98\": True,\n",
        "        \"green_hsv\": {\"H\": [25,95], \"S_min\": 30, \"V_min\": 30},\n",
        "        \"green_exg_thr\": 0.35,\n",
        "        \"morph\": {\"open\": 1, \"close\": 2, \"kernel\": [5,5]}\n",
        "    },\n",
        "    \"note\": \"Final deploy training uses all deploy-set (manual + auto_confident). Evaluation reported separately using GOLD K-fold.\"\n",
        "}\n",
        "\n",
        "CFG_PATH = MODEL_DIR / \"inference_configv2.json\"\n",
        "with open(CFG_PATH, \"w\") as f:\n",
        "    json.dump(INFER_CFG, f, indent=2)\n",
        "\n",
        "print(\"âœ… Saved config:\", CFG_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79NmAxso5RoA"
      },
      "source": [
        "## 6.5 Sanity Check Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9hfZyde5TYy"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 6.5 â€” SANITY INFERENCE CHECK\n",
        "# ============================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# pilih model mana yang dipakai\n",
        "MODEL_PATH = str(OUT_FINAL)  # atau str(OUT_FINAL_FT) kalau fine-tuning dipakai\n",
        "m = tf.keras.models.load_model(MODEL_PATH)\n",
        "print(\"Loaded:\", MODEL_PATH)\n",
        "\n",
        "# ambil sampel kecil\n",
        "df_sample = df_final.sample(16, random_state=SEED).copy()\n",
        "ds_sample = make_ds(df_sample, training=False)\n",
        "\n",
        "y_prob = m.predict(ds_sample).ravel()\n",
        "y_true = df_sample[\"y_final\"].astype(int).values\n",
        "y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "print(\"Sample preds (first 10):\")\n",
        "for i in range(min(10, len(y_prob))):\n",
        "    print(df_sample.iloc[i][\"patch_id\"], \"true=\", y_true[i], \"prob=\", float(y_prob[i]), \"pred=\", int(y_pred[i]))\n",
        "\n",
        "print(\"âœ… Sanity check done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQRS0vJrDkxN"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 6.5 â€” SANITY INFERENCE CHECK\n",
        "# ============================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# pilih model mana yang dipakai\n",
        "MODEL_PATH = str(OUT_FINAL_FT)  # atau str(OUT_FINAL_FT) kalau fine-tuning dipakai\n",
        "m = tf.keras.models.load_model(MODEL_PATH)\n",
        "print(\"Loaded:\", MODEL_PATH)\n",
        "\n",
        "# ambil sampel kecil\n",
        "df_sample = df_final.sample(16, random_state=SEED).copy()\n",
        "ds_sample = make_ds(df_sample, training=False)\n",
        "\n",
        "y_prob = m.predict(ds_sample).ravel()\n",
        "y_true = df_sample[\"y_final\"].astype(int).values\n",
        "y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "print(\"Sample preds (first 10):\")\n",
        "for i in range(min(10, len(y_prob))):\n",
        "    print(df_sample.iloc[i][\"patch_id\"], \"true=\", y_true[i], \"prob=\", float(y_prob[i]), \"pred=\", int(y_pred[i]))\n",
        "\n",
        "print(\"âœ… Sanity check done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVpDUCzAZ4f9"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# FINAL TRAINING â€” RGB ONLY (DEPLOY-set) + SAVE .keras\n",
        "# =====================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# --- 0) Output path\n",
        "OUT_RGB_DEPLOY = MODEL_DIR / \"model_final_deploy_rgb_onlyv2.keras\"\n",
        "print(\"Will save to:\", OUT_RGB_DEPLOY)\n",
        "\n",
        "# --- 1) Cari dataframe deploy (pilih yang paling masuk akal)\n",
        "# Prioritas: deploy_df / df_deploy / train_df / df_train\n",
        "candidates = [\n",
        "    \"deploy_df\", \"df_deploy\",\n",
        "    \"train_df\", \"df_train\",\n",
        "    \"df_train_full\", \"train_df_full\",\n",
        "    \"df_all\", \"df_final\", \"final_df\"\n",
        "]\n",
        "\n",
        "df_deploy = None\n",
        "for name in candidates:\n",
        "    if name in globals() and isinstance(globals()[name], pd.DataFrame):\n",
        "        tmp = globals()[name]\n",
        "        # syarat minimal harus punya label y_final\n",
        "        if \"y_final\" in tmp.columns:\n",
        "            df_deploy = tmp.copy()\n",
        "            print(f\"âœ… Using dataframe: {name} | rows={len(df_deploy)}\")\n",
        "            break\n",
        "\n",
        "# Kalau belum ketemu, kamu SET manual di sini:\n",
        "# df_deploy = df_train.copy()\n",
        "\n",
        "assert df_deploy is not None, \"âŒ Tidak menemukan dataframe deploy. Set manual: df_deploy = <df kamu>\"\n",
        "\n",
        "# --- 2) (Opsional tapi disarankan) filter hanya label yang kamu anggap 'deploy'\n",
        "# Kalau dataset deploy kamu memang gabungan manual + auto_confident:\n",
        "# - keep semuanya, atau\n",
        "# - keep auto_confident + manual saja (buang yang noisy/uncertain)\n",
        "if \"label_source\" in df_deploy.columns:\n",
        "    # default: keep manual + auto_confident (yang kamu pakai untuk training deploy biasanya)\n",
        "    keep = df_deploy[\"label_source\"].isin([\"manual\", \"auto_confident\"])\n",
        "    if keep.any():\n",
        "        df_deploy = df_deploy[keep].copy()\n",
        "        print(\"Filtered label_source to {manual, auto_confident} -> rows:\", len(df_deploy))\n",
        "\n",
        "print(\"DEPLOY label dist:\")\n",
        "print(df_deploy[\"y_final\"].value_counts(dropna=False))\n",
        "\n",
        "# --- 3) Dataset RGB-only: reuse make_ds(df, training=...) yang sudah kamu punya\n",
        "# make_ds harus menghasilkan ((rgb, th), y) atau dict {'rgb','th'}\n",
        "def make_ds_rgb_only(df, training):\n",
        "    ds = make_ds(df, training=training)\n",
        "\n",
        "    def take_rgb(x, y):\n",
        "        # case A: ((rgb, th), y)\n",
        "        if isinstance(x, (tuple, list)):\n",
        "            return x[0], y\n",
        "        # case B: {'rgb':..., 'th':...}\n",
        "        if isinstance(x, dict):\n",
        "            return x[\"rgb\"], y\n",
        "        # case C: already rgb\n",
        "        return x, y\n",
        "\n",
        "    return ds.map(take_rgb, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "ds_train = make_ds_rgb_only(df_deploy, training=True)\n",
        "\n",
        "# --- 4) Build RGB-only model (MobileNetV2 baseline)\n",
        "def build_rgb_only_model(img_size=128, lr=1e-4):\n",
        "    inp = tf.keras.Input((img_size, img_size, 3), name=\"rgb\")\n",
        "    base = tf.keras.applications.MobileNetV2(\n",
        "        include_top=False, input_tensor=inp, weights=\"imagenet\"\n",
        "    )\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(base.output)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    m = tf.keras.Model(inp, out, name=\"rgb_only_deploy\")\n",
        "    m.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\n",
        "            tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.5),\n",
        "            tf.keras.metrics.AUC(curve=\"ROC\", name=\"roc_auc\"),\n",
        "            tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\"),\n",
        "            tf.keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n",
        "            tf.keras.metrics.Recall(name=\"recall\", thresholds=0.5),\n",
        "        ],\n",
        "    )\n",
        "    return m\n",
        "\n",
        "model_rgb_deploy = build_rgb_only_model(img_size=IMG_SIZE, lr=1e-4)\n",
        "model_rgb_deploy.summary()\n",
        "\n",
        "# --- 5) Train FINAL (deploy) â€” tidak pakai K-Fold\n",
        "# (kalau kamu punya ds_val deploy, boleh tambah validation_data=..., tapi ini opsional untuk deploy)\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "history = model_rgb_deploy.fit(\n",
        "    ds_train,\n",
        "    epochs=15,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- 6) SAVE\n",
        "model_rgb_deploy.save(OUT_RGB_DEPLOY)\n",
        "print(\"âœ… Saved RGB-only DEPLOY model:\", OUT_RGB_DEPLOY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDfa5IRDFbl0"
      },
      "source": [
        "# TAHAP 7 - Flask App & DSS Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj2kIjF-Fcz9"
      },
      "source": [
        "## Imports + Paths + Load Model & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAfUXG4uFouf"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 7.1 â€” Core Inference Setup (Colab)\n",
        "# - Load model_final_deploy.keras\n",
        "# - Load inference_config.json\n",
        "# ============================================\n",
        "\n",
        "import os, json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "# ---- Paths (sesuaikan kalau perlu) ----\n",
        "MODEL_DIR = FIX_ROOT / \"models\"\n",
        "MODEL_PATH = MODEL_DIR / \"model_final_deploy.keras\"   # gunakan yg kamu pilih\n",
        "CFG_PATH   = MODEL_DIR / \"inference_config.json\"\n",
        "\n",
        "assert MODEL_PATH.exists(), f\"Model not found: {MODEL_PATH}\"\n",
        "assert CFG_PATH.exists(),   f\"Config not found: {CFG_PATH}\"\n",
        "\n",
        "# ---- Load config ----\n",
        "with open(CFG_PATH, \"r\") as f:\n",
        "    INFER_CFG = json.load(f)\n",
        "\n",
        "THR = float(INFER_CFG.get(\"threshold\", 0.5))\n",
        "IMG_SIZE = int(INFER_CFG.get(\"img_size\", 128))\n",
        "\n",
        "print(\"âœ… Loaded config:\", CFG_PATH)\n",
        "print(\"IMG_SIZE:\", IMG_SIZE, \"| THR:\", THR)\n",
        "\n",
        "# ---- Load model ----\n",
        "model = tf.keras.models.load_model(str(MODEL_PATH))\n",
        "print(\"âœ… Loaded model:\", MODEL_PATH)\n",
        "print(\"Model inputs:\", [t.shape for t in model.inputs])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_AJwKZBFujE"
      },
      "source": [
        "## Utility: Load RGB & Thermal Frame (npy) + Align"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pexsno9FscZ"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 7.2 (REPLACE) â€” Load + Align RGB->Thermal grid (crop+zoom)\n",
        "# ============================================\n",
        "\n",
        "import cv2, numpy as np\n",
        "\n",
        "def center_crop(img, crop_h, crop_w):\n",
        "    H, W = img.shape[:2]\n",
        "    y0 = (H - crop_h) // 2\n",
        "    x0 = (W - crop_w) // 2\n",
        "    return img[y0:y0+crop_h, x0:x0+crop_w], (x0, y0, crop_w, crop_h)\n",
        "\n",
        "def rgb_to_thermal_grid(rgb_bgr, th_shape_hw, zoom=2.2):\n",
        "    \"\"\"\n",
        "    RGB wide -> crop center (zoom-in) -> resize to thermal grid (Hth,Wth)\n",
        "    Return: rgb_aligned_bgr (Hth,Wth,3), roi on original RGB\n",
        "    \"\"\"\n",
        "    Hth, Wth = th_shape_hw\n",
        "    Hr, Wr = rgb_bgr.shape[:2]\n",
        "    crop_h = int(Hr / zoom)\n",
        "    crop_w = int(Wr / zoom)\n",
        "    rgb_crop, roi = center_crop(rgb_bgr, crop_h, crop_w)\n",
        "    rgb_aligned = cv2.resize(rgb_crop, (Wth, Hth), interpolation=cv2.INTER_LINEAR)\n",
        "    return rgb_aligned, roi\n",
        "\n",
        "def load_rgb_bgr(rgb_path: str) -> np.ndarray:\n",
        "    rgb = cv2.imread(str(rgb_path), cv2.IMREAD_COLOR)\n",
        "    if rgb is None:\n",
        "        raise FileNotFoundError(rgb_path)\n",
        "    return rgb  # BGR uint8\n",
        "\n",
        "def load_thermal_npy(th_path: str) -> np.ndarray:\n",
        "    th = np.load(str(th_path)).astype(np.float32)\n",
        "    if th.ndim != 2:\n",
        "        raise ValueError(f\"Thermal must be 2D, got: {th.shape}\")\n",
        "    return th\n",
        "\n",
        "print(\"âœ… 7.2 (replace) ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt2_ufHBFyal"
      },
      "source": [
        "## Patch Grid Extractor (RGB + Thermal) + Masking + Normalisasi Thermal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noo5PjiEFwg_"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 7.3 (REPLACE) â€” Extract patches on thermal grid\n",
        "# ============================================\n",
        "\n",
        "def extract_paired_patches_on_thermal_grid(rgb_aligned_bgr, th_2d, patch=128, stride=128):\n",
        "    \"\"\"\n",
        "    rgb_aligned_bgr: (Hth,Wth,3) BGR\n",
        "    th_2d          : (Hth,Wth) float32\n",
        "    Return:\n",
        "      rgb_batch: (N,patch,patch,3) float32 RGB 0..1\n",
        "      th_batch : (N,patch,patch,1) float32 0..1 (normalized)\n",
        "      meta     : list dict\n",
        "    \"\"\"\n",
        "    H, W = th_2d.shape[:2]\n",
        "    assert rgb_aligned_bgr.shape[:2] == (H, W), \"RGB aligned must match thermal shape\"\n",
        "\n",
        "    rgb_list, th_list, meta = [], [], []\n",
        "    for y in range(0, H - patch + 1, stride):\n",
        "        for x in range(0, W - patch + 1, stride):\n",
        "            rgb_p_bgr = rgb_aligned_bgr[y:y+patch, x:x+patch]\n",
        "            th_p      = th_2d[y:y+patch, x:x+patch].astype(np.float32)\n",
        "\n",
        "            # BGR->RGB then normalize\n",
        "            rgb_p = cv2.cvtColor(rgb_p_bgr, cv2.COLOR_BGR2RGB)\n",
        "            rgb_f = rgb_p.astype(np.float32) / 255.0\n",
        "\n",
        "            # canopy mask from aligned RGB patch (expects uint8 RGB)\n",
        "            mask = canopy_mask_v2_from_rgb(rgb_p)  # bool (patch,patch)\n",
        "\n",
        "            # thermal canopy-only stats\n",
        "            th_can = th_p.copy()\n",
        "            th_can[~mask] = np.nan\n",
        "            vals = th_can[np.isfinite(th_can)]\n",
        "            if vals.size < 10:\n",
        "                vals = th_p[np.isfinite(th_p)]\n",
        "\n",
        "            lo = np.percentile(vals, 1) if vals.size else 0.0\n",
        "            hi = np.percentile(vals, 99) if vals.size else 1.0\n",
        "\n",
        "            th_norm = (th_p - lo) / (hi - lo + 1e-6)\n",
        "            th_norm = np.clip(th_norm, 0.0, 1.0).astype(np.float32)[..., None]\n",
        "\n",
        "            rgb_list.append(rgb_f.astype(np.float32))\n",
        "            th_list.append(th_norm)\n",
        "            meta.append({\"x\": x, \"y\": y, \"row\": y//stride, \"col\": x//stride})\n",
        "\n",
        "    rgb_batch = np.stack(rgb_list, axis=0) if rgb_list else np.zeros((0,patch,patch,3), np.float32)\n",
        "    th_batch  = np.stack(th_list, axis=0)  if th_list  else np.zeros((0,patch,patch,1), np.float32)\n",
        "    return rgb_batch, th_batch, meta\n",
        "\n",
        "print(\"âœ… 7.3 (replace) ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-GERLxEF3Lf"
      },
      "source": [
        "## Predict patches (Fusion model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuCoWDqiF0gg"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 7.5 (REPLACE) â€” Build heatmap on thermal grid\n",
        "# ============================================\n",
        "\n",
        "def build_heatmap_grid(meta, y_prob, H, W, patch=128, stride=128):\n",
        "    n_rows = 1 + (H - patch)//stride\n",
        "    n_cols = 1 + (W - patch)//stride\n",
        "    grid = np.zeros((n_rows, n_cols), dtype=np.float32)\n",
        "    for m, p in zip(meta, y_prob):\n",
        "        grid[m[\"row\"], m[\"col\"]] = p\n",
        "    heat = cv2.resize(grid, (W, H), interpolation=cv2.INTER_NEAREST)  # (H,W)\n",
        "    return grid, heat\n",
        "\n",
        "print(\"âœ… 7.5 (replace) ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wt3ZsaKF8Jk"
      },
      "source": [
        "## Heatmap + DSS decision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVRq-wTiF5Bd"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 7.5 â€” Heatmap + DSS decision\n",
        "# ============================================\n",
        "\n",
        "def build_heatmap(meta, y_prob, H, W, patch=128, stride=128):\n",
        "    # hitung grid size\n",
        "    n_rows = 1 + (H - patch)//stride\n",
        "    n_cols = 1 + (W - patch)//stride\n",
        "\n",
        "    grid = np.zeros((n_rows, n_cols), dtype=np.float32)\n",
        "    for m, p in zip(meta, y_prob):\n",
        "        grid[m[\"row\"], m[\"col\"]] = p\n",
        "\n",
        "    # upscale grid ke ukuran frame (biar bisa di-overlay)\n",
        "    heat = cv2.resize(grid, (W, H), interpolation=cv2.INTER_NEAREST)\n",
        "    return grid, heat\n",
        "\n",
        "def dss_decision(y_prob, thr=0.5):\n",
        "    if y_prob.size == 0:\n",
        "        return {\"stress_ratio\": 0.0, \"dss_status\": \"NO_DATA\", \"recommended_action\": \"No data\"}\n",
        "\n",
        "    stress_ratio = float((y_prob >= thr).mean())\n",
        "\n",
        "    if stress_ratio < 0.15:\n",
        "        return {\"stress_ratio\": stress_ratio, \"dss_status\": \"LOW_STRESS\", \"recommended_action\": \"No action needed\"}\n",
        "    elif stress_ratio < 0.35:\n",
        "        return {\"stress_ratio\": stress_ratio, \"dss_status\": \"MED_STRESS\", \"recommended_action\": \"Field inspection / localized check\"}\n",
        "    else:\n",
        "        return {\"stress_ratio\": stress_ratio, \"dss_status\": \"HIGH_STRESS\", \"recommended_action\": \"Immediate action: targeted irrigation/fertilization check\"}\n",
        "\n",
        "print(\"âœ… 7.5 heatmap + DSS ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaH-MQvnF9n5"
      },
      "outputs": [],
      "source": [
        "## MAIN: predict_frame (Fusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAhPYonxF627"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 7.6 (REPLACE) â€” MAIN predict_frame on thermal grid\n",
        "# ============================================\n",
        "\n",
        "def predict_frame_thermal_grid(rgb_path: str, thermal_npy_path: str,\n",
        "                              zoom=2.2, patch=128, stride=128, thr=None,\n",
        "                              save_dir=None):\n",
        "    thr = THR if thr is None else float(thr)\n",
        "\n",
        "    rgb_bgr = load_rgb_bgr(rgb_path)\n",
        "    th = load_thermal_npy(thermal_npy_path)\n",
        "\n",
        "    # Align RGB -> thermal grid\n",
        "    rgb_aligned_bgr, roi = rgb_to_thermal_grid(rgb_bgr, th.shape[:2], zoom=zoom)\n",
        "\n",
        "    Hth, Wth = th.shape[:2]\n",
        "    rgb_batch, th_batch, meta = extract_paired_patches_on_thermal_grid(\n",
        "        rgb_aligned_bgr, th, patch=patch, stride=stride\n",
        "    )\n",
        "\n",
        "    y_prob = model.predict([rgb_batch, th_batch], verbose=0).ravel().astype(np.float32)\n",
        "\n",
        "    # DSS decision (same as before)\n",
        "    dss = dss_decision(y_prob, thr=thr)\n",
        "\n",
        "    grid, heat = build_heatmap_grid(meta, y_prob, Hth, Wth, patch=patch, stride=stride)\n",
        "\n",
        "    out = {\n",
        "        \"mode\": \"FUSION_THERMAL_GRID\",\n",
        "        \"zoom\": float(zoom),\n",
        "        \"roi_on_rgb\": roi,     # (x0,y0,w,h) in original RGB\n",
        "        \"thermal_shape\": (int(Hth), int(Wth)),\n",
        "        \"n_patches\": int(len(meta)),\n",
        "        \"threshold\": float(thr),\n",
        "        **dss,\n",
        "        \"patch_results\": [{**m, \"p_stress\": float(p), \"pred\": int(p >= thr)} for m, p in zip(meta, y_prob)],\n",
        "        \"heatmap_path\": None\n",
        "    }\n",
        "\n",
        "    # Save outputs\n",
        "    if save_dir is not None:\n",
        "        save_dir = Path(save_dir); save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # heatmap grayscale\n",
        "        heat_u8 = np.clip(heat * 255.0, 0, 255).astype(np.uint8)\n",
        "        hm_path = save_dir / (Path(rgb_path).stem + f\"_heatmap_zoom{zoom}.png\")\n",
        "        cv2.imwrite(str(hm_path), heat_u8)\n",
        "        out[\"heatmap_path\"] = str(hm_path)\n",
        "\n",
        "        # overlay preview on aligned RGB\n",
        "        rgb_show = cv2.cvtColor(rgb_aligned_bgr, cv2.COLOR_BGR2RGB)\n",
        "        overlay = rgb_show.copy()\n",
        "        # simple alpha blend using heat as mask\n",
        "        heat3 = np.stack([heat_u8]*3, axis=-1)\n",
        "        overlay = cv2.addWeighted(overlay, 1.0, heat3, 0.45, 0.0)\n",
        "        ov_path = save_dir / (Path(rgb_path).stem + f\"_overlay_zoom{zoom}.png\")\n",
        "        cv2.imwrite(str(ov_path), cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
        "        out[\"overlay_path\"] = str(ov_path)\n",
        "\n",
        "    return out\n",
        "\n",
        "print(\"âœ… 7.6 (replace) ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MBzcheWF_jP"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 7.7 â€” TEST RUN\n",
        "# ============================================\n",
        "\n",
        "RGB_PATH = \"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/data/raw/rgb/DJI_20251223095727_0030_V.JPG\"\n",
        "TH_PATH  = \"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/data/raw/thermal/DJI_20251223095727_0030_T.npy\"\n",
        "\n",
        "out2 = predict_frame_thermal_grid(\n",
        "    rgb_path=RGB_PATH,\n",
        "    thermal_npy_path=TH_PATH,\n",
        "    zoom=2.2,\n",
        "    patch=128,\n",
        "    stride=128,\n",
        "    thr=THR,\n",
        "    save_dir=str(FIX_ROOT / \"dss_outputs\")\n",
        ")\n",
        "\n",
        "print(\"Mode:\", out2[\"mode\"])\n",
        "print(\"n_patches:\", out2[\"n_patches\"])\n",
        "print(\"stress_ratio:\", out2[\"stress_ratio\"])\n",
        "print(\"dss_status:\", out2[\"dss_status\"])\n",
        "print(\"overlay_path:\", out2.get(\"overlay_path\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkuwnW2XHKt-"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 7.10 â€” DSS scoring (extent + severity)\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "probs = np.array([r[\"p_stress\"] for r in out2[\"patch_results\"]], dtype=np.float32)\n",
        "\n",
        "thr = out2[\"threshold\"]\n",
        "pred = (probs >= thr)\n",
        "\n",
        "extent = float(pred.mean())                                # proporsi patch stress\n",
        "severity_mean = float(probs[pred].mean()) if pred.any() else 0.0\n",
        "severity_p90  = float(np.percentile(probs, 90))\n",
        "\n",
        "print(\"extent:\", extent)\n",
        "print(\"severity_mean(stress patches):\", severity_mean)\n",
        "print(\"severity_p90(all patches):\", severity_p90)\n",
        "\n",
        "# ---- DSS rule (lebih stabil dari cuma ratio) ----\n",
        "# Kamu bisa tweak angka ini nanti berdasarkan evaluasi / expert judgement\n",
        "if extent < 0.25 and severity_p90 < 0.75:\n",
        "    status = \"LOW_STRESS\"\n",
        "    action = \"No action needed; continue monitoring.\"\n",
        "elif extent < 0.50 and severity_p90 < 0.85:\n",
        "    status = \"MODERATE_STRESS\"\n",
        "    action = \"Check irrigation & nutrients in highlighted zones.\"\n",
        "else:\n",
        "    status = \"HIGH_STRESS\"\n",
        "    action = \"Immediate action: targeted field inspection + irrigation/fertilization adjustment.\"\n",
        "\n",
        "print(\"\\nDSS_STATUS_V2:\", status)\n",
        "print(\"recommended_action:\", action)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rirezbCHMaa"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 7.9 (REVISED) â€” Show saved overlay (thermal-grid aligned)\n",
        "# ============================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "ov_path = out2.get(\"overlay_path\", None)\n",
        "hm_path = out2.get(\"heatmap_path\", None)\n",
        "\n",
        "assert ov_path is not None, \"overlay_path tidak ada. Pastikan save_dir dipakai saat predict_frame_thermal_grid.\"\n",
        "\n",
        "overlay_bgr = cv2.imread(ov_path, cv2.IMREAD_COLOR)\n",
        "assert overlay_bgr is not None, f\"Gagal baca overlay: {ov_path}\"\n",
        "overlay_rgb = cv2.cvtColor(overlay_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(overlay_rgb)\n",
        "plt.title(f\"Overlay (thermal-grid aligned) | zoom={out2.get('zoom')} | n_patches={out2.get('n_patches')}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# (opsional) tampilkan heatmap grayscale juga\n",
        "if hm_path is not None:\n",
        "    heat_u8 = cv2.imread(hm_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if heat_u8 is not None:\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.imshow(heat_u8, cmap=\"gray\")\n",
        "        plt.title(\"Heatmap grayscale (saved)\")\n",
        "        plt.axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHwmbWIjJQrj"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 7.10 â€” DSS scoring (extent + severity)\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "probs = np.array([r[\"p_stress\"] for r in out2[\"patch_results\"]], dtype=np.float32)\n",
        "\n",
        "thr = out2[\"threshold\"]\n",
        "pred = (probs >= thr)\n",
        "\n",
        "extent = float(pred.mean())                                # proporsi patch stress\n",
        "severity_mean = float(probs[pred].mean()) if pred.any() else 0.0\n",
        "severity_p90  = float(np.percentile(probs, 90))\n",
        "\n",
        "print(\"extent:\", extent)\n",
        "print(\"severity_mean(stress patches):\", severity_mean)\n",
        "print(\"severity_p90(all patches):\", severity_p90)\n",
        "\n",
        "# ---- DSS rule (lebih stabil dari cuma ratio) ----\n",
        "# Kamu bisa tweak angka ini nanti berdasarkan evaluasi / expert judgement\n",
        "if extent < 0.25 and severity_p90 < 0.75:\n",
        "    status = \"LOW_STRESS\"\n",
        "    action = \"No action needed; continue monitoring.\"\n",
        "elif extent < 0.50 and severity_p90 < 0.85:\n",
        "    status = \"MODERATE_STRESS\"\n",
        "    action = \"Check irrigation & nutrients in highlighted zones.\"\n",
        "else:\n",
        "    status = \"HIGH_STRESS\"\n",
        "    action = \"Immediate action: targeted field inspection + irrigation/fertilization adjustment.\"\n",
        "\n",
        "print(\"\\nDSS_STATUS_V2:\", status)\n",
        "print(\"recommended_action:\", action)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMnOZs09JsUK"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 7.10b â€” DSS Rule V3 (extent + severity + hotspot)\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "probs = np.array([r[\"p_stress\"] for r in out2[\"patch_results\"]], dtype=np.float32)\n",
        "thr = float(out2.get(\"threshold\", 0.5))\n",
        "pred = (probs >= thr)\n",
        "\n",
        "extent = float(pred.mean())\n",
        "sev_mean = float(probs[pred].mean()) if pred.any() else 0.0\n",
        "p90 = float(np.percentile(probs, 90))\n",
        "p95 = float(np.percentile(probs, 95))\n",
        "p99 = float(np.percentile(probs, 99))\n",
        "\n",
        "print(\"extent:\", extent)\n",
        "print(\"sev_mean:\", sev_mean, \"| p90:\", p90, \"| p95:\", p95, \"| p99:\", p99)\n",
        "\n",
        "# ---- DSS V3 ----\n",
        "# tweakable thresholds (awal yang masuk akal)\n",
        "HOTSPOT = (p99 >= 0.90)                    # ada area sangat yakin stress\n",
        "SEVERE_WIDE = (extent >= 0.60 and p90 >= 0.80)  # luas + yakin tinggi\n",
        "MOD_WIDE = (extent >= 0.50 and p90 >= 0.65)     # luas + keyakinan moderat\n",
        "\n",
        "if SEVERE_WIDE or HOTSPOT:\n",
        "    status = \"HIGH_STRESS\"\n",
        "    action = \"Immediate action: targeted field inspection + irrigation/fertilization adjustment.\"\n",
        "elif MOD_WIDE:\n",
        "    status = \"MODERATE_STRESS\"\n",
        "    action = \"Action soon: inspect highlighted zones; adjust irrigation/nutrients if confirmed.\"\n",
        "else:\n",
        "    status = \"LOW_STRESS\"\n",
        "    action = \"No immediate action; continue monitoring.\"\n",
        "\n",
        "print(\"\\nDSS_STATUS_V3:\", status)\n",
        "print(\"recommended_action:\", action)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0l1smrHL9m2"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TAHAP 7.11 â€” Save DSS output JSON (single frame)\n",
        "# ============================================\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "SAVE_DIR = FIX_ROOT / \"dss_outputs\"\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def build_dss_v3(out):\n",
        "    probs = np.array([r[\"p_stress\"] for r in out[\"patch_results\"]], dtype=np.float32)\n",
        "    thr = float(out.get(\"threshold\", 0.5))\n",
        "    pred = probs >= thr\n",
        "\n",
        "    extent = float(pred.mean())\n",
        "    sev_mean = float(probs[pred].mean()) if pred.any() else 0.0\n",
        "    p90 = float(np.percentile(probs, 90))\n",
        "    p95 = float(np.percentile(probs, 95))\n",
        "    p99 = float(np.percentile(probs, 99))\n",
        "\n",
        "    HOTSPOT = (p99 >= 0.90)\n",
        "    SEVERE_WIDE = (extent >= 0.60 and p90 >= 0.80)\n",
        "    MOD_WIDE = (extent >= 0.50 and p90 >= 0.65)\n",
        "\n",
        "    if SEVERE_WIDE or HOTSPOT:\n",
        "        status = \"HIGH_STRESS\"\n",
        "        action = \"Immediate action: targeted field inspection + irrigation/fertilization adjustment.\"\n",
        "    elif MOD_WIDE:\n",
        "        status = \"MODERATE_STRESS\"\n",
        "        action = \"Action soon: inspect highlighted zones; adjust irrigation/nutrients if confirmed.\"\n",
        "    else:\n",
        "        status = \"LOW_STRESS\"\n",
        "        action = \"No immediate action; continue monitoring.\"\n",
        "\n",
        "    return {\n",
        "        \"threshold\": thr,\n",
        "        \"extent\": extent,\n",
        "        \"severity_mean\": sev_mean,\n",
        "        \"p90\": p90,\n",
        "        \"p95\": p95,\n",
        "        \"p99\": p99,\n",
        "        \"dss_status\": status,\n",
        "        \"recommended_action\": action,\n",
        "    }\n",
        "\n",
        "# --- build summary ---\n",
        "summary = build_dss_v3(out2)\n",
        "\n",
        "# --- choose a filename (use overlay name) ---\n",
        "overlay_name = Path(out2.get(\"overlay_path\",\"frame\")).stem\n",
        "json_path = SAVE_DIR / f\"{overlay_name}_dss.json\"\n",
        "\n",
        "payload = {\n",
        "    \"mode\": out2.get(\"mode\"),\n",
        "    \"zoom\": out2.get(\"zoom\"),\n",
        "    \"thermal_shape\": out2.get(\"thermal_shape\"),\n",
        "    \"n_patches\": out2.get(\"n_patches\"),\n",
        "    \"overlay_path\": out2.get(\"overlay_path\"),\n",
        "    \"heatmap_path\": out2.get(\"heatmap_path\"),\n",
        "    \"summary\": summary,\n",
        "    \"patch_results\": out2.get(\"patch_results\", [])  # row,col,x,y,p_stress,pred\n",
        "}\n",
        "\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(payload, f, indent=2)\n",
        "\n",
        "print(\"âœ… Saved:\", json_path)\n",
        "print(\"DSS:\", summary[\"dss_status\"], \"| extent:\", round(summary[\"extent\"],3), \"| p90:\", round(summary[\"p90\"],3), \"| p99:\", round(summary[\"p99\"],3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGATYlVX3OPo"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yzfKPZN51hE"
      },
      "source": [
        "# Backup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b7vrdEI52Uz"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "ROOT = Path(\"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/DatasetTerbaruFIX\")\n",
        "MODEL_PATH = ROOT / \"models\" / \"model_final_deploy_ft.keras\"\n",
        "EXPORT_DIR = ROOT / \"exports\"\n",
        "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"MODEL exists:\", MODEL_PATH.exists(), MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLAPjjMM7U6G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# coba beberapa kandidat path CSV (sesuaikan bila perlu)\n",
        "candidates = [\n",
        "    ROOT / \"patches\" / \"meta\" / \"patches_all_splits.csv\",\n",
        "    ROOT / \"patches\" / \"meta\" / \"patches_all_enriched.csv\",\n",
        "    ROOT / \"patches\" / \"meta\" / \"patches_all.csv\",\n",
        "]\n",
        "\n",
        "df = None\n",
        "for p in candidates:\n",
        "    if p.exists():\n",
        "        df = pd.read_csv(p)\n",
        "        print(\"Loaded:\", p, \"shape:\", df.shape)\n",
        "        break\n",
        "\n",
        "assert df is not None, \"CSV meta patch tidak ditemukan. Cek folder patches/meta di Drive.\"\n",
        "print(df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG-e4XYC7cR0"
      },
      "outputs": [],
      "source": [
        "# sesuaikan nama split jika berbeda\n",
        "if \"split\" in df.columns:\n",
        "    df_dep = df[df[\"split\"].astype(str).str.upper().isin([\"DEPLOY\",\"DEPLOYSET\",\"DEPLOY_SET\"])].copy()\n",
        "else:\n",
        "    # kalau tidak ada split, pakai semua yang punya label\n",
        "    df_dep = df.copy()\n",
        "\n",
        "# ambil yang labelnya ada\n",
        "label_col_candidates = [\"label\", \"y\", \"target\", \"label_final\"]\n",
        "label_col = next((c for c in label_col_candidates if c in df_dep.columns), None)\n",
        "assert label_col is not None, \"Kolom label tidak ketemu (label/y/target/label_final).\"\n",
        "\n",
        "df_dep = df_dep[df_dep[label_col].isin([0,1])].reset_index(drop=True)\n",
        "print(\"DEPLOY rows:\", len(df_dep))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MKol_ExktKP"
      },
      "source": [
        "# COBA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXnovt_jn0dZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "FINAL_LABELED_CSV = Path(\n",
        "    \"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/\"\n",
        "    \"DatasetTerbaruFIX/labels/final/patches_all_final_labeled_topk_10pct.csv\"\n",
        ")\n",
        "\n",
        "print(\"File exists:\", FINAL_LABELED_CSV.exists())\n",
        "\n",
        "df = pd.read_csv(FINAL_LABELED_CSV)\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YGmkisLku68"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# =========================\n",
        "# Load CSV (self-contained)\n",
        "# =========================\n",
        "df = pd.read_csv(\n",
        "    \"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/\"\n",
        "    \"DatasetTerbaruFIX/labels/final/patches_all_final_labeled_topk_10pct.csv\"\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# Filter manual + auto_confident\n",
        "# =========================\n",
        "df_eval = df[df[\"label_source\"].isin([\"manual\", \"auto_confident\"])].copy()\n",
        "\n",
        "df_eval[\"y_final\"] = pd.to_numeric(df_eval[\"y_final\"], errors=\"coerce\")\n",
        "df_eval = df_eval[df_eval[\"y_final\"].isin([0, 1])]\n",
        "df_eval[\"y_final\"] = df_eval[\"y_final\"].astype(int)\n",
        "\n",
        "df_eval = df_eval.reset_index(drop=True)\n",
        "\n",
        "print(\"Eval rows:\", len(df_eval))  # harus 1293\n",
        "\n",
        "# =========================\n",
        "# Build RGBâ€“Thermal pairing\n",
        "# =========================\n",
        "def rgb_to_thermal(rgb_path):\n",
        "    return str(rgb_path).replace(\"/rgb/\", \"/thermal/\")\n",
        "\n",
        "df_pairs = pd.DataFrame({\n",
        "    \"RGB_path\": df_eval[\"rgb_patch_path\"].astype(str),\n",
        "    \"Thermal_path\": df_eval[\"rgb_patch_path\"].apply(rgb_to_thermal),\n",
        "    \"Label\": df_eval[\"y_final\"]\n",
        "})\n",
        "\n",
        "# =========================\n",
        "# Buat kolom NAMA FILE saja\n",
        "# =========================\n",
        "df_pairs[\"RGB\"] = df_pairs[\"RGB_path\"].apply(lambda p: Path(p).name)\n",
        "df_pairs[\"Thermal\"] = df_pairs[\"Thermal_path\"].apply(lambda p: Path(p).name)\n",
        "\n",
        "# =========================\n",
        "# Pilih kolom untuk ditampilkan\n",
        "# =========================\n",
        "df_show = df_pairs[[\"RGB\", \"Thermal\", \"Label\"]]\n",
        "\n",
        "# =========================\n",
        "# TAMPILKAN: 10 awal + 10 akhir\n",
        "# =========================\n",
        "df_preview = pd.concat([df_show.head(10), df_show.tail(10)])\n",
        "\n",
        "display(df_preview)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBQlXOnFokhT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# =========================\n",
        "# PATH CSV\n",
        "# =========================\n",
        "CSV_PATH = (\n",
        "    \"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/\"\n",
        "    \"DatasetTerbaruFIX/labels/final/patches_all_final_labeled_topk_10pct.csv\"\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# LOAD & FILTER DATA\n",
        "# =========================\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "df_eval = df[df[\"label_source\"].isin([\"manual\", \"auto_confident\"])].copy()\n",
        "df_eval[\"y_final\"] = pd.to_numeric(df_eval[\"y_final\"], errors=\"coerce\")\n",
        "df_eval = df_eval[df_eval[\"y_final\"].isin([0, 1])]\n",
        "df_eval[\"y_final\"] = df_eval[\"y_final\"].astype(int)\n",
        "\n",
        "print(\"Total eval patches:\", len(df_eval))  # harus 1293\n",
        "\n",
        "# =========================\n",
        "# HITUNG JUMLAH LABEL\n",
        "# =========================\n",
        "label_counts = df_eval[\"y_final\"].value_counts().sort_index()  # index=0,1\n",
        "normal_count = label_counts.get(0, 0)\n",
        "stress_count = label_counts.get(1, 0)\n",
        "\n",
        "# =========================\n",
        "# PLOT DENGAN WARNA BERBEDA\n",
        "# =========================\n",
        "labels = ['Normal', 'Stress']\n",
        "counts = [normal_count, stress_count]\n",
        "colors = ['green', 'red']  # Normal = hijau, Stress = merah\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(labels, counts, color=colors)\n",
        "plt.ylabel(\"Jumlah Patch\")\n",
        "plt.title(\"Perbandingan Dataset Normal vs Stress\")\n",
        "\n",
        "# Optional: tampilkan angka di atas bar\n",
        "for i, count in enumerate(counts):\n",
        "    plt.text(i, count + 5, str(count), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwucGVkk9dDc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# =========================\n",
        "# Paths\n",
        "# =========================\n",
        "FIX_ROOT = Path(\"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/DatasetTerbaruFIX\")\n",
        "GOLD_CSV = FIX_ROOT / \"labels\" / \"gold\" / \"labels_gold.csv\"\n",
        "RGB_PATCH_DIR = FIX_ROOT / \"patches\" / \"rgb\"  # folder patch RGB asli\n",
        "SAVE_DIR = FIX_ROOT / \"dokumentasi\" / \"skip\"   # folder simpan patch skip\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# =========================\n",
        "# Load GOLD CSV\n",
        "# =========================\n",
        "if not GOLD_CSV.exists():\n",
        "    raise FileNotFoundError(f\"Tidak ditemukan: {GOLD_CSV}\")\n",
        "\n",
        "gold = pd.read_csv(GOLD_CSV)\n",
        "\n",
        "# normalisasi label\n",
        "gold[\"label_manual_num\"] = pd.to_numeric(gold[\"label_manual\"], errors=\"coerce\")\n",
        "notes = gold[\"notes\"].fillna(\"\").astype(str) if \"notes\" in gold.columns else pd.Series([\"\"]*len(gold))\n",
        "is_skip_note = notes.str.contains(r\"\\[SKIP\\]\", case=False, regex=True)\n",
        "is_empty_label = gold[\"label_manual_num\"].isna()\n",
        "\n",
        "# filter patch skip / empty\n",
        "skip_patches = gold[is_empty_label | is_skip_note]\n",
        "print(\"Total patch skip / empty:\", len(skip_patches))\n",
        "\n",
        "# ambil 20 patch random (atau semua jika <20)\n",
        "sample_skip = skip_patches.sample(min(10, len(skip_patches)), random_state=42)\n",
        "\n",
        "# =========================\n",
        "# Tampilkan & Simpan patch\n",
        "# =========================\n",
        "plt.figure(figsize=(15,10))\n",
        "possible_exts = [\".jpg\", \".jpeg\", \".png\"]\n",
        "\n",
        "saved_count = 0\n",
        "for i, row in enumerate(sample_skip.itertuples(), 1):\n",
        "    rgb_path = None\n",
        "    for ext in possible_exts:\n",
        "        candidate = RGB_PATCH_DIR / f\"{row.patch_id}{ext}\"\n",
        "        if candidate.exists():\n",
        "            rgb_path = candidate\n",
        "            break\n",
        "    if rgb_path is None:\n",
        "        print(f\"File tidak ditemukan untuk patch_id {row.patch_id}\")\n",
        "        continue\n",
        "\n",
        "    # load image\n",
        "    img = cv2.imread(str(rgb_path))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # tampilkan\n",
        "    plt.subplot(4,5,i)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    title = f\"{row.patch_id}\\nEmpty:{pd.isna(row.label_manual_num)} SkipNote:{'[SKIP]' in str(row.notes)}\"\n",
        "    plt.title(title, fontsize=8)\n",
        "\n",
        "    # simpan ke folder dokumentasi/skip\n",
        "    save_path = SAVE_DIR / f\"{row.patch_id}.jpg\"\n",
        "    cv2.imwrite(str(save_path), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "    saved_count += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(f\"âœ… Total patch berhasil disimpan: {saved_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u-Cc1p-GBEa6"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# FULL PIPELINE: load -> align (crop+zoom) -> overlay -> patches\n",
        "# =========================================================\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Utils\n",
        "# -----------------------------\n",
        "def normalize_to_u8(x, p1=1, p99=99):\n",
        "    \"\"\"Normalize 2D float array to uint8 for visualization.\"\"\"\n",
        "    x = x.astype(np.float32)\n",
        "    lo, hi = np.nanpercentile(x, p1), np.nanpercentile(x, p99)\n",
        "    x = np.clip((x - lo) / (hi - lo + 1e-6), 0, 1)\n",
        "    return (x * 255).astype(np.uint8)\n",
        "\n",
        "def center_crop(img, crop_h, crop_w):\n",
        "    H, W = img.shape[:2]\n",
        "    y0 = (H - crop_h) // 2\n",
        "    x0 = (W - crop_w) // 2\n",
        "    return img[y0:y0+crop_h, x0:x0+crop_w], (x0, y0, crop_w, crop_h)\n",
        "\n",
        "def rgb_to_thermal_grid(rgb_bgr, th_shape_hw, zoom=2.2):\n",
        "    \"\"\"\n",
        "    RGB wide -> crop center (zoom-in) -> resize to thermal grid.\n",
        "    zoom bigger => crop smaller => RGB looks more zoomed-in.\n",
        "    \"\"\"\n",
        "    Hth, Wth = th_shape_hw\n",
        "    Hr, Wr = rgb_bgr.shape[:2]\n",
        "    crop_h = int(Hr / zoom)\n",
        "    crop_w = int(Wr / zoom)\n",
        "\n",
        "    rgb_crop, roi = center_crop(rgb_bgr, crop_h, crop_w)\n",
        "    rgb_aligned = cv2.resize(rgb_crop, (Wth, Hth), interpolation=cv2.INTER_LINEAR)\n",
        "    return rgb_aligned, roi\n",
        "\n",
        "def show_alignment(rgb_aligned_bgr, th_2d, title_suffix=\"\"):\n",
        "    \"\"\"3-panel plot: RGB aligned, Thermal, Overlay.\"\"\"\n",
        "    th_u8 = normalize_to_u8(th_2d)\n",
        "    rgb_show = cv2.cvtColor(rgb_aligned_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1,3,1); plt.imshow(rgb_show); plt.title(f\"RGB aligned{title_suffix}\"); plt.axis(\"off\")\n",
        "    plt.subplot(1,3,2); plt.imshow(th_u8, cmap=\"inferno\"); plt.title(f\"Thermal{title_suffix}\"); plt.axis(\"off\")\n",
        "    plt.subplot(1,3,3); plt.imshow(rgb_show); plt.imshow(th_u8, cmap=\"inferno\", alpha=0.35); plt.title(f\"Overlay{title_suffix}\"); plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def extract_paired_patches(rgb_aligned_bgr, th_2d, patch=128, stride=128):\n",
        "    \"\"\"\n",
        "    Extract paired patches (RGB + Thermal) from same grid.\n",
        "    Return:\n",
        "      rgb_patches: (N, patch, patch, 3) uint8\n",
        "      th_patches : (N, patch, patch) float32\n",
        "      meta       : list dict per patch\n",
        "    \"\"\"\n",
        "    H, W = th_2d.shape[:2]\n",
        "    rgb_patches, th_patches, meta = [], [], []\n",
        "\n",
        "    for y in range(0, H - patch + 1, stride):\n",
        "        for x in range(0, W - patch + 1, stride):\n",
        "            rgb_p = rgb_aligned_bgr[y:y+patch, x:x+patch]\n",
        "            th_p  = th_2d[y:y+patch, x:x+patch]\n",
        "\n",
        "            rgb_patches.append(rgb_p)\n",
        "            th_patches.append(th_p.astype(np.float32))\n",
        "\n",
        "            meta.append({\n",
        "                \"x\": x, \"y\": y,\n",
        "                \"patch\": patch,\n",
        "                \"row\": y // stride,\n",
        "                \"col\": x // stride\n",
        "            })\n",
        "\n",
        "    return np.stack(rgb_patches, axis=0), np.stack(th_patches, axis=0), meta\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Paths (EDIT THIS)\n",
        "# -----------------------------\n",
        "rgb_path = \"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/data/raw/rgb/DJI_20251223095741_0048_V.JPG\"\n",
        "th_path  = \"/content/drive/MyDrive/rice_stress_detection_rgb_thermal/data/raw/thermal/DJI_20251223095741_0048_T.npy\"\n",
        "\n",
        "zoom = 2.2       # <-- ganti kalau perlu (2.0 / 2.2 / 2.4 / 2.6)\n",
        "patch = 128\n",
        "stride = 128\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Load data\n",
        "# -----------------------------\n",
        "rgb = cv2.imread(rgb_path)   # BGR\n",
        "if rgb is None:\n",
        "    raise ValueError(f\"RGB gagal dibaca. Cek path: {rgb_path}\")\n",
        "\n",
        "th = np.load(th_path)\n",
        "if th.ndim != 2:\n",
        "    raise ValueError(f\"Thermal npy harus 2D. Dapat shape={th.shape}\")\n",
        "\n",
        "print(\"RGB shape:\", rgb.shape, rgb.dtype)\n",
        "print(\"TH  shape:\", th.shape, th.dtype)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Align RGB -> thermal grid\n",
        "# -----------------------------\n",
        "rgb_aligned, roi = rgb_to_thermal_grid(rgb, th.shape[:2], zoom=zoom)\n",
        "print(\"ALIGN PARAMS:\", {\"zoom\": zoom, \"roi\": roi})\n",
        "print(\"RGB aligned shape:\", rgb_aligned.shape)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Sanity check overlay\n",
        "# -----------------------------\n",
        "show_alignment(rgb_aligned, th, title_suffix=f\" (zoom={zoom})\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Extract paired patches\n",
        "# -----------------------------\n",
        "rgb_patches, th_patches, meta = extract_paired_patches(rgb_aligned, th, patch=patch, stride=stride)\n",
        "print(\"RGB patches:\", rgb_patches.shape, rgb_patches.dtype)\n",
        "print(\"TH  patches:\", th_patches.shape, th_patches.dtype)\n",
        "print(\"Example meta[0]:\", meta[0])\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 7) (Optional) Visualize a few random paired patches\n",
        "# -----------------------------\n",
        "idxs = [0, len(meta)//2, len(meta)-1] if len(meta) >= 3 else list(range(len(meta)))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, idx in enumerate(idxs, 1):\n",
        "    rgb_p = cv2.cvtColor(rgb_patches[idx], cv2.COLOR_BGR2RGB)\n",
        "    th_u8 = normalize_to_u8(th_patches[idx])\n",
        "\n",
        "    ax = plt.subplot(2, len(idxs), i)\n",
        "    ax.imshow(rgb_p); ax.set_title(f\"RGB patch #{idx}\"); ax.axis(\"off\")\n",
        "\n",
        "    ax = plt.subplot(2, len(idxs), i + len(idxs))\n",
        "    ax.imshow(th_u8, cmap=\"inferno\"); ax.set_title(f\"TH patch #{idx}\"); ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =================================================\n",
        "# 8) Fungsi baru: overlay dari pasangan patch RGB+TH\n",
        "# =================================================\n",
        "def show_paired_patch_overlay(rgb_patches, th_patches, idxs=None, alpha=0.35):\n",
        "    \"\"\"\n",
        "    Tampilkan overlay RGB + Thermal dari patch yang sudah diekstrak.\n",
        "    Args:\n",
        "        rgb_patches: np.array (N, H, W, 3) uint8\n",
        "        th_patches : np.array (N, H, W) float32\n",
        "        idxs       : list[int] indices patch yang ingin ditampilkan, default: semua\n",
        "        alpha      : float, opacity thermal overlay\n",
        "    \"\"\"\n",
        "    N = len(rgb_patches)\n",
        "    if idxs is None:\n",
        "        idxs = list(range(N))\n",
        "\n",
        "    plt.figure(figsize=(12, 4*len(idxs)))\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        rgb = cv2.cvtColor(rgb_patches[idx], cv2.COLOR_BGR2RGB)\n",
        "        th_u8 = normalize_to_u8(th_patches[idx])\n",
        "\n",
        "        plt.subplot(len(idxs), 1, i)\n",
        "        plt.imshow(rgb)\n",
        "        plt.imshow(th_u8, cmap=\"inferno\", alpha=alpha)\n",
        "        plt.title(f\"Overlay patch #{idx}\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Contoh penggunaan\n",
        "# -----------------------------\n",
        "# pilih beberapa patch acak atau tertentu\n",
        "sample_idxs = [0, 10, 19] if len(rgb_patches) > 20 else list(range(len(rgb_patches)))\n",
        "show_paired_patch_overlay(rgb_patches, th_patches, idxs=sample_idxs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# =========================\n",
        "# PATHS\n",
        "# =========================\n",
        "RAW_RGB_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"rgb\"\n",
        "RAW_TH_DIR  = PROJECT_ROOT / \"data\" / \"raw\" / \"thermal\"\n",
        "\n",
        "if not RAW_RGB_DIR.exists():\n",
        "    raise FileNotFoundError(f\"RAW_RGB_DIR tidak ditemukan: {RAW_RGB_DIR}\")\n",
        "if not RAW_TH_DIR.exists():\n",
        "    raise FileNotFoundError(f\"RAW_TH_DIR tidak ditemukan: {RAW_TH_DIR}\")\n",
        "\n",
        "rgb_exts = (\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\")\n",
        "rgb_paths = sorted([p for p in RAW_RGB_DIR.iterdir() if p.is_file() and p.name.endswith(rgb_exts)])\n",
        "th_paths  = sorted([p for p in RAW_TH_DIR.iterdir() if p.is_file() and p.suffix.lower() == \".npy\"])\n",
        "\n",
        "print(\"Total RGB files   :\", len(rgb_paths))\n",
        "print(\"Total Thermal npy :\", len(th_paths))\n",
        "\n",
        "# =========================\n",
        "# HELPERS\n",
        "# =========================\n",
        "def parse_frame_id_from_rgb_name(rgb_filename: str) -> str:\n",
        "    base = rgb_filename\n",
        "    for ext in [\".JPG\", \".JPEG\", \".PNG\", \".jpg\", \".jpeg\", \".png\"]:\n",
        "        if base.endswith(ext):\n",
        "            base = base[:-len(ext)]\n",
        "            break\n",
        "    if base.endswith(\"_V\"):\n",
        "        return base[:-2]\n",
        "    return base\n",
        "\n",
        "# DJI_YYYYMMDDHHMMSS_#### pattern\n",
        "_pat = re.compile(r\"DJI_(\\d{14})_(\\d{4})\", re.IGNORECASE)\n",
        "\n",
        "def parse_ts_seq_from_name(name: str):\n",
        "    \"\"\"\n",
        "    Return (datetime|None, int|None) from filename.\n",
        "    \"\"\"\n",
        "    m = _pat.search(name)\n",
        "    if not m:\n",
        "        return None, None\n",
        "    ts_str = m.group(1)\n",
        "    seq_str = m.group(2)\n",
        "    dt = datetime.strptime(ts_str, \"%Y%m%d%H%M%S\")\n",
        "    return dt, int(seq_str)\n",
        "\n",
        "def fmt_ts(dt: datetime | None):\n",
        "    if dt is None:\n",
        "        return \"-\"\n",
        "    return dt.strftime(\"%H:%M:%S\")\n",
        "\n",
        "def human_ts_note(rgb_dt, th_dt):\n",
        "    \"\"\"\n",
        "    Return human-friendly timestamp note, e.g.\n",
        "    'Sesuai (09:57:08)' or 'Beda 1 dtk (RGB 09:57:08 vs TH 09:57:09)'\n",
        "    \"\"\"\n",
        "    if rgb_dt is None or th_dt is None:\n",
        "        return \"Tidak terbaca\"\n",
        "    diff = int(abs((th_dt - rgb_dt).total_seconds()))\n",
        "    if diff == 0:\n",
        "        return f\"Sesuai ({fmt_ts(rgb_dt)})\"\n",
        "    return f\"Beda {diff} dtk (RGB {fmt_ts(rgb_dt)} vs TH {fmt_ts(th_dt)})\"\n",
        "\n",
        "# =========================\n",
        "# INDEX THERMAL BY (seq) AND BY TIMESTAMP\n",
        "# =========================\n",
        "thermal_meta = []\n",
        "for th_p in th_paths:\n",
        "    th_dt, th_seq = parse_ts_seq_from_name(th_p.name)\n",
        "    thermal_meta.append({\n",
        "        \"th_name\": th_p.name,\n",
        "        \"th_path\": str(th_p),\n",
        "        \"th_dt\": th_dt,\n",
        "        \"th_seq\": th_seq\n",
        "    })\n",
        "\n",
        "th_df = pd.DataFrame(thermal_meta)\n",
        "th_df_ts = th_df.dropna(subset=[\"th_dt\"]).sort_values(\"th_dt\").reset_index(drop=True)\n",
        "\n",
        "# quick map: seq -> list of thermal rows (in case duplicates)\n",
        "th_by_seq = {}\n",
        "for _, r in th_df.dropna(subset=[\"th_seq\"]).iterrows():\n",
        "    th_by_seq.setdefault(int(r[\"th_seq\"]), []).append(r)\n",
        "\n",
        "def find_nearest_thermal_same_seq(rgb_seq: int, rgb_dt: datetime):\n",
        "    \"\"\"\n",
        "    Find thermal with same sequence number, closest by timestamp.\n",
        "    \"\"\"\n",
        "    candidates = th_by_seq.get(rgb_seq, [])\n",
        "    if not candidates:\n",
        "        return None\n",
        "    best = None\n",
        "    best_diff = None\n",
        "    for r in candidates:\n",
        "        if r[\"th_dt\"] is None or rgb_dt is None:\n",
        "            continue\n",
        "        diff = abs((r[\"th_dt\"] - rgb_dt).total_seconds())\n",
        "        if best is None or diff < best_diff:\n",
        "            best = r\n",
        "            best_diff = diff\n",
        "    return best\n",
        "\n",
        "# =========================\n",
        "# BUILD TWO SIMPLE TABLES\n",
        "# =========================\n",
        "rows_ok = []\n",
        "rows_no = []\n",
        "\n",
        "# Tolerance to call it \"offset kecil\" (detik)\n",
        "OFFSET_TOL_SEC = 2\n",
        "\n",
        "for rgb_p in rgb_paths:\n",
        "    frame_id = parse_frame_id_from_rgb_name(rgb_p.name)\n",
        "    rgb_dt, rgb_seq = parse_ts_seq_from_name(rgb_p.name)\n",
        "\n",
        "    expected_th_name = f\"{frame_id}_T.npy\"\n",
        "    expected_th_path = RAW_TH_DIR / expected_th_name\n",
        "\n",
        "    # CASE 1: Exact filename exists => Pairing OK\n",
        "    if expected_th_path.exists():\n",
        "        th_dt, _ = parse_ts_seq_from_name(expected_th_name)\n",
        "        rows_ok.append({\n",
        "            \"RGB_File\": rgb_p.name,\n",
        "            \"Thermal_File\": expected_th_name,\n",
        "            \"Timestamp\": human_ts_note(rgb_dt, th_dt),\n",
        "            \"Pairing\": \"OK\"\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # CASE 2: Exact missing => try find \"thermal terdekat\" dengan frame number sama (seq sama)\n",
        "    near = None\n",
        "    if rgb_seq is not None and rgb_dt is not None:\n",
        "        near = find_nearest_thermal_same_seq(int(rgb_seq), rgb_dt)\n",
        "\n",
        "    if near is not None and near[\"th_dt\"] is not None and rgb_dt is not None:\n",
        "        diff = abs((near[\"th_dt\"] - rgb_dt).total_seconds())\n",
        "        # kalau offset kecil -> \"Tidak OK\" (karena tidak sinkron nama/timestamp persis)\n",
        "        if diff <= OFFSET_TOL_SEC:\n",
        "            rows_no.append({\n",
        "                \"RGB_File\": rgb_p.name,\n",
        "                \"Thermal_File_Terdekat\": near[\"th_name\"],\n",
        "                \"Timestamp\": human_ts_note(rgb_dt, near[\"th_dt\"]),\n",
        "                \"Pairing\": \"Tidak OK (offset waktu)\"\n",
        "            })\n",
        "        else:\n",
        "            # kasus lain: kemungkinan thermal benar-benar beda sesi / hilang\n",
        "            rows_no.append({\n",
        "                \"RGB_File\": rgb_p.name,\n",
        "                \"Thermal_File_Terdekat\": near[\"th_name\"],\n",
        "                \"Timestamp\": human_ts_note(rgb_dt, near[\"th_dt\"]),\n",
        "                \"Pairing\": \"Tidak OK (tidak sinkron)\"\n",
        "            })\n",
        "    else:\n",
        "        # tidak menemukan thermal dengan frame yang sama\n",
        "        rows_no.append({\n",
        "            \"RGB_File\": rgb_p.name,\n",
        "            \"Thermal_File_Terdekat\": \"-\",\n",
        "            \"Timestamp\": \"Thermal tidak ditemukan\",\n",
        "            \"Pairing\": \"Tidak OK (thermal tidak ada)\"\n",
        "        })\n",
        "\n",
        "df_ok = pd.DataFrame(rows_ok)\n",
        "df_no = pd.DataFrame(rows_no)\n",
        "\n",
        "print(\"\\n=== RINGKASAN ===\")\n",
        "print(\"Pairing OK       :\", len(df_ok))\n",
        "print(\"Pairing Tidak OK :\", len(df_no))\n",
        "\n",
        "display(df_ok.head(5))\n",
        "display(df_no.head(5))\n",
        "\n",
        "# =========================\n",
        "# OPTIONAL: SAVE (kalau perlu)\n",
        "# =========================\n",
        "OUT_DIR = PROJECT_ROOT / \"dokumentasi\" / \"pairing_audit\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ok_csv = OUT_DIR / \"table_pairing_ok.csv\"\n",
        "no_csv = OUT_DIR / \"table_pairing_tidak_ok.csv\"\n",
        "\n",
        "df_ok.to_csv(ok_csv, index=False)\n",
        "df_no.to_csv(no_csv, index=False)\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(ok_csv)\n",
        "print(no_csv)\n"
      ],
      "metadata": {
        "id": "s52uPbkbz4IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== RINGKASAN ===\")\n",
        "print(\"Pairing OK       :\", len(df_ok))\n",
        "print(\"Pairing Tidak OK :\", len(df_no))\n",
        "\n",
        "# =========================\n",
        "# DISPLAY: 5 AWAL & 5 AKHIR\n",
        "# =========================\n",
        "\n",
        "print(\"\\n--- TABLE 1: Pairing OK (5 Awal) ---\")\n",
        "display(df_ok.head(5))\n",
        "\n",
        "print(\"\\n--- TABLE 1: Pairing OK (5 Akhir) ---\")\n",
        "display(df_ok.tail(5))\n",
        "\n",
        "print(\"\\n--- TABLE 2: Pairing Tidak OK (5 Awal) ---\")\n",
        "display(df_no.head(5))\n",
        "\n",
        "print(\"\\n--- TABLE 2: Pairing Tidak OK (5 Akhir) ---\")\n",
        "display(df_no.tail(5))\n"
      ],
      "metadata": {
        "id": "0L_OKQkx4-3w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SeNZJmbybzP1"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}